---
date: 2026-02-15
categories:
- GPU Virtualization
- System Design
title: "GPU Virtualization Part 1: An Introduction to PCIe and MMIO"
description: How modern CPUs talk to PCIe devices.
draft: true
---

![](https://f005.backblazeb2.com/file/landons-blog/assets/posts/2026-02-15-pcie-mmio/pcie-mmio-mountains.png){ loading=lazy  style="width: 100%; height: 425px; object-fit: cover; object-position: 0 40%"}

PCIe is a packet-switched fabric, and modern devices are controlled by two primitives: memory-mapped registers (BARs via MMIO) and DMA. GPU virtualization ultimately reduces to controlling access to those primitives, who can access which address ranges, and which device-initiated transactions are permitted.

This series starts at the bottom of the stack and builds upward. In this first post, we’ll examine how PCIe routing works, how BARs carve out regions of the system’s physical address space, and how the IOMMU constrains device-initiated memory access. These details form the hardware boundary that virtualization systems rely on.

<!-- more -->

## PCIe

### A Brief Introduction

PCIe is an acronym that stands for Peripheral Component Interconnect Express. It's a standard that was created to solve the problem of how CPUs should be able to access peripheral devices like network cards, storage, audio devices, and graphical accelerators. The _express_ portion of the name alludes to the fact that it's an extension to the older PCI bus standard. The Original Gangster PCI standard, which is mostly no longer used, relied on a shared bus topology. That is, all devices communicated through a single data bus and clock line in a flat topology and shared a common address space. This scheme was greatly limiting for a number of reasons, one of which being that the clock was limited to the slowest peripheral on the bus.

PCI _Express_, on the other hand, looks more like a switched network. It's comprised of a tree of interconnected components, with PCIe bridges and switches connecting components together. Its architecture allows for full duplex communication and is theoretically extendible to a virtually infinite number of endpoints. PCIe also allows for a variable number of lanes, anywhere from 1 to 16 for standard PCIe form factors (such as what is used in consumer grade desktops) and even higher for certain CPU chipsets. A _lane_ comprises of a differential signal pair, so 16 lanes actually represents 32 physical lines. Sending data through differential signal pairs allows noise to be effectively filtered out. This is a technique used in other data transmission standards like XLR cables for audio equipment.

### Topology Considerations

Below is an example PCIe topology inspired from the [Wikipedia page](https://en.wikipedia.org/wiki/PCI_Express).

```title="" 
                              +-----------------+                                                        
                              |                 |                                                        
                              |                 |                                                        
                              |      CPU        |                                                        
                              |                 |                                                        
                              |                 |                                                        
                              |                 |                                                        
                              +--------+--------+                                                        
                                       |                                                                 
                                       |                                                                 
                                       |                                                                 
                                       |                                                                 
            +--------------------------+-----------------------------------------+          +-----------+
            |                                                                    |          |           |
            |                           Root Complex                             +----------+  Memory   |
            |                                                                    |          |           |
            |                                                                    |          |           |
            +---------+---------------------------+------------------------+-----+          +-----------+
                      |                           |                        |                             
                      |                           |                        |                             
                      |                           |                        |                             
                      |                           |                        |                             
                +-----+-----+               +-----+-----+            +-----+-----+                       
                |           |               |           |            |           |                       
                |           |               |  Endpoint |            |  Bridge   |                       
      +---------+  Switch   +---------+     |           |            |           |                       
      |         |           |         |     |           |            |           |                       
      |         +-----+-----+         |     +-----------+            +-----------+                       
      |               |               |                                                                  
      |               |               |                                                                  
      |               |               |                                                                  
+-----+-----+   +-----+-----+    +----+------+                                                           
|           |   |           |    |           |                                                           
|  Endpoint |   |  Endpoint |    |  Endpoint |                                                           
|           |   |           |    |           |                                                           
|           |   |           |    |           |                                                           
+-----------+   +-----------+    +-----------+
```

This diagram is obviously dramatically simplified, however we can see a few notable components. The "Root Complex" at the top (which we will talk more about later) connects the CPU with downstream components like switches, endpoints, and legacy buses via a bridge. While I worked at Lambda, I developed an interactive Terminal UI (TUI) called PCIEx that you can use to explore the PCIe topology on your local system. You can find that [here](https://github.com/LandonTClipp/pciex). I encourage you to submit issues to my fork of Lambda's upstream repo. The navigation controls are admittedly a little clunky but it was also my first attempt at a TUI, so all is fair I guess.

![PCIEx](https://f005.backblazeb2.com/file/landons-blog/assets/posts/2026-02-15-pcie-mmio/pciex.png)

The screenshot above is of a Supermicro A10 system. You can see two "3D Controllers" (which is PCIe parlance for a GPU) that live behind a set of two bridges each. You can also see an Infiniband CX6 network card. The right pane shows a number of details about each device as you scroll that is scraped from the `/sys/bus/pci/devices` directory, including the device's BDF (its Bus-Device-Function address), its NUMA node, its local CPU list, and its lane width of 64.[^1]

The `Intel Corporation` devices you see at the bottom are actually one physical device that is exposing multiple different functions. Bridges generally only have one downstream port, so any time you see what appears to be multiple devices living behind a single bridge, it's often because the device itself is exposing multiple functions. 

Another scenario not pictured here is when you have multiple devices living under the same switch, or underneath the same root port. In these cases, such devices may be able to talk directly to one another through peer-to-peer (P2P). This may not always desirable. Consider a situation where you're a cloud provider and you want to allow multiple customers to rent the same physical node. There may be some devices that while technically speaking _could_ do peer-to-peer with each other, for the purposes of isolating customers from one another, you might want to disallow this. This is where [ACS](#access-control-services-acs) comes into play which we will talk about later on.

### Root Complex

The Root Complex is a physical component, often part of the motherboard's chipset but also sometimes resident on the CPU die itself, that performs a number of important functions. It is the boundary between the PCIe fabric and the rest of the system. It connects PCIe devices to the CPU cores and memory controllers, and it is where device-initiated transactions (DMA) enter the system and are subject to translation and access control.

#### DMA

The Root Complex also plays a critical role in Direct Memory Access (DMA) transactions. Consider that you're a GPU that wants to transfer some of your VRAM to the host memory so that programs running on the CPU can read it. The traditional/old fashioned way of transferring the memory would be for the CPU to initiate PCIe transactions and read data from the device, then copy this data to host memory via CPU registers. This is an extremely inefficient method of memory transfer because not only does it require CPU involvement, but it often requires long iterations to fully sweep the entirety of the device's memory. DMA instead cuts the CPU out of the equation and allows the GPU to send to and receive data from the host memory of its own volition.

Allowing PCIe devices to write arbitrary data to arbitrary locations in memory is extremely dangerous for obvious reasons. Bugs in firmware could lead to writes to critical areas in memory such as kernel data structures, and even worse might allow a device owned by customer A to read customer B's data. This calls for a method of sandboxing GPU memory access. The IOMMU exists to solve that problem.

#### IOMMUs

The Input-Output Memory Management Unit (IOMMU) is a component of the root complex that maintains a number of page tables specifically designed for IO devices. It's used when a device wants to read or write data into the host memory and needs to translate its IO Virtual Address (IOVA) to a Host Physical Address (HPA). The IOMMU enforces translations per domain (a context). Devices are assigned to a domain, and the domain points to the page tables the IOMMU walks. An abstracted diagram of what this is doing is shown below:

```title="IOMMU" linenums="1"
                       IOMMU                         
┌───────────────────────────────────────────────────┐
│                                                   │
│        IOVA                          HPA          │
│ ┌────────────────┐            ┌────────────────┐  │
│ │   0x00-0xF0    ├───────────►│    0x5000      │  │
│ ├────────────────┤            ├────────────────┤  │
│ │                │            │                │  │
│ │   0xF1-0xFF    │            ├────────────────┤  │
│ │                ├──────┐     │                │  │
│ │                │      │     ├────────────────┤  │
│ ├────────────────┤      │     │                │  │
│ │                │      │     ├────────────────┤  │
│ │                │      └────►│    0xF000      │  │
│ │                │            ├────────────────┤  │
│ │  0x100-0xFFF   │            │                │  │
│ │                ├──────┐     ├────────────────┤  │
│ │                │      │     │                │  │
│ │                │      │     ├────────────────┤  │
│ ├────────────────┤      └────►│    0xBF00      │  │
│ │                │            ├────────────────┤  │
│ │                │      ┌────►│    0xDEADC0DE  │  │
│ │  0x1000-0x10F0 │      │     ├────────────────┤  │
│ │                ├──────┘     │                │  │
│ └────────────────┘            └────────────────┘  │
│                                                   │
└───────────────────────────────────────────────────┘
```

When a PCIe device performs a memory read or write, it provides an IOVA to the IOMMU which gets translated to the host physical address. The transaction is then forwarded to the memory controller (for RAM) or back into the root complex (for MMIO). This provides three crucial functions to devices:

1. It allows the OS/hypervisor to present a contiguous IOVA space even if the system’s physical memory is fragmented, and even if the device can’t address all physical memory directly.
2. It provides a method for restricting where in host memory a device can DMA to.
3. It also enforces access control based on physical address ranges. If a device attempts to access a physical region (including another device’s BAR) that is not mapped into its IOMMU domain, the transaction is blocked.

Number 2 is particularly crucial when it comes to device virtualization. When doing what's called direct passthrough to a virtual machine, the CPU will program the IOMMU such that a device is physically restricted to DMA to the memory allocated for the guest. This provides a hard level of hardware memory isolation. The components interact with each other like this:

```title="" linenums="1"
                           ┌─────────────────┐                           
                           │                 │                           
                           │                 │                           
                           │      CPU        │                           
                           │                 │                           
                           │                 │                           
                           │                 │                           
                           └────────┬────────┘                           
                                    │                                    
                                    │programs                            
                                    │                                    
                                    ▼                                    
┌─────────────────┐        ┌─────────────────┐        ┌─────────────────┐
│                 │        │                 │        │                 │
│                 │        │                 │        │                 │
│                 │ IOVA   │                 │ HPA    │                 │
│   PCIe Device   ├───────►│     IOMMU       ├───────►│ RAM Controller  │
│                 │        │                 │        │                 │
│                 │        │                 │        │                 │
└─────────────────┘        └─────────────────┘        └─────────────────┘
```

Just as the CPU MMU translates process virtual addresses to physical memory, the IOMMU translates device virtual addresses (IOVAs) to physical memory.

### Bridges and Switches

PCIe fabrics are built out of endpoints, bridges, and switches. At a high level:

- A **bridge** connects one upstream port to one downstream port.
- A **switch** connects one upstream port to multiple downstream ports.

Bridges were originally used to connect different PCI variants (for example, PCIe to legacy PCI), and more generally to segment a hierarchy into multiple buses. In the PCI model, the topology consists only of devices and bridges.

A PCIe switch is conceptually more powerful: it is responsible for routing transactions between multiple downstream ports. However, for software compatibility reasons, the PCIe specification defines a switch as a logical collection of PCI-to-PCI bridges. To the operating system, a switch appears as multiple downstream bridge devices connected to a single upstream bridge.

![](https://f005.backblazeb2.com/file/landons-blog/assets/posts/2026-02-15-pcie-mmio/pcie_switch.png)

!!! quote "[PCIe Base Specification Revision 5.0 Version 1.0 §1.3.3](https://f005.backblazeb2.com/file/landons-blog/assets/posts/2026-02-15-pcie-mmio/pcie_spec_rev5.pdf)"

    A Switch is defined as a logical assembly of multiple virtual PCI-to-PCI Bridge devices as illustrated in Figure 1-3 . All Switches are governed by the following base rules.

    - Switches appear to configuration software as two or more logical PCI-to-PCI Bridges.
    - A Switch forwards transactions using PCI Bridge mechanisms; e.g., address-based routing except when engaged in a Multicast, as defined in Section 6.14.

This detail matters because routing decisions, including whether a transaction is forwarded to a peer device or forced upstream (as with ACS), occur at these bridge/switch boundaries.

### Transaction Layer Packets

We've talked about many of the components that are involved in PCIe, but we haven't talked about how the PCIe network handles routing decisions. PCIe uses packetized transactions. Payload sizes are bounded by negotiated limits like Max Payload Size and Max Read Request Size. Each packet is called a Transaction Layer Packet (TLP). TLPs are fairly analogous to TCP/IP packets: they contain headers with address information, payloads, and digests used for error correction. The header of a TLP looks like this:

![TLP Header](https://f005.backblazeb2.com/file/landons-blog/assets/posts/2026-02-15-pcie-mmio/tlp_header.JPG)

This [blog post](https://www.semisaga.com/2019/07/pcie-tlp-header-packet-formats-address.html) provides information on the various types of TLP packets that can be sent:

|TLP Type|Format|Type|Description|
|--- |--- |--- |--- |
|MR|000 / 001|0 0000|Memory Read Request|
|MRL|000 / 001|0 0001|Memory Read Request Locked|
|MW|010|0 0000|Memory Write Request|
|IOR|000|0 0010|I / O Read Request|
|IOW|010|0 0010|I / O Write Request|
|CR0|000|0 0100|Configuration Read Type 0|
|CW0|010|0 0100|Configuration Write Type 0|
|CR1|000|0 0101|Configuration Read Type 1|
|CW1|010|0 0101|Configuration Write Type 1|
|Msg|001|1 0 r2 r1 r0|Message Request|
|MsgD|011|1 0 r2 r1 r0|Message Request with Data|

### Windows and Routing

Every bridge or switch has three specific "windows" that can be addressed by a TLP:

1. Actions on configuration. In this scenario, the TLP is taking action on a specific device's configuration space.
2. IO ports. This is used for legacy port-based IO and for the most part is not used in any modern systems with very small exceptions.
3. Memory. This is used mainly in memory mapped IO (which we will also talk about later) to access the memory inside of a device.

When the system BIOS boots and walks the PCIe tree, it assigns address to each of the devices and saves it into their configuration registers. A switch is aware not only of its address, but also the address windows for each of its downstream ports. Let's take a simple example of what this might look like:

```title="" linenums="1"
CPU / Root Complex
        |
   Root Port
        |
   PCIe Switch
   ├── Port 1 → GPU (MMIO: 0x4000_0000_0000–0x43FF_FFFF_FFFF)
   ├── Port 2 → NIC (MMIO: 0x4400_0000_0000–0x44FF_FFFF_FFFF)
   └── Port 3 → NVMe (MMIO: 0x4500_0000_0000–0x4500_0FFF_FFFF)
```

When the TLP with an address of `0x4000_0000_0000` arrives to the PCIe switch, it immediately knows that the packet must be forwarded to port 1. In the opposite direction, if the GPU on port 1 sends a TLP with an address of `0x3000_0000_0000`, the switch is not aware of any port with such a window and thus forwards the packet to its upstream port. In the P2P case with [ACS](#access-control-services-acs) disabled, if Port 1 sends a packet to address `0x4400_0000_0000`, the switch will route it to port 2 without any involvement of the root complex.

We can take a slightly more complicated example to see how this works in multi-layered switch topologies. Take for example this scenario:

```title="" linenums="1"
CPU / Root Complex
        |
     Root Port
        |
     PCIe Switch S0
   ├── Port 1 → PCIe Switch S1
   │              ├── Port 1 → GPU  (MMIO: 0x4000_0000_0000–0x4000_0FFF_FFFF)
   │              └── Port 2 → NVMe (MMIO: 0x4000_1000_0000–0x4000_1000_FFFF)
   │
   │      (S0 Port 1 window: 0x4000_0000_0000–0x4000_1FFF_FFFF)
   │
   └── Port 2 → NIC (MMIO: 0x5000_0000_0000–0x5000_00FF_FFFF)
```

In this case, Port 1 of switch S0 is programmed with the window `0x4000_0000_0000–0x4000_1FFF_FFFF` so that it encompasses all of the devices in that subtree. In fact, you can view what windows these ports have been programmed with in your own system:

```title="" linenums="1"
$ lspci | grep -i bridge
00:0f.0 PCI bridge: Intel Corporation Device 1bbf (rev 11)
$ lspci -vv -s '00:0f.0' |& grep -i behind
        I/O behind bridge: 2000-2fff [size=4K] [16-bit]
        Memory behind bridge: 94000000-950fffff [size=17M] [32-bit]
        Prefetchable memory behind bridge: [disabled] [64-bit]
```

This is just a random bridge, but let's find one that is directly in front of a GPU. We can use my pciex tool to easily find a good example:

```title="" linenums="1"
├── bridge | PCIBUS:0000:1e
│   └── bridge | PCIBUS:0000:1f        
│       ├── bridge | PCIBUS:0000:20    
│       │   └── display | AD102GL [L40]
```

What we should see is that the bridge immediately adjacent to the GPU has an identical memory window. This is the GPU's memory regions:

```title="" linenums="1"
lspci -vv -s '20:00.0' |& grep Region
        Region 0: Memory at a8000000 (32-bit, non-prefetchable) [size=16M]
        Region 1: Memory at 6d000000000 (64-bit, prefetchable) [size=64G]
        Region 3: Memory at 6f040000000 (64-bit, prefetchable) [size=32M]
```

This is the bridge just above it:

```title="" linenums="1"
$ lspci -vv -s '1f:00.0' | grep -i behind
        I/O behind bridge: [disabled] [32-bit]
        Memory behind bridge: a8000000-a97fffff [size=24M] [32-bit]
        Prefetchable memory behind bridge: 6d000000000-6f041ffffff [size=132128M] [32-bit]
```

We can see the `Prefetchable memory behind bridge` section starts exactly where `Region 1` of the device does. The end address of `0x6f041ffffff` not coincidentally happens to be the start of `Region 3` plus 32MiB. `0x6f040000000` + `0x2000000` = `6F042000000`. If we look in the `/proc/iomem` file, we see these values represented again, but this time with an even better representation of how these windows are laid out (with some annotations of my own):

```title="" linenums="1"
60000000000-6ffffffffff : PCI Bus 0000:1d
  6a000000000-6f041ffffff : PCI Bus 0000:1e    <-- Bridge two levels above GPU
    6a000000000-6f041ffffff : PCI Bus 0000:1f  <-- Bridge immediately adjacent to GPU
      6a000000000-6c041ffffff : PCI Bus 0000:21
        6a000000000-6afffffffff : 0000:21:00.0
        6b000000000-6bfffffffff : 0000:21:00.0
        6c000000000-6c03fffffff : 0000:21:00.0
        6c040000000-6c041ffffff : 0000:21:00.0
      6c042000000-6c0421fffff : PCI Bus 0000:22
      6c042200000-6c0423fffff : PCI Bus 0000:23
      6c042400000-6c0425fffff : PCI Bus 0000:24
      6d000000000-6f041ffffff : PCI Bus 0000:20  <-- GPU in question
        6d000000000-6dfffffffff : 0000:20:00.0
        6e000000000-6efffffffff : 0000:20:00.0
        6f000000000-6f03fffffff : 0000:20:00.0
        6f040000000-6f041ffffff : 0000:20:00.0
```

Pretty cool! This confirms our mental model is accurate.

### Access Control Services (ACS)

If two devices live underneath the same switch, and if those two devices know the physical MMIO addresses assigned to each other, they can initiate a peer-to-peer PCIe transaction. Like we mentioned before, this may not be desirable when you have multiple tenancies on a node that need to be separated from each other. To enforce isolation, we remove the switch’s ability to perform direct peer-to-peer routing. This is done by enabling Access Control Services.

When ACS forces the transaction upstream, it reaches the root complex as a device-originated Memory Read/Write TLP. Because it originated from a PCIe endpoint, the transaction is subject to IOMMU checks. The IOMMU looks up the Requester ID (GPU A’s BDF), determines the associated IOMMU domain, and verifies whether the target physical address range (GPU B’s BAR window) is mapped in that domain. 

If the address is permitted, the transaction is routed back down the tree to GPU B. If not, the IOMMU blocks the request and raises a fault. Importantly, the IOMMU is enforcing access to physical address ranges, not abstract “device-to-device” permissions. Isolation emerges because each device’s domain maps only the memory and MMIO ranges it is allowed to access.

Let's first illustrate an example scenario. 

```title="" linenums="1"
        ┌──────────────────────────────────┐ 
        │           Root Complex           │ 
        │                                  │ 
        │       3: 0x4100       ┌───────┐  │ 
        │     ┌─────────────────►       │  │ 
        │     │       4: 0x4100 │ IOMMU │  │ 
        │     │         ┌───────┼       │  │ 
        │     │         │       └───────┘  │ 
        │    ┌┴─────────▼─┐                │ 
        │    │  Root Port │                │ 
        └────│────────────│────────────────┘ 
             │            │                  
             └─────▲────┬─┘                  
                   │    │                    
         2: 0x4100 │    │5: 0x4100           
                   │    │                    
           ┌───────┴────▼─────┐              
           │                  │              
           │                  │              
           │      Switch      │              
           │                  │              
           │                  │              
           └─────▲─────────┬──┘              
                 │         │                 
       1: 0x4100 │         │6: 0x4100        
                 │         │                 
┌────────────────┴───┐ ┌───▼────────────────┐
│                    │ │                    │
│ GPU A              │ │ GPU B              │
│                    │ │                    │
│ BDF: 0000:12:00.0  │ │ BDF: 0000:13:00.0  │
│ BAR: 0x4000-0x40FF │ │ BAR: 0x4100-0x41FF │
│                    │ │                    │
└────────────────────┘ └────────────────────┘
```

We go through each step of the process:

1. GPU A sends a TLP with a destination of `0x4100`. This address represents GPU B's base address (read more about [Base Address Registers](#base-address-registers) below). The switch receives the packet and sees that `0x4100` is on its downstream port, but ACS is enabled. So, it forwards it to step 2.
2. The packet arrives at the root complex via the root port.
3. Because the transaction originated from a PCIe endpoint, it is subject to IOMMU checks before further routing.
4. The IOMMU sees the TLP's Requester ID (RID) which is just GPU A's BDF address of `0000:12:00.0`. It maps this RID to a specific IOMMU Domain. It maps this RID to a specific IOMMU domain and verifies that the target physical address (0x4100) is mapped in that domain’s page tables. If this address maps to a valid region in this domain, it forwards the packet back down. In our case, the packet is allowed.
5. The packet makes it back down to the switch.
6. The switch sees an incoming packet for `0x4100` on the upstream port and forwards it to the appropriate downstream port. If the address had not been mapped into GPU A’s IOMMU domain, the IOMMU would have raised a fault and the transaction would never reach GPU B.

!!! tip

      In this example, the target address (`0x4100`) is already a host physical address corresponding to GPU B’s BAR. No IOVA to HPA translation is required. The IOMMU is performing a permission check, not remapping the address.

To reiterate, ACS doesn’t block traffic by itself; it forces certain peer-to-peer transactions to go upstream so the root complex/IOMMU can enforce policy. Without ACS, peer-to-peer transactions never reach the root complex, which means the IOMMU cannot enforce isolation. This is why ACS capability is a hard requirement for safe multi-tenant device passthrough.

## Memory Mapped IO (MMIO)

We've talked about how PCIe routing works, how P2P enforcement works via ACS, and the structure of TLP packets in order to gain an understand of how data flows in a PCIe fabric. Now we will explore how devices expose data to the fabric and what that interface looks like.

Historically, CPUs used what's called Port Mapped IO (PMIO) to talk to devices. In PMIO, CPUs issue special x86 instructions such as `IN` or `OUT` in to talk to ports that live in a separate address space from physical memory. This method of device communication had many inherent limitations. For one, the port address space was limited to 16 bits or 64,000 total ports. PMIO instructions could only operate on a small number of bytes at a time. It's also harder to virtualize because it requires trapping specific instructions and then emulating the behavior of that device. MMIO on the other hand looks like memory access and can be more easily emulated through page tables. Because I/O devices appear as memory locations, any instruction that manipulates memory can also manipulate a device register. This simplifies compiler design and allows for complex operations directly on the hardware, unlike the restrictive, specialized instructions required for PMIO.

Modern systems nowadays rely almost entirely on MMIO to interact with peripherals. The historic bifurcation of IO devices and main memory led to unnecessary complexities, and unifying them under a single instruction set, a single address space, and a single data structure (page tables) allowed compilers, virtualization technologies, and CPU design to all be simplified.

### Base Address Registers

All devices in PCIe expose a large set of registers that provide various information about the device itself, its capabilities, and how much addressable memory space it needs on the host system. These registers comprise a device's Configuration Space. You can see the set of common registers in the PCIe spec:

![PCIE config space registers](https://f005.backblazeb2.com/file/landons-blog/assets/posts/2026-02-15-pcie-mmio/pcie_config_register.png)

How does a device tell the system which MMIO ranges it needs? Through its configuration space, specifically, its Base Address Registers (BARs). When a system initially boots, the host firmware (BIOS, UEFI, OVMF etc) write all 1's to the Base Address Register. This prompts the device to then write an integer value that corresponds to the amount of bytes of address space it requires which is read back by the host firmware.[^5] The host will allocate this region in its address space and then write the base of this address back into the device's BAR. This is where the term BAR comes from because it stores the base address of the physical address space that has been allocated for it.

After the BAR has been written to with its final base address, the device will listen for PCIe transactions that fall within this BAR range. Transactions in this range will be claimed by the device, otherwise they will be ignored.

![BAR structure](https://f005.backblazeb2.com/file/landons-blog/assets/posts/2026-02-15-pcie-mmio/bar_structure.png)

It should be noted that BARs are not indicating to the host how much physical RAM they want to allocate. Instead, they are asking for a certain slice of the host's 64 bit address space to be reserved for them. This means if a device requests 2MiB of address space, the host firmware might allocate the address space `0x200000` to `0x400000` for this device. If the CPU ever performs a memory read or write within this address range, the request is directed away from RAM and instead routed into the PCIe topology and eventually the device itself. Indeed, this is the core principle of how MMIO works.

We can see an example of what this looks like using `lspci`:

```title="" linenums="1"
$ lspci -s 20:00.0 -vv
Region 0: Memory at 3f80000000 (64-bit, prefetchable) [size=16G]
Region 2: Memory at 3f00000000 (64-bit, non-prefetchable) [size=32M]
```

There are two regions of memory that are each described by a separate BAR. The first shows prefetchable memory of size 16G and the other is non-prefetchable control registers. Large prefetchable BARs (e.g., 16–64 GiB) often correspond to device memory apertures, such as GPU VRAM mappings.

!!! question "Prefetchable vs Non-Prefetchable"

      Prefetchable memory is a kind of memory that has the following properties:

      - Reads have no side effect
      - Data behaves like normal memory
      - Speculative reads are safe
      - Read merging/reordering is safe.

      This type of memory is how normal RAM operates. Non-prefetchable memory on the other hand can behave in unintuitive ways:

      - Reads may have side effects.
      - Strict ordering of reads is required.
      
      These slices of memory behave more like registers where the order in which you interact with it must be specific, and in some cases the mere act of reading can cause the underling data to mutate. Often, this type of memory is reserved for control registers.

      The distinction between these two tells the CPU and root complex how aggressively they are allowed to reorder or speculate on accesses to that region.

### MMIO is Not DMA

MMIO is a mechanism for the CPU to access a device, not for the device to access the CPU’s memory. To understand this conceptually, we must make a distinction between what is the physical address space of a system and the underlying RAM that may or may not back it. The address space of a 64 bit system is every value between `0x0000000000000000` and `0xFFFFFFFFFFFFFFFF`. When a system boots up, it's given an address map that looks something like this:

```title="" linenums="1"
0x00000000 - 0x7fffffff    -> RAM
0x80000000 - 0x8fffffff    -> PCIe MMIO
[...]
```

If the CPU makes an access to 0x10000000, the request is routed to the system memory controller because that address range is backed by RAM. If the CPU makes a request to `0x80000000`, the Root Complex forwards the address down the root port whose window encompasses this value. This does not grant the device access to system memory. Devices write to system memory only through DMA, which is separately controlled by the IOMMU.

It is important to keep the direction of the transaction in mind:

- **MMIO**: CPU -> device (based on physical address windows)
- **DMA**: device -> system memory (subject to IOMMU enforcement)

### System Memory Maps in Linux

You can view Linux's address map by inspecting the `/proc/iomem` special file (you need to read this file as root, otherwise you'll get all zeros). In a VM I'm running on my MacOS, we see in this file the MMIO regions allocated for the PCIe devices:

```title="" linenums="1"
10000000-1000ffff : GICD
10010000-1004ffff : GICR
20060000-20060fff : ARMH0061:00
  20060000-20060fff : ARMH0061:00 ARMH0061:00
40000000-4fffffff : PCI ECAM
50000000-6ffdffff : PCI Bus 0000:00
  50000000-50000fff : 0000:00:0f.0
    50000000-50000fff : xhci-hcd
  50001000-500013ff : 0000:00:0f.0
  50001400-5000147f : 0000:00:05.0
  50001480-500014ff : 0000:00:0a.0
  50001500-5000157f : 0000:00:0b.0
  50001580-500015ff : 0000:00:0c.0
  50001600-5000163f : 0000:00:01.0
  50001640-5000167f : 0000:00:06.0
  50001680-500016bf : 0000:00:07.0
  500016c0-500016ff : 0000:00:08.0
  50001700-5000173f : 0000:00:09.0
  50001740-5000177f : 0000:00:0d.0
  50001780-500017bf : 0000:00:0e.0
```

We also see the system RAM mapped to various regions:

```title="" linenums="1"
ee330000-ee33ffff : reserved
ee340000-ee56ffff : System RAM
  ee452000-ee452fff : reserved
ee570000-ee6fffff : reserved
ee700000-efafffff : System RAM
  ef58e000-efafffff : reserved
efb00000-efb8ffff : reserved
efb90000-efb9ffff : System RAM
  efb90000-efb9ffff : reserved
```

## Looking Forward

PCIe is a huge spec and there are a lot of interesting topics I won't cover here, but here are some other interesting concepts:

- [SR-IOV](https://en.wikipedia.org/wiki/Single-root_input/output_virtualization): A technology that allows PCIe devices to expose multiple endpoints at once, each with their own set of BARs. This is used as a type of device virtualization that is mediated by the device firmware itself.
- Address Translation Service: A distributed cache of the result of page table walks in the IOMMU. This speeds up DMA transactions.
- Device-level interrupts: how do devices tell each other and the CPU when they must be attended to?
- [Posted vs non-posted transactions](https://www.linkedin.com/pulse/pci-express-pcie-architecture-posted-non-posted-refer-io8gc/): Posted transactions are essentially like UDP packets that don't expect an acknowledgement, where non-posted is more like TCP where it requires a response.
- Resizable BARs.

We've learned in this post how the PCIe topology works, how packets are routed through the fabric, how the CPU talks to devices through MMIO, how devices safely perform DMA to the host memory, and how we can enforce P2P transactions via IOMMU Domains. This provides for us a solid foundation for understanding higher-level topics like GPU virtualization. In the next article, we'll explore more of the Linux kernel's perspective on device management and how it attaches drivers to PCIe endpoints. There, we will learn the common method for GPU interactions where NVIDIA kernel drivers are loaded directly into the host environment. We'll also learn what VFIO is and how it enables passthrough virtualization into VMs.

This article was a long one, and if you made it this far: congratulations! You've learned more about one of the most foundational technologies in modern computing that most software engineers gloss over. You are well positioned to understand the higher levels of abstractions we'll explore. As always, I invite comments and corrections using the Giscus comment system below. All you need is to link your GitHub account.

## References

- https://www.edn.com/the-pci-express-switch-and-bridge-landscape/
- https://stackoverflow.com/questions/49050847/how-is-pci-segmentdomain-related-to-multiple-host-bridgesor-root-bridges#:~:text=Unfortunately%2C%20the%20word%20PCI%20domain,port%20is%20a%20PCIe%20link.
- https://f005.backblazeb2.com/file/landons-blog/assets/posts/2026-02-15-pcie-mmio/pci-sig-sr-iov-primer-sr-iov-technology-paper.pdf
- https://www.semisaga.com/2019/07/pcie-tlp-header-packet-formats-address.html
- https://f005.backblazeb2.com/file/landons-blog/assets/posts/2026-02-15-pcie-mmio/pcie_spec_rev5.pdf


[^1]: Recall that I said before that traditional PCIe form factors for consumer-grade hardware can go up to 16 lanes, but these datacenter grade GPUs, which are often using SXM form factors, can go far beyond this limit!
[^2]: NVIDIA GPUs often support 48 bits to 52 bit IOVA widths, but other devices can support substantially smaller widths.
[^3]: An IOMMU Group is a Linux kernel concept that basically specifies which devices the kernel believes can be properly separated by ACS and which cannot. Devices that connect to a switch that doesn't have ACS capabilities cannot be isolated from each other, and will consequently be placed into the same IOMMU Group. This has specific implications. If two devices are placed into the same group and you try to pass only one of them into a VM, the kernel will refuse to let you do so. This is because you would be attempting to create an IOMMU Domain that does not have all of the members of that IOMMU Group present. This should make sense: devices which cannot be hardware-isolated from each other should not be shared amongst VMs. It's all or nothing.
[^4]: For more information on what this looks like, read the [Linux VFIO documentation](https://www.kernel.org/doc/html/v6.7/driver-api/vfio.html) and look for references for IOMMU Groups.
[^5]: BARs can be either 32 or 64 bit. If the BAR is 64 bit, bits `2:1` are set to `10b` to indicate as such.
