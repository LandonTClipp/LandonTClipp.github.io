{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to <code>topofmind.dev</code>","text":""},{"location":"#about-me","title":"About Me","text":"<p>My name is Landon Clipp and I am a software engineer who currently lives in Chicago, IL. I've worked in a number of different fields, including public research (mainly climatology), high frequency trading, and most recently in the public cloud building out high performance infrastructure for AI research.</p> <p>I'm passionate about a number of things, and consequently this blog will dive into many various topics. The two most interesting sections of this website are my Blog and also what I call my \"Professional\" Notes.</p>"},{"location":"#what-is-this-blog","title":"What is this Blog?","text":"<p>This blog is my dumping ground for every random thought, every interesting idea, and every useful thing I learn. It's not meant so much for others, but moreso as a tool for me to remember my experiences and to make sense of the million ideas that swirl around in my head. I wasn't born with an eidetic memory so my brain only has a certain amount of informational capacity. This blog, in a sense, is a cross-section of my mind at various points in time. Then again, isn't that true for all written word? By writing things down, I hope to enshrine things I've learned and to reinforce the pathways I want to encourage. The side-effect, I hope, is that you find it somehow useful as well </p> <ul> <li> <p></p> </li> <li> <p></p> </li> <li></li> <li> <p></p> </li> <li> <p></p> </li> </ul>"},{"location":"conventions/","title":"Conventions","text":""},{"location":"conventions/#optimization","title":"\u2013 Optimization","text":"<p>This section describes an optimization technique.</p>"},{"location":"blog/","title":"Index","text":""},{"location":"blog/#landons-personal-and-coding-blog","title":"Landon's Personal and Coding Blog","text":"<p>Welcome to my blog! This blog contains various blogs around coding, flying, RV-ing, and any other topic I may find useful to disseminate.</p> <p> </p>"},{"location":"blog/2018/08/21/aws-batch-extended-amis/","title":"AWS Batch Extended AMIs","text":"<p>AWS Batch provides an excellent way to process hundreds or thousands of batch jobs very easily, but the default configurations provided may not be enough for most users. This post will help address the limitations of the default Amazon Machine Image (AMI) used in the managed compute environment.</p> <p>First, here is a nice picture of some clouds to get you in the right mindset:</p> <p></p> <p>According to the main page of Amazon's AWS Batch:</p> <p>AWS Batch enables developers, scientists, and engineers to easily and efficiently run hundreds of thousands of batch computing jobs on AWS. AWS Batch dynamically provisions the optimal quantity and type of compute resources (e.g., CPU or memory optimized instances) based on the volume and specific resource requirements of the batch jobs submitted. With AWS Batch, there is no need to install and manage batch computing software or server clusters that you use to run your jobs, allowing you to focus on analyzing results and solving problems. AWS Batch plans, schedules, and executes your batch computing workloads across the full range of AWS compute services and features, such as Amazon EC2 and Spot Instances.</p> <p>For instance, say that you have a program that takes some input parameter <code>n</code> and gives you some result based on that number, but you want the ability to feed hundreds, perhaps thousands of different inputs into your program to compute the corresponding outputs. AWS Batch allows you to define this simple operation (for some <code>n</code>, compute the output) and will then take care of provisioning the necessary EC2 instances, cloud watch logs, error handling, and any other task necessary for you to run your workload.</p>"},{"location":"blog/2018/08/21/aws-batch-extended-amis/#the-problem","title":"The Problem","text":"<p>There are 4 primary steps that one must complete in order to run your first job on AWS Batch: 1. Define a compute environment. This includes selecting the IAM roles, your EC2 key pairs, the allowed EC2 instance types, networking configurations, autoscaling limits, etc. Essentially, you define the limits of your environment and what kind of hardware your VMs will be exposed to. 2. Define the job queue. Job queues act as the gatekeepers to your compute environments and decide which jobs to run and when based on the queues priority and hardware requirements. This is roughly analagous to a High Performance Computing (HPC) cluster's various queues that one might see, such as high priority queues that will give their tenants presidence over the jobs residing in lower priority queues. 3. Define the job definition. This step is optional, but it is useful if you have a job that you know you will run with different parameters in the future. This is arguably the most challenging step because you must create a Docker image for your program and decide how you want to interface with your program through the Docker ecosystem. The job definition is where you declare how many virtual CPUs (vCPU) you need, how much memory, the wallclock limit (called the \"Execution Timeout\"), IAM roles, environment variables, and various other configurations.. 4. Submit your job to a specific queue using your job definition.</p> <p>Depending on the kind of program you are running, the amount of control that AWS Batch directly gives you over defining your compute environment might not be enough. I run an atmospheric science program called Basic Fusion (BF) that requires over 100GB of disk space, but when I tried to run it on AWS batch, I realized that the VMs that were automatically provisioned to me did not include a large enough disk volume. According to one of the official responses on the AWS developer forums, the default Amazon Machine Image (AMI) only provides 8GB of root storage and 22GB for Docker and metadata storage. </p> <p>The recommended solution by Jamie is to use an unmanaged compute environment. Unfortunately this ended up being poor advice because not only is creating your own unmanaged compute environment difficult and esoteric, and not only does it kind-of defeat the entire purpose of AWS batch, but there is a much better (and much simpler) solution. Jamie had also promised in various other threads a blog post that would describe how to fix this problem, but she was never heard from again. So let me impart my knowledge to you!</p>"},{"location":"blog/2018/08/21/aws-batch-extended-amis/#the-solution","title":"The solution","text":"<p>The solution to this problem is to create an Amazon Machine Image that is dervied from the default AMI that AWS Batch uses. An AMI allows you to configure an operating system exactly how you want it by installing libraries, modifying startup scripts, customizing configuration files, and most importantly for our purposes: define the logical partitioning and mount points of data volumes. Let's get started.</p>"},{"location":"blog/2018/08/21/aws-batch-extended-amis/#step-1-choose-a-base-ami-to-start-from-configure-your-instance","title":"Step 1: Choose a base AMI to start from, configure your instance","text":"<p>The AMIs we want to base ourselves off of are the official ECS-optimized AMIs. Take a gander at this page to find which AMI you need according to the AWS region you are running on. I am running in us-east-2, so I will choose that one.</p> <p>After identifying your AMI, click the \"Launch instance\" link in the righthand column. You'll be taken to this page:</p> <p></p> <p>Choose the t2.micro instance type. NOTE that your AMI will not be married to the instance type you choose here, so don't agonize over this decision.</p> <p>Choose <code>Next: Configuration Details</code>.</p> <p>Give your instance an appropriate IAM role if desired. What constitutes \"appropriate\" is at your discretion. Leave the rest of the default options. Click <code>Next: Add Storage</code>.</p> <p>Now is where you can configure what your data volumes will look like on your AMI. This step also does not define the final volume configuration for your AMI, but I find it to be useful to configure this how you want. You will have the opportunity to change this later before you create your AMI. Once you're done, click <code>Next: Add Tags</code>.</p> <p>Add any tags that you want (optional). Click <code>Next: Configure Security Group</code>.</p> <p>Select <code>SSH</code> for <code>Type</code>, and set the <code>Source</code> to be <code>Anywhere</code>, or if you are more responsible than me, set a specific set of IP ranges that you know you will be using to connect to your instance. Click <code>Review and Launch</code>.</p> <p>This page will allow you to review the options you have set. If everything looks good, then <code>Launch</code>. When it asks for a keypair, either select and existing keypair you've created, or create a new one. Failing to do this step will leave you unable to connect to your instance.</p>"},{"location":"blog/2018/08/21/aws-batch-extended-amis/#step-2-configure-your-software-environment","title":"Step 2: Configure your software environment","text":"<p>After you have clicked launch, go to your EC2 dashboard to see your running instances:</p> <p></p> <p>Wait for your instance to start, then right click it. Click <code>Connect</code>, then copy-paste the Example ssh command into an ssh-capable terminal. The <code>-i \"keyname.pem\"</code> is actually a path to your .pem file, so make sure you either <code>cd</code> to your <code>~/.ssh</code> directory, or change the flag's value to be the path to where you stored your private SSH key. You also may need to change \"root\" to \"ec2-user\".</p> <p></p> <p>After you have logged in, you can configure your VM however you want by installing any packages, libraries, and configurations you need your VM to have. If you used the AWS-provided ECS-optimized AMI, your AMI will already meet the base requirements for an ECS AMI. If you for some (strange) reason choose not to use the ECS-optimized AMI, you will have install and configure the following packages:</p> <ol> <li>The latest version of the Amazon ECS container agent</li> <li>The latest version of the ecs-init agent</li> <li>The recommended version of Docker for your version of the ECS container agent</li> </ol> <p>Also note that if you want to attach another volume separate from your root volume, you will want to modify the <code>/etc/fstab</code> file so that your new volume is mounted on instance startup. I refer you to Google on how to do this.</p>"},{"location":"blog/2018/08/21/aws-batch-extended-amis/#step-3-save-your-ami","title":"Step 3: Save your AMI","text":"<p>After all of your software configuration and installation is done, go back to your EC2 dashboard and view your running instances.</p> <p>Right click on the instance you just made. Hover over <code>Image</code>, then select <code>Create Image</code>.</p> <p></p> <p>You'll see that this has the volume configuration you chose in Step 1. I did not change my volumes from their default settings, so you can see in the screenshot above that the default volumes for the ECS-optimized AMI are in fact 8GB for <code>/dev/xvda/</code> (root), and 22GB for <code>/dev/xvdc/</code> (docker images, etc). Ensure that the <code>Delete on Termination</code> options are selected so that your Batch compute environment removes the volumes after the instances are terminated, or else you'll risk creating an unbounded number of EBS volumes (very expensive, so I'm told). I will configure my AMI to have only 111GB of root storage, and nothing else. You do not necessarily need a separate volume for Docker.</p> <p>Give your image a name and description, then select <code>Create Image</code>.</p> <p>Your instance will be rebooted. Once the instance is turned off, AWS will create an image of it, then turn the instance back on. </p> <p>In your EC2 console, go to <code>Images, AMIs</code> on the left hand side. After a few minutes, you should see your newly created AMI in the list.</p> <p></p> <p>If you launch a new instance with this AMI, it should be automatically configured with the data volumes you specified.</p>"},{"location":"blog/2018/08/21/aws-batch-extended-amis/#step-4-configure-aws-batch-to-use-your-new-ami","title":"Step 4: Configure AWS Batch to use your new AMI","text":"<p>Go back to your AWS dashboard and navigate to the AWS Batch page. Select <code>Compute environments</code> on the left hand side. Select <code>Create environment</code>.</p> <p></p> <p>Configure your environment by selecting the appropriate IAM roles for both your container (Service Role) and EC2 instance (Instance role), provisioning model, networking, and tags. Here are the options I've specified for this tutorial:</p> Option Value Compute environment type Managed Compute environment name ami_test Service role AWSBatchServiceRole Instance role ecsInstanceRole EC2 key pair landonkey.pem (use name of your private key) Provisioning model On-Demand (choose spot for significantly cheaper provisioning) Allowed instance types Optimal Minimum vCPUs 0 Desired vCPUs 0 Maximum vCPUs 256 Enable user-specified Ami ID True AMI ID [ID of AMI you generated] VPC id [default value] Subnets [select all options] Security groups default <p>The critical step for this tutorial is selecting <code>Enable user-specified Ami ID</code> and specifying the AMI ID you generated in previous steps.</p> <p>Once all of your options are configured, select <code>Create</code>.</p>"},{"location":"blog/2018/08/21/aws-batch-extended-amis/#step-5-create-job-queues-and-job-definitions","title":"Step 5: Create job queues and job definitions","text":"<p>In order to test that our compute environment actually works, let's go ahead and create some simple queues and job definitions.</p> <p>Select <code>Job queues</code> on the left hand side and enter the following options:</p> Option Value Queue name ami_test_queue Priority 1 Enable Job queue True Select a compute environment ami_test <p>Select <code>Create</code>. Wait for the Status on your new queue to be <code>VALID</code>.</p> <p>Go to <code>Job definitions</code> and select <code>Create</code>. Enter the following values:</p> Option Value Job definition name ami_test_job_def Job role ECS_Administrator Container image amazonlinux Command df -h vCPUs 1 Memory (MiB) 1000 Job attempts 1 Execution timeout 100 Parameters [leave blank] Environment variables [leave blank] Volumes [leave blank] Mount points [leave blank] <p>Select <code>Create job definition</code>.</p> <p>Finally, go to <code>Jobs</code> on the left hand side and select <code>Submit job</code>. Give your job a name and select the <code>ami_test_job_def:1</code> for the Job definition. Leave the rest of the default values and select <code>Submit job</code>.</p> <p>If all goes well, you should now see that your job has entered either the <code>Pending</code> or <code>Runnable</code> states. Note that it may take upwards of 10 minutes before your job actually runs. The EC2 instance typically takes 5-10 minutes to be instantiated, and a few more minutes to pass status checks. If your job continues to be in the <code>Runnable</code> state after the instance has been created and has passed all status checks, something has gone wrong.</p> <p>Luckily for me, it looks like my job ran successfully.</p> <p></p> <p>Looking at the CloudWatch output logs, we can see that the container has visibility to a ~111GB partition, and it is mounted on the container's root.</p> <p></p>"},{"location":"blog/2018/08/21/aws-batch-extended-amis/#list-of-iam-roles-used","title":"List of IAM roles used","text":"<p>I'll also list the IAM roles I used so that you can exactly replicate the policies that my resources are given. </p> <p>Disclaimer: I explicitly do not condone giving any of your resources full administrative power! I only gave a few of my resources administrative power purely because I do not fully understand which policies are needed in certain circumstances, so I am naively doing the \"Nuclear Bomb\" approach of just granting them everything. I fully recognize that this is bad practice, so do as I say, not as I do! I would appreciate help in determining exactly which policies I need, so submit a GitHub issue if you know.</p> Role name Attached policies AWSBatchServiceRole AWSBatchServiceRole ecsInstanceRole CloudWatchAgentServerPolicy : CloudWatchFullAccess : CloudWatchLogsFullAccess : AmazonEC2ContainerServiceFullAccess : AmazonEC2ContainerSErviceforEC2Role : CloudWatchEventsFullAccess ECS_Administrator AdministratorAccess"},{"location":"blog/2018/08/21/aws-batch-extended-amis/#troubleshooting","title":"Troubleshooting","text":"<p>The most common issue you will run into is your jobs getting stuck in the <code>Runnable</code> state. Some things you can try to fix this:</p> <ul> <li>For each of the IAM roles you assign, go through each role, one at a time, and attach the AdministratorAccess policy to them. This includes the IAM roles on your compute environment and your job definitions. Then, restart your compute environments, job queues, and jobs to see if they will run. If your job successfully runs after changing one of the roles, you know that you must have insufficient permissions on that role.</li> <li>Create a separate compute environment that does not have a user-specified AMI. I have found in some cases that reverting back to the Batch-managed ECS-optimized AMI allowed my jobs to run, indicating that there must have been something wrong with the AMI I created. I strongly recommend everyone base their AMI off of the ECS-optimized AMIs, because these have all of the daemons necessary to be integrated in an ECS cluster, as well as daemons that send metrics and logs to CloudWatch.</li> <li>If you used a security policy that only allows incoming IPs from a specific range, try allowing all IP ranges in your policy. One clue that may tip you off is if you are unable to manually connect through SHH to any of the instances created by Batch.</li> <li>Re-check that your job definition is not requesting more resources than what your compute environment can provide. Setting the allowed instances to Optimal will usually allow most jobs to be run, but if you selected only specific allowed instances, your job may simply be too big.</li> </ul>"},{"location":"blog/2018/08/21/aws-batch-extended-amis/#limitations-and-alternatives","title":"Limitations and alternatives","text":"<p>The method described in this blog only allows you to create statically-sized data volumes for your instances. This means that even if you intend for each container process to have access to 100GB of storage, it is possible that your processes will end up competing with other processes for the same 100GB. This is because AWS Batch often places multiple container processes on the same EC2 instance if there is enough space. If you chose <code>Optimal</code> for your allowed instance type, this means that Batch could possibly choose some EC2 type capable of hosting dozens of container instances, thus splitting up your storage amongst all of them.</p> <p>The way that I get around this is by manually specifying which instances I allow so that I know exactly how Batch will partition the instance, as well as how many vCPUs and how much memory my jobs can request. If you choose the r5.large instance, you know for a fact that no more than two single-process container jobs will ever run on it.</p> <p>One of the alternatives suggested on the AWS developer forums is to use AWS Lambda to dynamically create and mount data volumes on the instances created by Batch. This gives you more flexibility to create appropriately-sized volumes for a dynamically chosen instance type (in the case of the <code>Optimal</code> option). However, you have to decide for yourself if this extra effort is worth it, keeping in mind that Occam's razor: \"the simpler solution is usually the better one.\"</p>"},{"location":"blog/2018/08/21/aws-batch-extended-amis/#suggestions","title":"Suggestions","text":"<p>If you find anything wrong with this blog post (and I can guarantee you that something is wrong), please post an issue on my github.io repository. Click the GitHub icon at the bottom of this page to visit the repo.</p>"},{"location":"blog/2018/08/21/aws-batch-extended-amis/#useful-links","title":"Useful links","text":"<ul> <li>https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-introduction-part-1-of-4/</li> </ul>"},{"location":"blog/2023/06/21/intech-sol-horizon-cellular-setup/","title":"Intech Sol Horizon Cellular Setup","text":"<p>In 2022, I bought myself an Intech Sol Horizon Rover. I had been looking at RVs for a long time and decided on this particular manufacturer because of the excellent build quality and reputation of the manufacturer. The Horizon was a good fit for my needs because it was spacious enough to be comfortable for two adults to live in for long periods of time, and small enough that I could tow it with my Jeep Grand Cherokee. My goal for this RV is to be something that I could work remotely from, which means I needed to get a reliable internet setup. This post will show you my journey with finding the right internet solution, the lessons I learned, and the resources I used to make this a successful build.</p>"},{"location":"blog/2023/06/21/intech-sol-horizon-cellular-setup/#the-rig","title":"The Rig","text":""},{"location":"blog/2023/06/21/intech-sol-horizon-cellular-setup/#categories-of-cellular-installations","title":"Categories of Cellular Installations","text":"<p>After a lot of researching online, I found that there are two main categories of cellular internet installations:</p>"},{"location":"blog/2023/06/21/intech-sol-horizon-cellular-setup/#mobile-hotspots","title":"Mobile Hotspots","text":"<p>Mobile hotspots work by receiving a cellular signal and exposing a WiFi access point inside your trailer. Most modern cellphones have a hotspot feature, but you can also buy dedicated hotspot modems like the Verizon Jetpack or the AT&amp;T Nighthawk.</p>"},{"location":"blog/2023/06/21/intech-sol-horizon-cellular-setup/#pros","title":"Pros","text":"<ul> <li>Easy to set up. You purchase a pre-packaged plan from the provider and they ship you a device that will work out of the box</li> <li>Easy to use. There is minimal wiring that needs to be setup. Usually you'll only need to plug the device into an AC outlet and any external antennas that may be provided.</li> <li>Portable. You don't have to confine the device to just your RV. You can take it anywhere you want.</li> <li>Heavy duty cellular boosters like the Weboost Destination RV are very easy to setup and can greatly increase the range of the booster with minimal effort.</li> </ul>"},{"location":"blog/2023/06/21/intech-sol-horizon-cellular-setup/#cons","title":"Cons","text":"<ul> <li>They are not meant for long-term heavy use. There have been reports of the devices overheating due to excessive use if you are full-timing.</li> <li>Poor cellular reception. In areas of low-quality cell service, the internal antennas will often be insufficient. Most of the window-mounted antennas are also inadequate. Usage of an external cellular booster like the Weboost Destination RV can help mitigate this.</li> <li>You're often tied to one carrier. Some devices can only be used with one carrier (not all of them are like this, but most are in my experience).</li> </ul>"},{"location":"blog/2023/06/21/intech-sol-horizon-cellular-setup/#dedicated-cellular-routers","title":"Dedicated Cellular Routers","text":"<p>A cellular router is a more robust and permanent platform. A router like the Peplink MAX Transit Duo are powerful devices that are capable of bonding multiple sources of internet into a single WiFi access point. The MAX Transit Duo, for example, is capable of bonding two separate cellular SIM cards and external WiFi signals (like what campgrounds offer) into a single access point. This is a form of multi-homing. Some of the more advanced Peplink models even allow bonding to a Starlink signal, providing you with quadruple internet redundancy.</p>"},{"location":"blog/2023/06/21/intech-sol-horizon-cellular-setup/#pros_1","title":"Pros","text":"<ul> <li>Carrier-agnostic. You won't be tied to any particular carrier and can independently swap out SIM cards any time you want.</li> <li>WAN Bonding. You can bond with any kind of internet signal, like from starlink, to provide multiple redundancy paths.</li> <li>Redundancy. Because of the WAN bonding, the multi-SIM Peplink devices will automatically and transparently failover to other internet sources. You also get cellular coverage redundancy, meaning if one carrier doesn't have coverage in a particular area, another carrier might. You would not have to configure anything for the failover to happen, the Peplink device would do it automatically.</li> <li>Energy efficient. The peplink devices can be wired directly to your 12V system which means you won't need an inverter to convert the battery DC to AC, and then another wall AC adapter to convert the AC back to DC. This DC-&gt;AC-&gt;DC conversion wastes lots of energy as heat.</li> <li>Options for high quality external antenna mounts. The devices can be plugged directly into a permanently mounted external omnidirectional antenna on your roof that provides large dB gains to your signal. You could also theoretically connect the directional antenna from the Weboost Destination RV into a Peplink router for situations where the omnidirectional antenna doesn't provide enough dB gain.</li> </ul>"},{"location":"blog/2023/06/21/intech-sol-horizon-cellular-setup/#cons_1","title":"Cons","text":"<ul> <li>More expensive. Most of the multi-WAN bonding devices start at around $1000 and quickly increase depending on the options you want.</li> <li>More complex. Careful consideration has to be taken on the physical installation with antenna placement, wire routing, and drilling. The devices themselves also tend to require manual configuration in order to work with the carriers and bands you need. For the Intech Sol Horizon specifically (as I'll elaborate on later), the antenna itself could not be drilled anywhere near the AC unit which means I had to figure out how to use a magnetic mount on a fiberglass roof.</li> </ul>"},{"location":"blog/2023/06/21/intech-sol-horizon-cellular-setup/#weboost-destination-rv","title":"Weboost Destination RV","text":"<p>My goal for my internet setup was ultimate redundancy. I wanted a setup where I could use my phone's internal hotspot for internet, as well as access to a directional antenna that will be powerful enough to punch through areas with incredibly weak signal. The Weboost Destination RV was a great choice because it required minimal setup and works seamlessly with all cellular devices that have their own internal antennas.</p> <p>Here is a general diagram of how it works:</p> <p></p> <p>An external antenna is mounted on a telescoping pole that you attach to the side of your RV using 3M tape. The antenna is connected to a signal repeater on the inside of the RV that boosts the signal to a higher dB. The repeater is then connected to a low-gain internal cellular antenna that your devices will interact with.</p> <p>The Destination RV works great with the Intech Sol Horizon because you can re-use the existing external coaxial port on the left side of the RV for running your cellular signal. This port runs behind the TV over the bed and is meant for routing a satellite antenna to the TV, but works perfectly well for a cellular signal as well.</p> <p></p> <p>You can then use simple 3M velcro tape to adhere the internal repeater box behind the TV and connect it to the coax port labelled <code>Satellite</code>. I strongly recommend using velcro 3M tape instead of permanently mounting with nails because you might find you need to readjust the positioning of your equipment. Connect the repeater to the AC plug using the provided AC adapter.</p> <p></p> <p>Obviously you'll want to clean up the wires like with a 3M cable hook or a coax staple. Once the repeater has been  mounted and connected, you need to find a place to put the internal antenna. Initially, I decided to place the antenna above the bed as seen here:</p> <p></p> <p>But I found this didn't work because the repeater box was flashing green and red which indicated that the internal and external antenna were getting to a runaway feedback loop condition, which caused the repeater to reduce the output power of the external antenna. This is a bit like putting a microphone up to its speaker. The feedback causes runaway noise and can only be fixed by reducing the volume (which is what happens when the repeater indicates a flashing red/green) or by moving the microphone away.</p> <p>I found that placing the antenna on the floor next to the bed was an easy option that didn't require any major cable routing. The feedback condition was removed and the signal as reported by my phone did indeed improve.</p> <p></p> <p>The bundled weBoost app is great for guiding you through the setup of pointing the antenna in the right direction and measuring the signal dB as reported by your phone. You can access the internal cellular metrics of your phone by dialing <code>*3001#12345#*</code>.</p>"},{"location":"blog/2023/06/21/intech-sol-horizon-cellular-setup/#peplink-cellular-router","title":"Peplink Cellular Router","text":"<p>The next project I tackled was adding a dedicated cellular router. I chose to use a prepackaged bundle from MobileMustHave.com. MobileMustHave is a reseller of cellular components and offers videos, articles, and ticket support to help you install one of their pre-packaged solutions.</p>"},{"location":"blog/2023/06/21/intech-sol-horizon-cellular-setup/#tools-and-supplies-used","title":"Tools and supplies used","text":"tool notes electric drill 1\" hole saw bit The 3/4\" cable glands need a 1\" hole in the junction box. 3/4\" hole saw bit For all the internal holes you will route the cables through 3/4\" cable glands Dicor self leveling sealant 24\"x36\" galvanized steel sheet Electrical junction box I did not buy directly, this was installed by the solar installers MobileMustHave Ultimate Road Warrior VR2 w/ Parsec Husky Pro 15ft Magnetic Mount Crimping Tool Any crimping tool for standard ~18 AWG power cables will do Ring connector crimps This is to attach the Peplink router DC cable to your battery terminals"},{"location":"blog/2023/06/21/intech-sol-horizon-cellular-setup/#prior-considerations","title":"Prior considerations","text":"<p>Before installing this system, we have to consider a number of factors.</p>"},{"location":"blog/2023/06/21/intech-sol-horizon-cellular-setup/#to-5g-or-not-to-5g","title":"To 5G or not to 5G?","text":"<p>5G is a new standard that operates in both sub-6GHz (called Frequency Range 1, or FR1) and 24.25 GHz to 71.0 GHz (FR2). FR2 is a microwave band that provides significantly faster download speeds, but in my experience the upload speed gains have been unimpressive. I've found through my own un-scientific tests that upload speeds for 5G in general are lackluster, which is an issue for video conferencing.</p> <p>Because of the somewhat untested and experimental nature of 5G, I decided to go with a 4G solution as it provides plenty of bandwidth for my needs.</p>"},{"location":"blog/2023/06/21/intech-sol-horizon-cellular-setup/#cellular-providers","title":"Cellular providers","text":"<p>The feature I'm optimising for in my choice of cellular providers is the coverage. The FCC publishes a coverage map for each provider:</p> <p>https://fcc.maps.arcgis.com/apps/webappviewer/index.html?id=6c1b2e73d9d749cdb7bc88a0d1bdd25b</p> <p>Here's an image of each coverage map, overlayed with various combinations of providers:</p> T-Mobile Verizon AT&amp;T Image <p>You can see that T-Mobile by far offers the worst overall coverage: their towers are concentrated along interstates and highly populated regions, but beyond those areas it becomes pretty sparse. Verizon and AT&amp;T offer fairly similar coverage out west, but there are many areas where only one of them provides coverage.</p> <p>Using this knowledge, it suggests that the best solution is to acquire data plans for both AT&amp;T and Verizon simultaneously. Using T-Mobile provides no advantage beyond a few select regions in the west.</p>"},{"location":"blog/2023/06/21/intech-sol-horizon-cellular-setup/#mobilemusthave-bundles","title":"MobileMustHave bundles","text":"<p>I ended up choosing the Ultimate Road Warrior VR2 bundle. It's a 4G-only solution that allows dual SIM card usage so you can multiplex two carriers at once. MMH also sells their own data plans that come in various flavors. I had to ask them which plan was carried by which provider, as their documentation didn't seem to say.</p>"},{"location":"blog/2023/06/21/intech-sol-horizon-cellular-setup/#mobilemusthave-data-plans","title":"MobileMustHave Data Plans","text":"Plan name Data cap Speed Cap 2023 Cost Carrier P1000 1000GB None $175 T-Mobile R300 300GB 25Mbps $129 Verizon R300+ 300GB 50Mbps $175 Verizon <p>But I want AT&amp;T </p> <p>MMH discontinued their AT&amp;T offerings for reasons unknown, but you can still purchase AT&amp;T sim cards from various online vendors. Many folks have had success on eBay where many companies sell AT&amp;T plans under their own business plan. I bought mine from cellphonex and I have a legitimately unlimited AT&amp;T plan (it does in fact work) for only $100 a month.</p> <p>I ended up purchasing their R300 plan to have Verizon coverage to use in tandem with my AT&amp;T plan from eBay.</p>"},{"location":"blog/2023/06/21/intech-sol-horizon-cellular-setup/#installation","title":"Installation","text":""},{"location":"blog/2023/06/21/intech-sol-horizon-cellular-setup/#roof-mount","title":"Roof mount","text":"<p>The Parsec Husky 7-in-1 antenna has an adhesive mounting option and a magnetic mount option. I initially wanted the adhesive mount option but I realized that it comes with a large bolt on the bottom that you have to put directly over a hole. There is no way to have the cables run laterally along the roof, it has to go directly on top of a mounting hole.</p> <p>Instead, I went with the magnetic mount option, as the magnetic base gives the cables room to run laterally along the roof. MMH support recommended I adhere a galvanized steel sheet to the roof, which would give the magnetic mount the ferrous base it needs. Galvanized steel is resistant to rust and should last a very long time.</p> <p></p> <p>I applied a liberal amount of dicor self leveling sealant to the bottom of the sheet and stuck it to the roof. Dicor is not technically supposed to be used as an adhesive but I thought it should be fine since the sheet itself will not have to resist much upwards force. The better solution is to use 3M VHB but hindsight is 20/20 and all. I applied Dicor along the edges of sheet, ensuring that there were no gaps that water could intrude underneath the plate.</p> <p>A few months prior, I had solar experts install a solar panel system on the RV and they had pre-drilled a junction box to the roof. This enters into the microwave cavity and routes the power cables along a somewhat complicated path behind the fridge. All I need to do is route the antenna cables to this junction box and follow a similar path to the router.</p> <p>I used a 1\" hole saw to drill a hole into the junction box. I installed a 3/4\" cable glands that would serve as a water resistent entry point for the cables.</p> <p>Warning</p> <p>Cable glands are typically meant to only have a single cable passed through. Having multiple cables passing through weakens the water resistence of the hand-tightened seal. You might want to consider adding a small bit of expanding foam inside of the gland to make it truly watertight. I opted not to add the foam (yet), but I will be monitoring the box for any water intrusion during heavy rain. I will add expanding foam if I notice water intruding into the box.</p>"},{"location":"blog/2023/06/21/intech-sol-horizon-cellular-setup/#cable-routing","title":"Cable routing","text":"<p>Originally, I wanted to route the antenna cables across the dinette and into the righthand storage compartment like this:</p> <p></p> <p>However even with the longest cable option of the Parsec Husky antenna, there's not enough length to do this. So instead I decided I would mount the router next to the fuse box.</p> <p>Now that the cables were inside, the question now became how to route the cables from the microwave compartment into the electronic compartment just below. I asked the Intech Sol Horizon Facebook group about how they've previously done this. The best suggestion was that you can drill into the exterior shell of the shower surrounds. there is a fairly large gap between the interior shower surrounds fiberglass and the exterior, which is mainly where a lot of plumbing and electrical systems are routed through. If you take off the radio, you can see this more clearly:</p> <p>The factory-cut hole in the radio comparment is large enough that you can fit your whole arm in it, which should make it easy to fish the antenna cables through. I cut a 3/4\" hole and put the cables through that hole, then fished them into the radio compartment.</p> <p>I toyed around with how to place the router on the wall. I wanted it to be as close to the fuse box as possible to save space and eventually found the best place to drill another 3/4\" hole for the cables. After that, it's pretty easy to connect the cables to the router. Just make sure you follow MMH's directions on where to plug each cable.</p> <p>The Peplink router comes with a DC connection or you can plug it into the AC outlet. It's generally not recommended to use the AC outlet for electronics if it can be avoided because you incur a huge efficiency penalty when you invert the battery's DC into AC, just to convert it back to DC again at the wall outlet. Using a simple crimping tool and a ring connector, I attached the DC cable directly to the battery hubs.</p> <p>Note</p> <p>The better solution for the DC connection is to attach it to the power cut off switch instead of directly to the battery. The Peplink router consumes energy even when not in use, so having the ability to quickly shut it off along with all the other electronics in your camper will save your battery.</p> <p>After drilling the router into the wall, plug in your SIM cards and the DC power connection. Now the phyiscal installation is finally done! Whew, that was a lot! </p> <p></p>"},{"location":"blog/2023/06/21/intech-sol-horizon-cellular-setup/#configuration","title":"Configuration","text":"<p>Now that the router is connected to the external antenna and powered up, it's time to log in! The default credentials are posted on the back of the router. Connect your computer to the WiFi access point created by the Peplink router. Go to https://192.168.50.1 (or whatever IP address the router lists) to log into the management console. The default username and password is <code>admin</code>. You'll be taken to page that roughly looks like this:</p> <p></p> <p>The <code>Cellular 1</code>/<code>Cellular 2</code> status may take a couple minutes to update. If you're using a Verizon SIM card, you need to configure it. Click on <code>Details</code> to bring up its configuration page. There are a few settings I like to set manually. First, change the <code>Network Mode</code> from <code>auto</code> to <code>Verizon Wireless</code> (or your respsective carrier).</p> <p></p> <p>Verizon-specific steps</p> <p>The following steps should only be done for Verizon Wireless. Go to the <code>Cellular Settings</code> section:</p> <ol> <li>Set <code>SIM Card</code> to <code>Both SIMs</code>. NOTE: each individual SIM card slot appears as TWO SIM cards to the Peplink software. This section here is not referring to both physical SIM cards. Even though this appears to be configuring both SIM cards, you're only configuring one at a time.</li> <li>Set <code>Preferred SIM card</code> to <code>No preference</code>.</li> <li>Check <code>Data Roaming</code>.</li> <li>In <code>Operator Settings</code> click <code>Custom</code>.</li> <li>Set the <code>APN</code> field to <code>vzwinternet</code>.</li> <li>Optional: set a bandwidth monitor on both columns.</li> </ol> <p>Warning</p> <p>This device has two SIM cards. When you go to the <code>Details</code> page of <code>Cellular 1</code>/<code>Cellular 2</code>, it gives you two columns for <code>SIM A</code>/<code>SIM B</code>. These are not referring to the two physical SIM card slots. Confusingly, the configuration for each physical slot allows you to define settings for TWO SIM cards. One of these columns will refer to a SIM card that simply doesn't exist. To reduce the risk of misconfigurations, please ensure both columns are identical.</p> <p></p>"},{"location":"blog/2023/06/21/intech-sol-horizon-cellular-setup/#speedfusion-cloud","title":"SpeedFusion Cloud","text":"<p>We're about to get into a bunch of networking jargon so feel free to skip this step if you are not super technical. Here's their official video explaining the technology. It's worth a watch.</p> <p>SpeedFusion Cloud is a service that Peplink provides that allows you to multiplex your data connection between all of the ISPs your router is connected to. In my setup, I have Verizon and AT&amp;T as my two ISPs. The idea of SpeedFusion is that it acts somewhat as a VPN where the connection from each ISP is tunnelled into one of SpeedFusion's cloud datacenters. Their cloud infrastructure will combine these two connections into a single connection that is ultimately used to communicate with the end services you're using. The idea is that their cloud system is able to detect if one of your WAN connections is suffering and will automatically reroute traffic through the other connection. This allows for automatic failover behavior.</p>"},{"location":"blog/2023/06/21/intech-sol-horizon-cellular-setup/#wan-smoothing","title":"WAN Smoothing","text":"<p>Their WAN smoothing technology works by duplicating your upload traffic through two or more WAN connections. Their cloud service will arbitrate these two connections and if a particular sequence number from the UDP connection is dropped from one stream, it will attempt to find the missing sequence from the other stream. This allows double redundancy over lossy and unreliable cellular connections that are often traversing microwave links (especially when you are at a remote location where your local cell tower does not have access to a wired internet connection).</p> <p>The drawback of this scheme is that it increases both your latency (due to the fact that an intermediary service has to multiplex two WAN streams into one) and your overall data consumption (due to the packets having to be sent twice). It may be worth the investment if video conferencing is critical to your job.</p>"},{"location":"blog/2023/06/21/intech-sol-horizon-cellular-setup/#results","title":"Results","text":"<p>The results were simultaneously phenomenal and underwhelming. The first speed test I ran was under my Verizon plan which I expected would do very well, but it simply 'ed the bed:</p> <p></p> <p>The download speed is so bad that I was half convinced that I had installed something wrong. The AT&amp;T results on the other hand faired much better:</p> <p></p> <p>Throughout my tests on the road, the same story tends to play out where Verizon performs \"okay-ish\" and AT&amp;T performs well. This isn't entirely unexpected as my Verizon plan is rate limited to 25Mbps and the AT&amp;T plan is truly unlimited. The Peplink router provides WAN signal quality graphs and I've found that in general, AT&amp;T is capable of better Signal-to-Noise Ratios (SNR) in remote areas out west. Even so, a good SNR number is not indicative of a more reliable connection as it also depends on the provider's access to wired internet (meaning the quality of their microwave relays matters a lot), how congested the local tower is, what path the microwave relays take, the weather, and what bands your local tower supports.</p>"},{"location":"blog/2023/06/21/intech-sol-horizon-cellular-setup/#conclusion","title":"Conclusion","text":"<p>Hopefully this post was useful to other Sol Horizon owners that are looking to add a more robust cellular installation to their rig. My current goal in life is to be able to travel the US in my rig while working in remote areas, and this project was the last piece I needed to make that dream a reality.</p> <p>Let me know your thoughts below (you'll have to create a GitHub account to comment), thanks!</p>"},{"location":"blog/2023/06/21/introducing-mockerys-new-packages-feature/","title":"Macro Syntax Error","text":"<p>File: <code>blog/posts/2023-06-21-mockery-packages-feature.md</code></p> <p>Line 120 in Markdown file: unexpected '.' Markdown<pre><code>dir: \"{{.InterfaceDir}}\" # (1)!\n</code></pre></p>"},{"location":"blog/2023/06/22/prefer-systemd-timers-over-cron/","title":"Prefer Systemd Timers Over Cron","text":"<p>Systemd administrators often find themselves needing to run services on their bare-metal machines. Services can be broken down into roughly two broad categories:</p> <ol> <li>Long-running services that are started once and will run for the lifetime of the machine.</li> <li>Short-running services that are started at least once and will run for a short amount of time.</li> </ol> <p>Long-running services comprise of the majority of the services in use by Linux. One of the challenging aspects of long-running services in a production environment is the dual question of monitoring and reliability: </p> <ol> <li>How do you know if your service is running? </li> <li>How will you be alerted if your service dies?</li> <li>How do you handle automatic retries should the service die?</li> <li>How do you enable automatic start-up of your services when a machine boots, and how do you make them start up in the right order?</li> </ol> containers are great too <p>This post only looks at non-virtualized, non-containerized services. Many excellent solutions exist if you have a container management system like Kubernetes or any of the cloud-hosted systems like Amazon ECS.</p>"},{"location":"blog/2023/06/22/prefer-systemd-timers-over-cron/#running-short-lived-services-as-a-cron-job","title":"Running short-lived services as a cron job","text":"<p>The default suggestion is to create a cron job. cron is a simple utility in Linux for running commands periodically as defined in a cron table, or crontab for short.</p> <pre><code>0 16 * * * /usr/bin/echo \"hello world\" # (1)!\n0 12 * * 6 /usr/bin/curl https://www.google.com &gt;/tmp/google_html.txt &amp;2&gt;/dev/null # (2)!\n</code></pre> <ol> <li>This line specifies that the command should run at 16:00 every day of the month, on every month of the year, and on every day of the week.</li> <li>This line specifies that the command should run at 12:00 every day of the month, on every month of the year, but only on Saturdays. Note that the day-of-the-week specifier is AND-ed with the day-of-the-month specifier.</li> </ol>"},{"location":"blog/2023/06/22/prefer-systemd-timers-over-cron/#benefits","title":"Benefits","text":"<ol> <li>This requires minimal setup. Most linux distributions pre-install crond which allows you to define user-level crontabs with no prior configuration.</li> <li>It's easy to use. The line in the crontab is exactly what will be run. There is no templating system in use, so you generally won't have to worry about escaping special values.</li> </ol>"},{"location":"blog/2023/06/22/prefer-systemd-timers-over-cron/#drawbacks","title":"Drawbacks","text":"<ol> <li>There is no easy way to determine if the command completed successfully.</li> <li>There is no easy way to know if the command was run at all.</li> <li>Log management is a pain. Your only solution for logging stdout/stderr is to redirect it to a file, however you'd also need to ensure you rotate the logs to ensure the log's size is bounded.</li> <li>There is no way to specify if you want the command to be retried, should it fail.</li> <li>It's not easy to get a quick glance of how long each line has until its next execution. Reasoning about all of your cron lines as a whole is basically impossible.</li> </ol> <p>These drawbacks make a crontab wholly unsuitable for a reliable production system.</p>"},{"location":"blog/2023/06/22/prefer-systemd-timers-over-cron/#running-short-lived-services-as-a-systemd-timer","title":"Running short-lived services as a systemd timer","text":"<p>Let's take for example a service that needs to periodically scrape a user's home directory and send the total list of files to an external logging backend.</p> Bash<pre><code>$ find . -type f\n./file2.dat\n./subdir/file3.dat\n./file1.dat\n</code></pre> <p>We can use <code>ncdu</code> to give us a nice JSON representation of the file tree with each directory element's size. We pipe the JSON output to <code>jq</code> to format it into a more readable state.</p> Bash<pre><code>ubuntu@lclipp:~/systemd_blog$ ncdu . -o - | jq .\n[\n  1,\n  0,\n  {\n    \"progname\": \"ncdu\",\n    \"progver\": \"1.11\",\n    \"timestamp\": 1687459411\n  },\n  [\n    {\n      \"name\": \"/home/ubuntu/systemd_blog\",\n      \"asize\": 4096,\n      \"dsize\": 4096,\n      \"dev\": 2049,\n      \"ino\": 259370\n    },\n    {\n      \"name\": \"file2.dat\",\n      \"asize\": 2097152,\n      \"dsize\": 2097152,\n      \"ino\": 259372\n    },\n    [\n      {\n        \"name\": \"subdir\",\n        \"asize\": 4096,\n        \"dsize\": 4096,\n        \"ino\": 259382\n      },\n      {\n        \"name\": \"file3.dat\",\n        \"asize\": 1048576,\n        \"dsize\": 1048576,\n        \"ino\": 259383\n      }\n    ],\n    {\n      \"name\": \"file1.dat\",\n      \"asize\": 1048576,\n      \"dsize\": 1048576,\n      \"ino\": 259371\n    }\n  ]\n]\n</code></pre>"},{"location":"blog/2023/06/22/prefer-systemd-timers-over-cron/#create-the-service-file","title":"Create the <code>.service</code> file","text":"<p>We can write a user-level systemd unit file by placing it into the proper folder.</p> Bash<pre><code>ubuntu@lclipp:~/systemd_blog$ mkdir -p ~/.config/systemd/user/\nubuntu@lclipp:~$ vim ~/.config/systemd/user/ncdu.service \n</code></pre> ncdu.service<pre><code>[Unit]\nDescription=ncdu scraping of user homedir\n\n[Service]\nExecStart=/usr/bin/bash -c \"ncdu ~/systemd_blog -o - | jq .\"\nWorkingDirectory=/home/ubuntu\n</code></pre> <p>If everything was done correctly, you can now view the service</p> Bash<pre><code>ubuntu@lclipp:~$ systemctl --user status ncdu.service\n\u25cb ncdu.service - ncdu scraping of user homedir\n     Loaded: loaded (/home/ubuntu/.config/systemd/user/ncdu.service; static)\n     Active: inactive (dead)\nubuntu@lclipp:~$ systemctl --user status ncdu.service\n\u25cb ncdu.service - ncdu scraping of user homedir\n     Loaded: loaded (/home/ubuntu/.config/systemd/user/ncdu.service; static)\n     Active: inactive (dead)\n\nJun 22 18:16:05 lclipp bash[3592]:       }\nJun 22 18:16:05 lclipp bash[3592]:     ],\nJun 22 18:16:05 lclipp bash[3592]:     {\nJun 22 18:16:05 lclipp bash[3592]:       \"name\": \"file1.dat\",\nJun 22 18:16:05 lclipp bash[3592]:       \"asize\": 1048576,\nJun 22 18:16:05 lclipp bash[3592]:       \"dsize\": 1048576,\nJun 22 18:16:05 lclipp bash[3592]:       \"ino\": 259371\nJun 22 18:16:05 lclipp bash[3592]:     }\nJun 22 18:16:05 lclipp bash[3592]:   ]\nJun 22 18:16:05 lclipp bash[3592]: ]\n</code></pre> <p>We can use the systemd journal to view the logs:</p> Bash<pre><code>ubuntu@lclipp:~$ journalctl --user -u ncdu.service | tail -n 10\nJun 22 18:16:05 lclipp bash[3592]:       }\nJun 22 18:16:05 lclipp bash[3592]:     ],\nJun 22 18:16:05 lclipp bash[3592]:     {\nJun 22 18:16:05 lclipp bash[3592]:       \"name\": \"file1.dat\",\nJun 22 18:16:05 lclipp bash[3592]:       \"asize\": 1048576,\nJun 22 18:16:05 lclipp bash[3592]:       \"dsize\": 1048576,\nJun 22 18:16:05 lclipp bash[3592]:       \"ino\": 259371\nJun 22 18:16:05 lclipp bash[3592]:     }\nJun 22 18:16:05 lclipp bash[3592]:   ]\nJun 22 18:16:05 lclipp bash[3592]: ]\n</code></pre>"},{"location":"blog/2023/06/22/prefer-systemd-timers-over-cron/#create-the-timer-file","title":"Create the <code>.timer</code> file","text":"<p>The timer file can be made in a similar way.</p> ncdu.timer<pre><code>[Unit]\nDescription=Periodically run the ncdu service\nRequires=ncdu.service\n\n[Timer]\nUnit=ncdu.service\nOnCalendar=*-*-* *:*:00\n\n[Install]\nWantedBy=timers.target\n</code></pre> <p>Now we have to activate it</p> Bash<pre><code>ubuntu@lclipp:~$ vim ~/.config/systemd/user/ncdu.timer\nubuntu@lclipp:~$ systemctl --user enable ncdu.timer\nCreated symlink /home/ubuntu/.config/systemd/user/timers.target.wants/ncdu.timer \u2192 /home/ubuntu/.config/systemd/user/ncdu.timer.\nubuntu@lclipp:~$ systemctl --user start ncdu.timer\nubuntu@lclipp:~$ systemctl --user list-timers\nNEXT                        LEFT     LAST PASSED UNIT       ACTIVATES   \nThu 2023-06-22 18:21:00 CDT 30s left n/a  n/a    ncdu.timer ncdu.service\n\n1 timers listed.\nPass --all to see loaded but inactive timers, too.\n</code></pre>"},{"location":"blog/2023/06/22/prefer-systemd-timers-over-cron/#benefits_1","title":"Benefits","text":"<ul> <li>Tighter visibility into service states.</li> <li>Service states are queryable by external services </li> <li>Expressive configuration options for service restart behavior.</li> <li>Expressive configuration options for determining what an \"active\" state actually means</li> <li>Unit templating syntax</li> <li>Support for forwarding journal logs to external logging mechanisms</li> <li>Built-in and configurable log retention policies</li> </ul> <p>Because the states are queryable, you can do cool things like graph the states in a state timeline in grafana.</p> <p></p> <p>This chart was created by using the <code>systemd_unit</code> telegraf plugin which forwards the state information to an InfluxDB database, which grafana then queries for the visualization.</p>"},{"location":"blog/2023/06/22/prefer-systemd-timers-over-cron/#drawbacks_1","title":"Drawbacks","text":"<ul> <li>The configuration is more complicated</li> <li>The templating system is somewhat clunky because of the escaping you have to do in <code>ExecStart=</code>. You are also restricted to only a single template variable.</li> <li>timer units are incapable of sending variable data to the service's template variable. There are many cases in my job where I would like to have a separate systemd unit for each calendar date. For example, we might want something like this: Bash<pre><code>$ systemctl --user status ncdu@*.service\nncdu@2023-01-01.service\nncdu@2023-01-02.service\nncdu@2023-01-03.service\n</code></pre> but this is actually exceedingly difficult to do, due to the variable nature of the date string. The timers which would instantiate these services can only send a static string, not a variable.</li> </ul>"},{"location":"blog/2023/07/15/analyzing-go-heap-escapes/","title":"Analyzing Go Heap Escapes","text":"<p>In this blog post, we discover how you can analyze what variables the Go compiler decides should escape to the heap, a common source of performance problems in Golang. We'll also show how you can configure the gopls language server in VSCode to give you a Codelens view into your escaped variables.</p>"},{"location":"blog/2023/07/15/analyzing-go-heap-escapes/#what-is-a-heap","title":"What is a Heap?","text":"<p>The working memory of most modern programs is divided into two main categories: the stack, which contains short-lived memory whose lifetime is intrinsically tied to the lifecycle of the stack of function calls, and the heap, which contains long-lived memory whose lifetime transcends your stack of function calls. The Go compiler has to make a decision on where a particular piece of data should reside by running what's called an Escape Analysis Algorithm. If the analysis decides that an object can be referenced outside of the lexical scope which created it, it will allocate it on the heap.</p>"},{"location":"blog/2023/07/15/analyzing-go-heap-escapes/#why-is-the-heap-a-problem","title":"Why is the Heap a Problem?","text":"<p>Garbage-collected languages like Go have to periodically sweep the tree of object references allocated on the heap to determine if the reference is reachable (meaning some part of the code might still potentially access it) or if the reference is orphaned. If it's orphaned, it's impossible for the code to ever use it, so we should free that memory. This process is highly memory-intensive and slows execution of the application. The garbage collector is a necessary evil due to the fact that Go does not require the programmer to manually free memory.</p>"},{"location":"blog/2023/07/15/analyzing-go-heap-escapes/#configuring-vscode-for-gc-heap-escape-highlighting","title":"Configuring VSCode for GC Heap Escape Highlighting","text":"<p>You can configure VSCode to highlight cases of heap escapes:</p> <p></p> <p>The VSCode plugin for Go provides integrations with its <code>gopls</code> language server. The language server is simply a subprocess that VSCode calls and creates a UNIX pipe through which queries and responses to the server can be sent (you can also run gopls as a TCP server listening to a local port). <code>gopls</code> can be configured in your VSCode workspace settings to highlight instances of heap escape in your code.</p> <p>If VSCode has not yet created a workspace for your project, open <code>File</code>/<code>Save Workspace As</code>, and save a workspace file in the root of your project. The configuration I used for this blog is this:</p> workspace.json<pre><code>{\n    \"folders\": [\n        {\n            \"path\": \".\"\n        }\n    ],\n    \"settings\": {\n        \"go.enableCodeLens\": {\n            \"runtest\": true # (1)!\n        },\n        \"gopls\": {\n            \"ui.codelenses\": { # (2)!\n                \"generate\": true,\n                \"gc_details\": true  # (3)!\n            },\n            \"ui.diagnostic.annotations\": {\n                \"escape\": true # (4)!\n            },\n            \"ui.semanticTokens\": true\n        },\n    }\n}\n</code></pre> <ol> <li>This is not relevant to GC highlighting, but is useful for Codelenses for unit tests</li> <li>These are the parameters you need to enable general GC annotations</li> <li>This enables a Codelens option for toggling the GC decisions on/off</li> <li>This actually enables the escape annotations</li> </ol> <p>After doing this, hovering your mouse over the highlights show the results of the escape analysis:</p> <p></p>"},{"location":"blog/2023/07/15/analyzing-go-heap-escapes/#situations-which-cause-escapes","title":"Situations which cause escapes","text":""},{"location":"blog/2023/07/15/analyzing-go-heap-escapes/#returning-pointers-to-local-objects","title":"Returning pointers to local objects","text":"<p>We can run standard go CLI tools to generate an analysis of our code. The highlighted lines will represent lines where escapes were found.</p> Go<pre><code>package main\n\nimport \"fmt\"\n\nfunc foobar() *string {\n    foobar := \"foobar\"\n    return &amp;foobar\n}\n\nfunc main() {\n    r := foobar()\n    fmt.Print(*r)\n}\n</code></pre> <p>We call <code>go build</code> with the following garbage collector flags to tell it to generate debug info on various decisions it's made, and output the results into a directory called <code>out</code>:</p> Bash<pre><code>$ go build -gcflags='-m=3' . |&amp; grep escape\n# go-heap-escapes\n./main.go:6:2: foobar escapes to heap:\n./main.go:12:12: *r escapes to heap:\n./main.go:12:11: ... argument does not escape\n./main.go:12:12: *r escapes to heap\n</code></pre> <p>You can also specify <code>go build -gcflags='-m=3 -json=file://out' .</code> to have it print the results to a number of json files.</p> <p>Looking closer at the output, we see these messages:</p> Text Only<pre><code>./main.go:6:2: foobar escapes to heap:\n./main.go:6:2:   flow: ~r0 = &amp;foobar:\n./main.go:6:2:     from &amp;foobar (address-of) at ./main.go:7:9\n./main.go:6:2:     from return &amp;foobar (return) at ./main.go:7:2\n./main.go:6:2: moved to heap: foobar\n</code></pre> <p>This is telling us very clearly that <code>foobar escapes to heap</code>. It's pretty obvious why, let's take a closer look.</p> Go<pre><code>func foobar() *string {\n    foobar := \"foobar\"\n    return &amp;foobar\n}\n</code></pre> <p>This function instantiates a string named <code>foobar</code> that has the value <code>\"foobar\"</code>. We return a pointer to <code>foobar</code> which then means that the string initially allocated on the stack of <code>func foobar() *string</code> can now be referenced by functions outside of this lexical scope. So the variable can't remain on the stack; it has to escape to the heap.</p> <p>The message even tells us exactly why it escapes and what sequences of events had to happen for it to escape. By following the escape flow messages, we can see the two necessary events were:</p> Text Only<pre><code>./main.go:6:2:     from &amp;foobar (address-of) at ./main.go:7:9\n./main.go:6:2:     from return &amp;foobar (return) at ./main.go:7:2\n</code></pre> <p>It escapes because we:</p> <ol> <li>We took the address of <code>foobar</code></li> <li>We returned that address</li> </ol>"},{"location":"blog/2023/07/15/analyzing-go-heap-escapes/#use-of-reflection","title":"Use of reflection","text":"<p>We also see that there's another escape on line 12 in the <code>fmt.Print</code>. </p> Text Only<pre><code>./main.go:12:12: *r escapes to heap:\n./main.go:12:12:   flow: {storage for ... argument} = &amp;{storage for *r}:\n./main.go:12:12:     from *r (spill) at ./main.go:12:12\n./main.go:12:12:     from ... argument (slice-literal-element) at ./main.go:12:11\n./main.go:12:12:   flow: fmt.a = &amp;{storage for ... argument}:\n./main.go:12:12:     from ... argument (spill) at ./main.go:12:11\n./main.go:12:12:     from fmt.a := ... argument (assign-pair) at ./main.go:12:11\n./main.go:12:12:   flow: {heap} = *fmt.a:\n./main.go:12:12:     from fmt.Fprint(os.Stdout, fmt.a...) (call parameter) at ./main.go:12:11\n./main.go:12:11: ... argument does not escape\n./main.go:12:12: *r escapes to heap\n</code></pre> <p>What's going on here? Let's take a closer look at what <code>fmt.Print</code>is doing. Under the hood, it calls this function. We can port the relevant parts of this logic into our editor. We copy the lines that do all of the reflection but leave out the complicated format parameter logic that isn't needed in our toy example.</p> Go<pre><code>package main\n\nimport (\n    \"bytes\"\n    \"reflect\"\n)\n\nfunc doPrint(b *bytes.Buffer, a []any) {\n    prevString := false\n    for argNum, arg := range a {\n        isString := arg != nil &amp;&amp; reflect.TypeOf(arg).Kind() == reflect.String\n        // Add a space between two non-string arguments.\n        if argNum &gt; 0 &amp;&amp; !isString &amp;&amp; !prevString {\n            b.WriteByte(' ')\n        }\n        prevString = isString\n    }\n}\n\nfunc main() {\n    w := bytes.Buffer{}\n    doPrint(&amp;w, []any{\"foobar\"})\n}\n</code></pre> <pre><code>./main.go:8:14: b does not escape\n./main.go:22:20: \"foobar\" escapes to heap:\n./main.go:22:19: []any{...} does not escape\n./main.go:22:20: \"foobar\" escapes to heap\n</code></pre> <p>After some investigation, I found that simply removing the <code>.Kind()</code> call results in no escapes happening:</p> Go<pre><code>package main\n\nimport (\n    \"bytes\"\n    \"reflect\"\n)\n\nfunc doPrint(b *bytes.Buffer, a []any) {\n    prevString := false\n    for argNum, arg := range a {\n\n        isString := arg != nil &amp;&amp;\n            reflect.TypeOf(arg) == reflect.TypeOf(arg) // (1)!\n        // Add a space between two non-string arguments.\n        if argNum &gt; 0 &amp;&amp; !isString &amp;&amp; !prevString {\n            b.WriteByte(' ')\n        }\n        prevString = isString\n    }\n}\n\nfunc main() {\n    w := bytes.Buffer{}\n    doPrint(&amp;w, []any{\"foobar\"})\n}\n</code></pre> <pre><code>./main.go:8:14: b does not escape\n./main.go:8:31: a does not escape\n./main.go:24:19: []any{...} does not escape\n./main.go:24:20: \"foobar\" does not escape\n</code></pre> <ol> <li>We do this <code>reflect.TypeOf</code> check to ensure that <code>reflect.TypeOf</code> is actually used. Our goal is to keep the <code>reflect.TypeOf</code> call but not the <code>.Kind()</code></li> </ol> <p>As you can see, the <code>.Kind()</code> call itself seems to be the determining factor on whether or not it escapes to the heap. The exact reason is a bit unclear, but if you look at the source code of the <code>reflect</code> package, you see lots of examples of the usage of <code>unsafe.Pointer</code> which is probably defeating the escape analysis by obscuring the type that the pointer points to, which consequently limits its ability to inspect which lexical scopes have references to the type. Someone who is more familiar with the internals of <code>reflect</code> should chime in and let me know if this is an accurate assessment.</p>"},{"location":"blog/2023/07/15/analyzing-go-heap-escapes/#use-of-interfaces","title":"Use of interfaces","text":"<p>This is not a new discovery. It has been known about for a long time by multiple different bloggers. It turns out that the Go compiler is incapable of knowing at compile-time whether the underlying type in an interface could cause the reference to escape the stack. From the perspective of the function taking an interface as an argument, this knowledge is difficult to know at compile time. <sup>1</sup></p> <p>We can see this is true even in the simple case where the argument is a bare interface:</p> Go<pre><code>package main\n\nimport (\n    \"bytes\"\n    \"io\"\n)\n\nfunc print(w io.Writer, s string) {\n    asBytes := []byte(s)\n    w.Write(asBytes)\n}\n\nfunc main() {\n    buf := bytes.Buffer{}\n    print(&amp;buf, \"foobar\")\n}\n</code></pre> <pre><code>./main.go:9:20: ([]byte)(s) escapes to heap:\n./main.go:9:20:   flow: asBytes = &amp;{storage for ([]byte)(s)}:\n./main.go:9:20:     from ([]byte)(s) (spill) at ./main.go:9:20\n./main.go:9:20:     from asBytes := ([]byte)(s) (assign) at ./main.go:9:10\n./main.go:9:20:   flow: {heap} = asBytes:\n./main.go:9:20:     from w.Write(asBytes) (call parameter) at ./main.go:10:9\n./main.go:8:12: parameter w leaks to {heap} with derefs=0:\n./main.go:8:12:   flow: {heap} = w:\n./main.go:8:12:     from w.Write(asBytes) (call parameter) at ./main.go:10:9\n./main.go:8:12: leaking param: w\n./main.go:8:25: s does not escape\n./main.go:9:20: ([]byte)(s) escapes to heap\n</code></pre> <p>The compiler claims that <code>[]byte(s)</code> escapes because it's being passed to <code>w.Write</code>, which is a method on an interface. On the contrary, if we change <code>w</code> to <code>*bytes.Buffer</code>, the compiler no longer claims an escape:</p> Go<pre><code>package main\n\nimport (\n    \"bytes\"\n)\n\nfunc print(w *bytes.Buffer, s string) {\n    asBytes := []byte(s)\n    w.Write(asBytes)\n}\n\nfunc main() {\n    buf := bytes.Buffer{}\n    print(&amp;buf, \"foobar\")\n}\n</code></pre> <pre><code>./main.go:7:12: w does not escape\n./main.go:7:29: s does not escape\n./main.go:8:20: ([]byte)(s) does not escape\n</code></pre>"},{"location":"blog/2023/07/15/analyzing-go-heap-escapes/#use-of-reference-types-on-interface-methods","title":"Use of reference types on interface methods","text":"<p>It's not enough to say that interfaces cause escapes, as we'll find below. Interfaces cause escapes only if we send reference types to one of its methods. The astute reader may have noticed that in our previous examples, the types we were sending to our interfaces were all reference types. What if we send value types?</p> Go<pre><code>package main\n\ntype Writer interface {\n    Write(b string) (int, error)\n}\n\ntype writer struct{}\n\nfunc (w writer) Write(b string) (int, error) {\n    return 0, nil\n}\n\nfunc print(w Writer) {\n    s := \"foobar\"\n    w.Write(s)\n}\n\nfunc main() {\n    var w Writer = writer{}\n    print(w)\n}\n</code></pre> <pre><code>./main.go:9:23: b does not escape\n./main.go:19:23: writer{} does not escape\n</code></pre> <p>And going back to a reference type, we can see yet again that simply changing the argument to a reference type causes it to escape.</p> Go<pre><code>package main\n\ntype Writer interface {\n    Write(b []byte) (int, error)\n}\n\ntype writer struct{}\n\nfunc (w writer) Write(b []byte) (int, error) {\n    return 0, nil\n}\n\nfunc print(w Writer) {\n    s := \"foobar\"\n    b := []byte(s)\n    w.Write(b)\n}\n\nfunc main() {\n    var w Writer = writer{}\n    print(w)\n}\n</code></pre> <pre><code>./main.go:9:23: b does not escape\n./main.go:15:14: ([]byte)(s) escapes to heap:\n./main.go:15:14: ([]byte)(s) escapes to heap\n./main.go:20:23: writer{} does not escape\n./main.go:21:7: ([]byte)(s) does not escape\n</code></pre> <p>What if we use a reference type on a non-interface value? Instead of <code>print</code> taking a <code>Writer</code> interface, we modify it to take a <code>writer</code> struct directly:</p> Go<pre><code>package main\n\ntype writer struct{}\n\nfunc (w writer) Write(b []byte) (int, error) {\n    return 0, nil\n}\n\nfunc print(w writer) {\n    s := \"foobar\"\n    b := []byte(s)\n    w.Write(b)\n}\n\nfunc main() {\n    print(writer{})\n}\n</code></pre> <pre><code>./main.go:5:23: b does not escape\n./main.go:11:14: ([]byte)(s) does not escape\n./main.go:16:7: ([]byte)(s) does not escape\n</code></pre>"},{"location":"blog/2023/07/15/analyzing-go-heap-escapes/#criteria-for-escape","title":"Criteria for Escape","text":"<p>This repo provides a wonderful explanation of how escapes actually happen. </p> <p>Criteria for Escapes</p> <p>There is one requirement to be eligible for escaping to the heap:</p> <ol> <li>The variable must be a reference type, ex. channels, interfaces, maps, pointers, slices</li> <li>A value type stored in an interface value can also escape to the heap</li> </ol> <p>If the above criteria is met, then a parameter will escape if it outlives its current stack frame. That usually happens when either:</p> <ol> <li>The variable is sent to a function that assigns the variable to a sink outside the stack frame</li> <li>Or the function where the variable is declared assigns it to a sink outside the stack frame</li> </ol> <p>Interfaces are a special case of the reference type, because as stated before, the compiler at compile-time has no idea what the implementation of the interface looks like so it has to shortcut its analysis and assume that an escape will happen. The methods defined on other reference types, like <code>*bytes.Buffer</code>, can be statically inspected by the analyzer to determine escapes.</p>"},{"location":"blog/2023/07/15/analyzing-go-heap-escapes/#criteria-for-leaks","title":"Criteria for Leaks","text":"<p>The linked repo above also explains to us what a leak is.</p> <p>Criteria for Leaks</p> <p>There are two requirements to be eligible for leaking:</p> <ol> <li>The variable must be a function parameter</li> <li>The variable must be a reference type, ex. channels, interfaces, maps, pointers, slices</li> </ol> <p>Value types such as built-in numeric types, structs, and arrays are not elgible to be leaked. That does not mean they are never placed on the heap, it just means a parameter of int32 is not going to send you running for a mop anytime soon.</p> <p>If the above criteria is met, then a parameter will leak if:</p> <ol> <li>The variable is returned from the same function and/or</li> <li>is assigned to a sink outside of the stack frame to which the variable belongs.</li> </ol> <p>A leak can also happen without escaping. Consider the case where a value is allocated in frame 0, passed into frame 1 as a pointer, and frame 1 returns that pointer back to frame 0:</p> Go<pre><code>package main\n\nimport (\n    \"io\"\n    \"os\"\n)\n\nfunc print(w io.Writer, s string) {\n    asBytes := []byte(s)\n    w.Write(asBytes)\n}\n\nfunc foo(fooString *string) *string {\n    return fooString\n}\n\nfunc main() {\n    hello := \"hello\"\n    f := foo(&amp;hello)\n    print(os.Stdout, *f)\n}\n</code></pre> <pre><code>./main.go:14:2:[1] foo stmt: return fooString\n./main.go:13:10: parameter fooString leaks to ~r0 with derefs=0:\n./main.go:13:10:   flow: ~r0 = fooString:\n./main.go:13:10:     from return fooString (return) at ./main.go:14:2\n./main.go:13:10: leaking param: fooString to result ~r0 level=0\n</code></pre> <p>We can see it mentions <code>fooString</code> leaking, but nowhere does it say it escaped. This is because it knows that the string never escapes from <code>main</code> even though the pointer in <code>foo()</code> leaks its argument to the return value.</p>"},{"location":"blog/2023/07/15/analyzing-go-heap-escapes/#conclusions","title":"Conclusions","text":"<p>These experiments lead us to conclude a few main points:</p> <ol> <li>Usage of reflection involves unsafe pointers, which defeats the escape analysis and causes escapes.</li> <li>Some of the basic packages like <code>fmt</code> heavily use reflection (and consequently <code>unsafe.Pointer</code>) to determine the types being passed to print functions and how to resolve them into the print format specifiers.</li> <li>Reflection should not be used unless absolutely necessary. Leveraging type safety in go allows it to inspect your program to determine whether an object can truly remain on the stack, or if it must be on the heap. </li> <li>Go makes conservative assumptions. If there is any doubt whatsoever about whether something can escape, it assumes it can. The alternative would be a program that handles garbage data, and even segfaults, due to unclear reasons.</li> <li>Because it's difficult to know at compile time whether the underlying type of an interface could cause a value to escape, the escape analyzer has to assume it's possible. Thus, any time a reference type is passed to an interface, it will escape.</li> <li>Using VSCode Codelens can help us catch cases of heap escapes and make us think critically about whether or not our abstractions are truly necessary.</li> </ol>"},{"location":"blog/2023/07/15/analyzing-go-heap-escapes/#lifetime-annotations","title":"Lifetime annotations","text":"<p>The grammar of Go does not provide hints to the compiler that lets us tell it what the lexical scope of a reference will be. Other languages like Rust provide lifetime annotations that allow us compile-time guarantees that a reference will be valid at runtime. These annotations allow you to tell the compiler which lifetime a reference is attached to. Take for example this theoreical Go code:</p> example.go<pre><code>func yIfLongest&lt;'a, 'b&gt;(x &amp;'a *string, y &amp;'b *string) &amp;'b *string {\n    if len(*y) &gt; len(*x) {\n      return y\n    }\n    s := \"\"\n    return &amp;s\n}\n</code></pre> <p>This is some complicated syntax, but those familiar with Rust might understand what's going on. Otherwise, bear with me. <code>func yIfLongest&lt;'a, 'b&gt;</code> is telling us that there are two separate lifetimes in our function, <code>'a</code> and <code>'b</code>. We assign <code>x</code> to the <code>'a</code> lifetime and <code>y</code> to the <code>'b</code> lifetime, and claim that the return value's lifetime should be the same as <code>y</code>. If the compiler has this information and already knows that <code>y</code> should never escape the stack, then it by extension knows that the return value also cannot escape the stack. Consider the alternative Go code without these annotations:</p> Go<pre><code>package main\n\nimport (\n    \"os\"\n)\n\nfunc yIfLongest(x, y *string) *string {\n    if len(*y) &gt; len(*x) {\n        return y\n    }\n    s := \"\"\n    return &amp;s\n}\n\nfunc main() {\n    x := \"ab\"\n    y := \"abcde\"\n\n    result := yIfLongest(&amp;x, &amp;y)\n    os.Setenv(\"Y\", *result) //(1)!\n}\n</code></pre> <pre><code>./main.go:11:2: s escapes to heap:\n./main.go:11:2:   flow: ~r0 = &amp;s:\n./main.go:11:2:     from &amp;s (address-of) at ./main.go:12:9\n./main.go:11:2:     from return &amp;s (return) at ./main.go:12:2\n./main.go:7:20: parameter y leaks to ~r0 with derefs=0:\n./main.go:7:20:   flow: ~r0 = y:\n./main.go:7:20:     from return y (return) at ./main.go:9:3\n./main.go:7:17: x does not escape\n./main.go:7:20: leaking param: y to result ~r0 level=0\n./main.go:11:2: moved to heap: s\n</code></pre> <ol> <li>I'm not using <code>fmt.Print</code> here because we've already shown that it causes escapes. Setting an environment variable is an easy task that doesn't require the use of <code>reflect</code>.</li> </ol> <p>The escape analyzer is showing us that while the <code>y</code> argument does indeed leak out of the function because we're returning it in some logical flows, it never escapes because it inspects <code>main()</code> and sees there's no opportunity for it to escape. However, it does decide that <code>s</code> must escape because we're returning the address of a local variable.</p> <p>While go perfectly allows returning the address of local variables (due to the escape analysis and its garbage collector), a similar activity in Rust will greet you with an angry compiler<sup>2</sup>:</p> Rust<pre><code>fn longest&lt;'a&gt;(x: &amp;str, y: &amp;str) -&gt; &amp;'a str {\n    let result = String::from(\"\");\n    result.as_str()\n}\n</code></pre> <pre><code>$ cargo run\n   Compiling chapter10 v0.1.0 (file:///projects/chapter10)\nerror[E0515]: cannot return reference to local variable `result`\n  --&gt; src/main.rs:11:5\n   |\n11 |     result.as_str()\n   |     ^^^^^^^^^^^^^^^ returns a reference to data owned by the current function\n\nFor more information about this error, try `rustc --explain E0515`.\nerror: could not compile `chapter10` due to previous error\n</code></pre> <p>This is because Rust does not automatically allocate memory, while Go does. This is a tradeoff that Go has made for the benefit of a simpler developer experience. If Go were to adopt a lifetime annotation syntax, theoretically it could allow the compiler to make more informed decisions about whether or not a local variable needs to escape to the heap. If we're telling it that its lifetime is equal to <code>y</code>, then the compiler will decide based off of what <code>y</code>'s lifetime is doing. This could provide much more intelligence around the other more confusing behavior surrounding interfaces (and maybe even reflection). It could also enable the compiler to disallow any operations that would violate the lifetime we specified. This would reduce the flexibility granted by the garbage collector, but we could avoid its cost.</p>"},{"location":"blog/2023/07/15/analyzing-go-heap-escapes/#parting-thoughts","title":"Parting thoughts","text":"<p>I have seen lots of people grow upset about how even simple instructions like <code>fmt.Print</code> cause heap allocations. If you dive into the theoretical basis of Go and what it's trying to achieve, you begin to realize just how complicated it is to get reference lifetime decisions right if you don't have the proper syntax. Go's entire mantra is to make the grammar as simple and approachable as possible, which is likely why this sytax hasn't manifested. This has real benefits when it comes to developer productivity as instead of agonizing over the details of memory management, you simply write the code you want and the memory is handled for you.</p> <p>Go trades some amount of memory and CPU efficiency for the goal of developer friendliness. Its code is easy to read because debates about where memory should reside are deferred to the compiler, thus the syntax becomes minimalistic and free of memory-managing instructions. It is no doubt that languages like C, C++, or Rust will always beat Go in terms of latency, memory efficiency, and CPU efficiency. But the workflows Go is geared towards (cloud and systems-of-systems based environments) are heavily bound by external IO anyway, which makes most of these complaints irrelevant. There are many tools at your disposal, and you must always pick the right one for the job.</p> <ol> <li> <p>It might theoretically be possible to ascertain this knowledge by inspecting the entirety of a program and seeing if any type that is boxed into an interface might cause the reference to escape. To my knowledge, the escape analyzer does not do this, and it's unknown whether doing such a whole-program analysis is even tractable.\u00a0\u21a9</p> </li> <li> <p>These examples are copied directly from Rust's documentation \u21a9</p> </li> </ol>"},{"location":"blog/2023/08/25/profile-guided-optimizations-in-go/","title":"Profile Guided Optimizations in Go","text":"<p>In this post, we'll explore Profile Guided Optimizations (PGO) introduced in Go 1.20 and how they can be effectively used to improve the performance of your production applications. PGO is a compiler optimization technique that allows you to tune the performance of your Go applications in an environment- and workload-specific way. The profiles themselves are simple metadata files that contain information on what functions are called, how often they're called, what system calls are used, and generally what the computational profile is of your application. This information can be used by the compiler to better inform what sorts of optimizations are useful in your specific environment and workfload.</p>"},{"location":"blog/2023/08/25/profile-guided-optimizations-in-go/#why-another-blog-post","title":"Why another blog post?","text":"<p>The Go authors have an excellent blog post on how to use Go's PGO, so what's the point of another blog post? Well, partly because writing blogs is simply a good excuse for me to learn things, and second, to provide a user-facing perspective on PGO and to answer questions on its general usefulness. A lot of optimization questions can be papered over by throwing more horizontal scale at the problem, which is often the approach that people in the high performance computing community take. However there are some scenarios, specifically ones bound by CPU or where the local runtime environment is a scarce resource, where compute optimizations are desirable.</p>"},{"location":"blog/2023/08/25/profile-guided-optimizations-in-go/#optimization-overview","title":"Optimization overview","text":"<p>The steps you take to profile generally follow this pattern:</p> <ol> <li>Build a vanilla go application without any profiling</li> <li>Run the app in production and collect CPU pprof data. This can be done either through an external tool such as perf, through specific call points to runtime/pprof, or by hitting an HTTP endpoint in your application by using net/http/pprof.</li> <li>Feed the profile into the compiler for a new build</li> <li>Repeat from step 2.</li> </ol> <p>We will explore all of these options and discuss the merits and downfalls of each.</p>"},{"location":"blog/2023/08/25/profile-guided-optimizations-in-go/#the-program","title":"The Program","text":"<p>The type of workload we're aiming to optimize is a compute-heavy one. We will be implementing a simple factorization algorithm, with a sprinkle of some intentional inefficiencies so we can simulate a program with really hot code paths. There are a large number of algorithms we could choose from, but I'll choose Fermat's Factorization due to its simplicity.</p> main.go<pre><code>package main\n\nimport (\n    \"flag\"\n    \"fmt\"\n    \"math\"\n    \"os\"\n    \"strconv\"\n)\n\nfunc isSquare(i uint64) bool {\n    sqrt := math.Sqrt(float64(i))\n\n    // We do something tricky here to exceed the Go inlining algorithm's\n    // budget of 40 nodes. This call to NewExpensive has enough \"code nodes\"\n    // to cause the compiler to consider isSquare too expensive to inline.\n    expensive := NewExpensive()\n    os.Setenv(\"EXPENSIVE_VALUE\", strconv.Itoa(int(expensive)))\n\n    return sqrt == float64(uint64(sqrt))\n}\n\nfunc findFactors(n uint64) (uint64, uint64, uint64) {\n    var a uint64\n    var b2 uint64\n\n    numIters := uint64(0)\n    for a = uint64(math.Ceil(math.Sqrt(float64(n)))); ; a++ {\n        numIters++\n        b2 = a*a - n\n\n        // We run isSquare many times in order to make it appear very hot\n        for i := 0; i &lt; 5; i++ {\n            isSquare(b2)\n        }\n        if isSquare(b2) {\n            break\n        }\n\n    }\n    sqrtB := uint64(math.Sqrt(float64(b2)))\n    return a - sqrtB, a + sqrtB, numIters\n}\n\nfunc main() {\n    var nFlag = flag.Uint64(\"n\", 8051, \"integer to factor\")\n    var cpuprofile = flag.String(\"cpuprofile\", \"default.pgo\", \"write cpu profile to `file`\")\n    var httpProfile = flag.Bool(\"httpprof\", false, \"\")\n    var infinite = flag.Int(\"infinite\", 0, \"\")\n\n    flag.Parse()\n\n    // Start the CPU profiling routines using runtime/pprof\n    close := runtimeProf(*cpuprofile)\n    defer close()\n\n    if *httpProfile {\n        httpProf()\n    }\n\n    if *infinite != 0 {\n        for {\n            findFactors(*nFlag)\n        }\n    } else {\n        factor1, factor2, numIterations := findFactors(*nFlag)\n        if factor1*factor2 != *nFlag {\n            panic(fmt.Sprintf(\"%d x %d != %d\", factor1, factor2, *nFlag))\n        }\n        fmt.Printf(\"Found factors with i=%d: %d = %d x %d\\n\", numIterations, *nFlag, factor1, factor2)\n    }\n\n}\n</code></pre> <pre><code>$ go build .\n$ ./fermats-factorization -n 179957108976619\nstarting CPU profile\nFound factors with i=42698929: 179957108976619 = 1627093 x 110600383\n</code></pre>"},{"location":"blog/2023/08/25/profile-guided-optimizations-in-go/#the-benchmark","title":"The Benchmark","text":"<p>The benchmark we'll be using is going to be captured by timing the program's runtime over 60 seconds without any optimizations applied. This will give us a baseline performance value which we can compare to optimized builds later on. This will be done by using the benchmarking test as shown here:</p> main_test.go<pre><code>package main\n\nimport (\n    \"fmt\"\n    \"testing\"\n)\n\nfunc BenchmarkFindFactors(b *testing.B) {\n    var fact1, fact2, numIterations uint64\n    for i := 0; i &lt; b.N; i++ {\n        fact1, fact2, numIterations = findFactors(179957108976619)\n    }\n    fmt.Printf(\"numIterations=%d fact1=%d fact2=%d\\n\", numIterations, fact1, fact2)\n}\n</code></pre> <p>We then run the benchmark with <code>-cpuprofile default.pgo</code> to tell it to create a CPU profile during the benchmark, and <code>-pgo=off</code> to disable PGO (we want an unoptimized run to compare against).</p> Text Only<pre><code>$ go test -bench . -benchtime 60s -cpuprofile default.pgo -pgo=off\nnumIterations=42698929 fact1=1627093 fact2=110600383\ngoos: linux\ngoarch: arm64\npkg: fermats-factorization\nBenchmarkFindFactors    numIterations=42698929 fact1=1627093 fact2=110600383\n       9    7173565614 ns/op\nPASS\nok      fermats-factorization   71.860s\n</code></pre> <p>Our baseline in this benchmark is thus 7173565614 ns/op.</p>"},{"location":"blog/2023/08/25/profile-guided-optimizations-in-go/#pgo-result","title":"PGO Result","text":"<p>We can re-run our benchmark, but tell it to use the <code>default.pgo</code> CPU profile we generated during the initial benchmark run:</p> Text Only<pre><code>$ go test -bench . -benchtime 60s -pgo=auto\nnumIterations=42698929 fact1=1627093 fact2=110600383\ngoos: linux\ngoarch: arm64\npkg: fermats-factorization\nBenchmarkFindFactors    numIterations=42698929 fact1=1627093 fact2=110600383\n       9    6702628537 ns/op\nPASS\nok      fermats-factorization   67.028s\n</code></pre> <p>Our runtime per iteration is 6702628537 ns, which is a full \\((1 - \\frac{6702628537}{7173565614}) * 100\\% = 6.564895371\\%\\) faster than our original run! That's quite a significant improvement.</p>"},{"location":"blog/2023/08/25/profile-guided-optimizations-in-go/#methods-of-profiling","title":"Methods of Profiling","text":""},{"location":"blog/2023/08/25/profile-guided-optimizations-in-go/#using-test-based-pprof","title":"Using test-based pprof","text":"<p>We can generate a CPU profile by simply enabling it during tests. This will give us a rather artificial example of the CPU profile as compared to its real-world behavior, but it might be interesting nonetheless. Let's see what happens.</p> <p>We can write a simple benchmarking test for <code>findFactors</code>:</p> <code>main_test.go</code><code>main.go</code> main_test.go<pre><code>package main\n\nimport (\n    \"fmt\"\n    \"testing\"\n)\n\nfunc BenchmarkFindFactors(b *testing.B) {\n    var fact1, fact2, numIterations uint64\n    for i := 0; i &lt; b.N; i++ {\n        fact1, fact2, numIterations = findFactors(179957108976619)\n    }\n    fmt.Printf(\"numIterations=%d fact1=%d fact2=%d\\n\", numIterations, fact1, fact2)\n}\n</code></pre> main.go<pre><code>package main\n\nimport (\n    \"flag\"\n    \"fmt\"\n    \"math\"\n    \"os\"\n    \"strconv\"\n)\n\nfunc isSquare(i uint64) bool {\n    sqrt := math.Sqrt(float64(i))\n\n    // We do something tricky here to exceed the Go inlining algorithm's\n    // budget of 40 nodes. This call to NewExpensive has enough \"code nodes\"\n    // to cause the compiler to consider isSquare too expensive to inline.\n    expensive := NewExpensive()\n    os.Setenv(\"EXPENSIVE_VALUE\", strconv.Itoa(int(expensive)))\n\n    return sqrt == float64(uint64(sqrt))\n}\n\nfunc findFactors(n uint64) (uint64, uint64, uint64) {\n    var a uint64\n    var b2 uint64\n\n    numIters := uint64(0)\n    for a = uint64(math.Ceil(math.Sqrt(float64(n)))); ; a++ {\n        numIters++\n        b2 = a*a - n\n\n        // We run isSquare many times in order to make it appear very hot\n        for i := 0; i &lt; 5; i++ {\n            isSquare(b2)\n        }\n        if isSquare(b2) {\n            break\n        }\n\n    }\n    sqrtB := uint64(math.Sqrt(float64(b2)))\n    return a - sqrtB, a + sqrtB, numIters\n}\n\nfunc main() {\n    var nFlag = flag.Uint64(\"n\", 8051, \"integer to factor\")\n    var cpuprofile = flag.String(\"cpuprofile\", \"default.pgo\", \"write cpu profile to `file`\")\n    var httpProfile = flag.Bool(\"httpprof\", false, \"\")\n    var infinite = flag.Int(\"infinite\", 0, \"\")\n\n    flag.Parse()\n\n    // Start the CPU profiling routines using runtime/pprof\n    close := runtimeProf(*cpuprofile)\n    defer close()\n\n    if *httpProfile {\n        httpProf()\n    }\n\n    if *infinite != 0 {\n        for {\n            findFactors(*nFlag)\n        }\n    } else {\n        factor1, factor2, numIterations := findFactors(*nFlag)\n        if factor1*factor2 != *nFlag {\n            panic(fmt.Sprintf(\"%d x %d != %d\", factor1, factor2, *nFlag))\n        }\n        fmt.Printf(\"Found factors with i=%d: %d = %d x %d\\n\", numIterations, *nFlag, factor1, factor2)\n    }\n\n}\n</code></pre> <p>Recall the last section that we can the benchmark with the parameter <code>-cpuprofile default.pgo</code>. As mentioned before, this causes the benchmark to profile the code as its running, and outputs the <code>default.pgo</code> profile.</p>"},{"location":"blog/2023/08/25/profile-guided-optimizations-in-go/#using-in-lined-pprof","title":"Using in-lined pprof","text":"<code>runtime_profile.go</code><code>main.go</code> <p>The method used in <code>main.go</code> is to use <code>runtime/pprof</code>. The code for that is relatively simple:</p> runtime_profile.go<pre><code>package main\n\nimport (\n    \"fmt\"\n    \"log\"\n    _ \"net/http/pprof\"\n    \"os\"\n    \"runtime/pprof\"\n)\n\nfunc runtimeProf(cpuprofile string) (close func()) {\n    if cpuprofile == \"\" {\n        return func() {}\n    }\n    fmt.Println(\"starting CPU profile\")\n    f, err := os.Create(cpuprofile)\n    if err != nil {\n        log.Fatal(\"could not create CPU profile: \", err)\n    }\n\n    if err := pprof.StartCPUProfile(f); err != nil {\n        log.Fatal(\"could not start CPU profile: \", err)\n    }\n\n    close = func() {\n        pprof.StopCPUProfile()\n        f.Close()\n    }\n    return close\n}\n</code></pre> main.go<pre><code>package main\n\nimport (\n    \"flag\"\n    \"fmt\"\n    \"math\"\n    \"os\"\n    \"strconv\"\n)\n\nfunc isSquare(i uint64) bool {\n    sqrt := math.Sqrt(float64(i))\n\n    // We do something tricky here to exceed the Go inlining algorithm's\n    // budget of 40 nodes. This call to NewExpensive has enough \"code nodes\"\n    // to cause the compiler to consider isSquare too expensive to inline.\n    expensive := NewExpensive()\n    os.Setenv(\"EXPENSIVE_VALUE\", strconv.Itoa(int(expensive)))\n\n    return sqrt == float64(uint64(sqrt))\n}\n\nfunc findFactors(n uint64) (uint64, uint64, uint64) {\n    var a uint64\n    var b2 uint64\n\n    numIters := uint64(0)\n    for a = uint64(math.Ceil(math.Sqrt(float64(n)))); ; a++ {\n        numIters++\n        b2 = a*a - n\n\n        // We run isSquare many times in order to make it appear very hot\n        for i := 0; i &lt; 5; i++ {\n            isSquare(b2)\n        }\n        if isSquare(b2) {\n            break\n        }\n\n    }\n    sqrtB := uint64(math.Sqrt(float64(b2)))\n    return a - sqrtB, a + sqrtB, numIters\n}\n\nfunc main() {\n    var nFlag = flag.Uint64(\"n\", 8051, \"integer to factor\")\n    var cpuprofile = flag.String(\"cpuprofile\", \"default.pgo\", \"write cpu profile to `file`\")\n    var httpProfile = flag.Bool(\"httpprof\", false, \"\")\n    var infinite = flag.Int(\"infinite\", 0, \"\")\n\n    flag.Parse()\n\n    // Start the CPU profiling routines using runtime/pprof\n    close := runtimeProf(*cpuprofile)\n    defer close()\n\n    if *httpProfile {\n        httpProf()\n    }\n\n    if *infinite != 0 {\n        for {\n            findFactors(*nFlag)\n        }\n    } else {\n        factor1, factor2, numIterations := findFactors(*nFlag)\n        if factor1*factor2 != *nFlag {\n            panic(fmt.Sprintf(\"%d x %d != %d\", factor1, factor2, *nFlag))\n        }\n        fmt.Printf(\"Found factors with i=%d: %d = %d x %d\\n\", numIterations, *nFlag, factor1, factor2)\n    }\n\n}\n</code></pre> <p>It starts the CPU profile and returns a <code>close</code> function that the caller must <code>defer close()</code> in order to stop the profile and close the file.</p>"},{"location":"blog/2023/08/25/profile-guided-optimizations-in-go/#using-http-pprof","title":"Using HTTP pprof","text":"<p>The blog post previously linked shows us another way to gather profiles. We can instantiate an HTTP server and submit <code>GET</code> requests to our program. We do this by starting a server at port 6060 and by importing <code>net/http/pprof</code> (which automatically adds handlers to the server on import). The relevant function is <code>func httpProf()</code> in our <code>http_profile.go</code> file.</p> <code>http_profile.go</code><code>main.go</code> http_profile.go<pre><code>package main\n\nimport (\n    \"log\"\n    \"net/http\"\n    _ \"net/http/pprof\"\n)\n\nfunc httpProf() {\n    go func() {\n        log.Println(http.ListenAndServe(\"localhost:6060\", nil))\n    }()\n}\n</code></pre> main.go<pre><code>package main\n\nimport (\n    \"flag\"\n    \"fmt\"\n    \"math\"\n    \"os\"\n    \"strconv\"\n)\n\nfunc isSquare(i uint64) bool {\n    sqrt := math.Sqrt(float64(i))\n\n    // We do something tricky here to exceed the Go inlining algorithm's\n    // budget of 40 nodes. This call to NewExpensive has enough \"code nodes\"\n    // to cause the compiler to consider isSquare too expensive to inline.\n    expensive := NewExpensive()\n    os.Setenv(\"EXPENSIVE_VALUE\", strconv.Itoa(int(expensive)))\n\n    return sqrt == float64(uint64(sqrt))\n}\n\nfunc findFactors(n uint64) (uint64, uint64, uint64) {\n    var a uint64\n    var b2 uint64\n\n    numIters := uint64(0)\n    for a = uint64(math.Ceil(math.Sqrt(float64(n)))); ; a++ {\n        numIters++\n        b2 = a*a - n\n\n        // We run isSquare many times in order to make it appear very hot\n        for i := 0; i &lt; 5; i++ {\n            isSquare(b2)\n        }\n        if isSquare(b2) {\n            break\n        }\n\n    }\n    sqrtB := uint64(math.Sqrt(float64(b2)))\n    return a - sqrtB, a + sqrtB, numIters\n}\n\nfunc main() {\n    var nFlag = flag.Uint64(\"n\", 8051, \"integer to factor\")\n    var cpuprofile = flag.String(\"cpuprofile\", \"default.pgo\", \"write cpu profile to `file`\")\n    var httpProfile = flag.Bool(\"httpprof\", false, \"\")\n    var infinite = flag.Int(\"infinite\", 0, \"\")\n\n    flag.Parse()\n\n    // Start the CPU profiling routines using runtime/pprof\n    close := runtimeProf(*cpuprofile)\n    defer close()\n\n    if *httpProfile {\n        httpProf()\n    }\n\n    if *infinite != 0 {\n        for {\n            findFactors(*nFlag)\n        }\n    } else {\n        factor1, factor2, numIterations := findFactors(*nFlag)\n        if factor1*factor2 != *nFlag {\n            panic(fmt.Sprintf(\"%d x %d != %d\", factor1, factor2, *nFlag))\n        }\n        fmt.Printf(\"Found factors with i=%d: %d = %d x %d\\n\", numIterations, *nFlag, factor1, factor2)\n    }\n\n}\n</code></pre> <p>Let's start the program and tell it to infinitely find the factorization:</p> Text Only<pre><code>$ ./fermats-factorization -httpprof=true -infinite 1 -cpuprofile=\"\"\n</code></pre> <p>You can then open a browser to see a simple text-only page at <code>http://localhost:6060/debug/pprof/</code>:</p> Text Only<pre><code>/debug/pprof/\nSet debug=1 as a query parameter to export in legacy text format\n\n\nTypes of profiles available:\nCount   Profile\n10  allocs\n0   block\n0   cmdline\n4   goroutine\n10  heap\n0   mutex\n0   profile\n6   threadcreate\n0   trace\nfull goroutine stack dump\nProfile Descriptions:\n\nallocs: A sampling of all past memory allocations\nblock: Stack traces that led to blocking on synchronization primitives\ncmdline: The command line invocation of the current program\ngoroutine: Stack traces of all current goroutines. Use debug=2 as a query parameter to export in the same format as an unrecovered panic.\nheap: A sampling of memory allocations of live objects. You can specify the gc GET parameter to run GC before taking the heap sample.\nmutex: Stack traces of holders of contended mutexes\nprofile: CPU profile. You can specify the duration in the seconds GET parameter. After you get the profile file, use the go tool pprof command to investigate the profile.\nthreadcreate: Stack traces that led to the creation of new OS threads\ntrace: A trace of execution of the current program. You can specify the duration in the seconds GET parameter. After you get the trace file, use the go tool trace command to investigate the trace.\n</code></pre> <p>It shows the links we can get to inspect many things about the program state. Here are some examples of the endpoints you can query:</p> <code>goroutine</code><code>allocs</code><code>threadcreate</code> <pre><code>$ curl http://localhost:6060/debug/pprof/goroutine?debug=2\ngoroutine 96 [running]:\nruntime/pprof.writeGoroutineStacks({0x76b3e0, 0xc000126000})\n        /home/lclipp/src/golang-1.21/src/runtime/pprof/pprof.go:703 +0x6a\nruntime/pprof.writeGoroutine({0x76b3e0?, 0xc000126000?}, 0xc00005c7e8?)\n        /home/lclipp/src/golang-1.21/src/runtime/pprof/pprof.go:692 +0x25\nruntime/pprof.(*Profile).WriteTo(0x6c92e0?, {0x76b3e0?, 0xc000126000?}, 0xc?)\n        /home/lclipp/src/golang-1.21/src/runtime/pprof/pprof.go:329 +0x146\nnet/http/pprof.handler.ServeHTTP({0xc0000203a1, 0x9}, {0x76cf68, 0xc000126000}, 0x76b180?)\n        /home/lclipp/src/golang-1.21/src/net/http/pprof/pprof.go:267 +0x4a8\nnet/http/pprof.Index({0x76cf68?, 0xc000126000}, 0xc000122000?)\n        /home/lclipp/src/golang-1.21/src/net/http/pprof/pprof.go:384 +0xe5\nnet/http.HandlerFunc.ServeHTTP(0x443d60?, {0x76cf68?, 0xc000126000?}, 0x61a0fa?)\n        /home/lclipp/src/golang-1.21/src/net/http/server.go:2136 +0x29\nnet/http.(*ServeMux).ServeHTTP(0x95d4e0?, {0x76cf68, 0xc000126000}, 0xc000122000)\n        /home/lclipp/src/golang-1.21/src/net/http/server.go:2514 +0x142\nnet/http.serverHandler.ServeHTTP({0xc000089140?}, {0x76cf68?, 0xc000126000?}, 0x6?)\n        /home/lclipp/src/golang-1.21/src/net/http/server.go:2938 +0x8e\nnet/http.(*conn).serve(0xc0001e0090, {0x76d5d0, 0xc0000890e0})\n        /home/lclipp/src/golang-1.21/src/net/http/server.go:2009 +0x5f4\ncreated by net/http.(*Server).Serve in goroutine 6\n        /home/lclipp/src/golang-1.21/src/net/http/server.go:3086 +0x5cb\n\ngoroutine 1 [runnable]:\nsync.(*Mutex).Unlock(...)\n        /home/lclipp/src/golang-1.21/src/sync/mutex.go:219\nsync.(*RWMutex).Unlock(0x95da90)\n        /home/lclipp/src/golang-1.21/src/sync/rwmutex.go:216 +0x8e\nsyscall.Setenv({0x6f054a, 0xf}, {0x7031ee, 0x2})\n        /home/lclipp/src/golang-1.21/src/syscall/env_unix.go:123 +0x36a\nos.Setenv({0x6f054a?, 0x0?}, {0x7031ee?, 0x283?})\n        /home/lclipp/src/golang-1.21/src/os/env.go:120 +0x25\nmain.isSquare(0x4018000000000000?)\n        /home/lclipp/git/LandonTClipp/LandonTClipp.github.io/code/profile-guided-optimizations/fermats-factorization/main.go:18 +0x6a\nmain.findFactors(0x1f73)\n        /home/lclipp/git/LandonTClipp/LandonTClipp.github.io/code/profile-guided-optimizations/fermats-factorization/main.go:33 +0xbb\nmain.main()\n        /home/lclipp/git/LandonTClipp/LandonTClipp.github.io/code/profile-guided-optimizations/fermats-factorization/main.go:61 +0x273\n\ngoroutine 6 [IO wait]:\ninternal/poll.runtime_pollWait(0x7ffff7e47820, 0x72)\n        /home/lclipp/src/golang-1.21/src/runtime/netpoll.go:343 +0x85\ninternal/poll.(*pollDesc).wait(0xc0000ae100?, 0x5?, 0x0)\n        /home/lclipp/src/golang-1.21/src/internal/poll/fd_poll_runtime.go:84 +0x27\ninternal/poll.(*pollDesc).waitRead(...)\n        /home/lclipp/src/golang-1.21/src/internal/poll/fd_poll_runtime.go:89\ninternal/poll.(*FD).Accept(0xc0000ae100)\n        /home/lclipp/src/golang-1.21/src/internal/poll/fd_unix.go:611 +0x2ac\nnet.(*netFD).accept(0xc0000ae100)\n        /home/lclipp/src/golang-1.21/src/net/fd_unix.go:172 +0x29\nnet.(*TCPListener).accept(0xc00006a1e0)\n        /home/lclipp/src/golang-1.21/src/net/tcpsock_posix.go:152 +0x1e\nnet.(*TCPListener).Accept(0xc00006a1e0)\n        /home/lclipp/src/golang-1.21/src/net/tcpsock.go:315 +0x30\nnet/http.(*Server).Serve(0xc0001ba000, {0x76d178, 0xc00006a1e0})\n        /home/lclipp/src/golang-1.21/src/net/http/server.go:3056 +0x364\nnet/http.(*Server).ListenAndServe(0xc0001ba000)\n        /home/lclipp/src/golang-1.21/src/net/http/server.go:2985 +0x71\nnet/http.ListenAndServe(...)\n        /home/lclipp/src/golang-1.21/src/net/http/server.go:3239\nmain.httpProf.func1()\n        /home/lclipp/git/LandonTClipp/LandonTClipp.github.io/code/profile-guided-optimizations/fermats-factorization/profile.go:35 +0x3a\ncreated by main.httpProf in goroutine 1\n        /home/lclipp/git/LandonTClipp/LandonTClipp.github.io/code/profile-guided-optimizations/fermats-factorization/profile.go:34 +0x1a\n\ngoroutine 29 [IO wait]:\ninternal/poll.runtime_pollWait(0x7ffff7e47728, 0x72)\n        /home/lclipp/src/golang-1.21/src/runtime/netpoll.go:343 +0x85\ninternal/poll.(*pollDesc).wait(0xc0000ae000?, 0xc0001e2000?, 0x0)\n        /home/lclipp/src/golang-1.21/src/internal/poll/fd_poll_runtime.go:84 +0x27\ninternal/poll.(*pollDesc).waitRead(...)\n        /home/lclipp/src/golang-1.21/src/internal/poll/fd_poll_runtime.go:89\ninternal/poll.(*FD).Read(0xc0000ae000, {0xc0001e2000, 0x1000, 0x1000})\n        /home/lclipp/src/golang-1.21/src/internal/poll/fd_unix.go:164 +0x27a\nnet.(*netFD).Read(0xc0000ae000, {0xc0001e2000?, 0x4b3d45?, 0x0?})\n        /home/lclipp/src/golang-1.21/src/net/fd_posix.go:55 +0x25\nnet.(*conn).Read(0xc000038008, {0xc0001e2000?, 0x0?, 0xc000088e78?})\n        /home/lclipp/src/golang-1.21/src/net/net.go:179 +0x45\nnet/http.(*connReader).Read(0xc000088e70, {0xc0001e2000, 0x1000, 0x1000})\n        /home/lclipp/src/golang-1.21/src/net/http/server.go:791 +0x14b\nbufio.(*Reader).fill(0xc00006e1e0)\n        /home/lclipp/src/golang-1.21/src/bufio/bufio.go:113 +0x103\nbufio.(*Reader).Peek(0xc00006e1e0, 0x4)\n        /home/lclipp/src/golang-1.21/src/bufio/bufio.go:151 +0x53\nnet/http.(*conn).serve(0xc0001e0000, {0x76d5d0, 0xc0000890e0})\n        /home/lclipp/src/golang-1.21/src/net/http/server.go:2044 +0x75c\ncreated by net/http.(*Server).Serve in goroutine 6\n        /home/lclipp/src/golang-1.21/src/net/http/server.go:3086 +0x5cb\n\ngoroutine 97 [runnable]:\nnet/http.(*connReader).startBackgroundRead.func2()\n        /home/lclipp/src/golang-1.21/src/net/http/server.go:679\nruntime.goexit()\n        /home/lclipp/src/golang-1.21/src/runtime/asm_amd64.s:1650 +0x1\ncreated by net/http.(*connReader).startBackgroundRead in goroutine 96\n        /home/lclipp/src/golang-1.21/src/net/http/server.go:679 +0xba\n</code></pre> <pre><code>$ curl http://localhost:6060/debug/pprof/allocs?debug=2\nheap profile: 1: 24 [120369: 65498176] @ heap/1048576\n1: 24 [66654: 1599696] @ 0x454bb2 0x4545a5 0x454805 0x49bda5 0x4b82e5 0x67466a 0x6747db 0x674ab3 0x43b5bb 0x46d781\n#       0x49bda4        syscall.Setenv+0x1e4    /home/lclipp/src/golang-1.21/src/syscall/env_unix.go:114\n#       0x4b82e4        os.Setenv+0x24          /home/lclipp/src/golang-1.21/src/os/env.go:120\n#       0x674669        main.isSquare+0x69      /home/lclipp/git/LandonTClipp/LandonTClipp.github.io/code/profile-guided-optimizations/fermats-factorization/main.go:18\n#       0x6747da        main.findFactors+0xba   /home/lclipp/git/LandonTClipp/LandonTClipp.github.io/code/profile-guided-optimizations/fermats-factorization/main.go:33\n#       0x674ab2        main.main+0x272         /home/lclipp/git/LandonTClipp/LandonTClipp.github.io/code/profile-guided-optimizations/fermats-factorization/main.go:61\n#       0x43b5ba        runtime.main+0x2ba      /home/lclipp/src/golang-1.21/src/runtime/proc.go:267\n\n0: 0 [2: 1327104] @ 0x5e7685 0x5e765f 0x665bab 0x66c245 0x66a74b 0x669826 0x664fe5 0x46d781\n#       0x5e7684        compress/flate.NewWriter+0x2e4                                  /home/lclipp/src/golang-1.21/src/compress/flate/deflate.go:666\n#       0x5e765e        compress/gzip.(*Writer).Write+0x2be                             /home/lclipp/src/golang-1.21/src/compress/gzip/gzip.go:191\n#       0x665baa        runtime/pprof.(*profileBuilder).flush+0x4a                      /home/lclipp/src/golang-1.21/src/runtime/pprof/proto.go:146\n#       0x66c244        runtime/pprof.(*profileBuilder).emitLocation+0x13c4             /home/lclipp/src/golang-1.21/src/runtime/pprof/proto.go:650\n#       0x66a74a        runtime/pprof.(*profileBuilder).appendLocsForStack+0x4ca        /home/lclipp/src/golang-1.21/src/runtime/pprof/proto.go:463\n#       0x669825        runtime/pprof.(*profileBuilder).build+0x205                     /home/lclipp/src/golang-1.21/src/runtime/pprof/proto.go:376\n#       0x664fe4        runtime/pprof.profileWriter+0xc4                                /home/lclipp/src/golang-1.21/src/runtime/pprof/pprof.go:823\n\n0: 0 [2: 448] @ 0x668c10 0x668be6 0x66a5f3 0x669826 0x664fe5 0x46d781\n#       0x668be5        runtime/pprof.allFrames+0x25                                    /home/lclipp/src/golang-1.21/src/runtime/pprof/proto.go:214\n#       0x66a5f2        runtime/pprof.(*profileBuilder).appendLocsForStack+0x372        /home/lclipp/src/golang-1.21/src/runtime/pprof/proto.go:447\n#       0x669825        runtime/pprof.(*profileBuilder).build+0x205                     /home/lclipp/src/golang-1.21/src/runtime/pprof/proto.go:376\n#       0x664fe4        runtime/pprof.profileWriter+0xc4                                /home/lclipp/src/golang-1.21/src/runtime/pprof/pprof.go:823\n\n0: 0 [1: 262144] @ 0x5ddacb 0x5e769e 0x5e765f 0x669a78 0x664fe5 0x46d781\n#       0x5ddaca        compress/flate.(*compressor).init+0x46a         /home/lclipp/src/golang-1.21/src/compress/flate/deflate.go:586\n#       0x5e769d        compress/flate.NewWriter+0x2fd                  /home/lclipp/src/golang-1.21/src/compress/flate/deflate.go:667\n#       0x5e765e        compress/gzip.(*Writer).Write+0x2be             /home/lclipp/src/golang-1.21/src/compress/gzip/gzip.go:191\n#       0x669a77        runtime/pprof.(*profileBuilder).build+0x457     /home/lclipp/src/golang-1.21/src/runtime/pprof/proto.go:390\n#       0x664fe4        runtime/pprof.profileWriter+0xc4                /home/lclipp/src/golang-1.21/src/runtime/pprof/pprof.go:823\n\n0: 0 [1: 663552] @ 0x5e7685 0x5e765f 0x669a78 0x664fe5 0x46d781\n#       0x5e7684        compress/flate.NewWriter+0x2e4                  /home/lclipp/src/golang-1.21/src/compress/flate/deflate.go:666\n#       0x5e765e        compress/gzip.(*Writer).Write+0x2be             /home/lclipp/src/golang-1.21/src/compress/gzip/gzip.go:191\n#       0x669a77        runtime/pprof.(*profileBuilder).build+0x457     /home/lclipp/src/golang-1.21/src/runtime/pprof/proto.go:390\n#       0x664fe4        runtime/pprof.profileWriter+0xc4                /home/lclipp/src/golang-1.21/src/runtime/pprof/pprof.go:823\n\n0: 0 [1: 139264] @ 0x5dda47 0x5dda9d 0x5e769e 0x5e765f 0x665bab 0x66c245 0x66a74b 0x669826 0x664fe5 0x46d781\n#       0x5dda46        compress/flate.newDeflateFast+0x3e6                             /home/lclipp/src/golang-1.21/src/compress/flate/deflatefast.go:64\n#       0x5dda9c        compress/flate.(*compressor).init+0x43c                         /home/lclipp/src/golang-1.21/src/compress/flate/deflate.go:585\n#       0x5e769d        compress/flate.NewWriter+0x2fd                                  /home/lclipp/src/golang-1.21/src/compress/flate/deflate.go:667\n#       0x5e765e        compress/gzip.(*Writer).Write+0x2be                             /home/lclipp/src/golang-1.21/src/compress/gzip/gzip.go:191\n#       0x665baa        runtime/pprof.(*profileBuilder).flush+0x4a                      /home/lclipp/src/golang-1.21/src/runtime/pprof/proto.go:146\n#       0x66c244        runtime/pprof.(*profileBuilder).emitLocation+0x13c4             /home/lclipp/src/golang-1.21/src/runtime/pprof/proto.go:650\n#       0x66a74a        runtime/pprof.(*profileBuilder).appendLocsForStack+0x4ca        /home/lclipp/src/golang-1.21/src/runtime/pprof/proto.go:463\n#       0x669825        runtime/pprof.(*profileBuilder).build+0x205                     /home/lclipp/src/golang-1.21/src/runtime/pprof/proto.go:376\n#       0x664fe4        runtime/pprof.profileWriter+0xc4                                /home/lclipp/src/golang-1.21/src/runtime/pprof/pprof.go:823\n\n0: 0 [1: 2048] @ 0x61ae87 0x61ae60 0x61c41e 0x620059 0x46d781\n#       0x61ae86        bufio.NewWriterSize+0x1c6               /home/lclipp/src/golang-1.21/src/bufio/bufio.go:599\n#       0x61ae5f        net/http.newBufioWriterSize+0x19f       /home/lclipp/src/golang-1.21/src/net/http/server.go:853\n#       0x61c41d        net/http.(*conn).readRequest+0x9dd      /home/lclipp/src/golang-1.21/src/net/http/server.go:1066\n#       0x620058        net/http.(*conn).serve+0x338            /home/lclipp/src/golang-1.21/src/net/http/server.go:1934\n\n0: 0 [1: 131072] @ 0x448c59 0x4085f1 0x664d78 0x671885 0x6214e9 0x622e02 0x623aae 0x620314 0x46d781\n#       0x664d77        runtime/pprof.StartCPUProfile+0xf7      /home/lclipp/src/golang-1.21/src/runtime/pprof/pprof.go:793\n#       0x671884        net/http/pprof.Profile+0x2a4            /home/lclipp/src/golang-1.21/src/net/http/pprof/pprof.go:149\n#       0x6214e8        net/http.HandlerFunc.ServeHTTP+0x28     /home/lclipp/src/golang-1.21/src/net/http/server.go:2136\n#       0x622e01        net/http.(*ServeMux).ServeHTTP+0x141    /home/lclipp/src/golang-1.21/src/net/http/server.go:2514\n#       0x623aad        net/http.serverHandler.ServeHTTP+0x8d   /home/lclipp/src/golang-1.21/src/net/http/server.go:2938\n#       0x620313        net/http.(*conn).serve+0x5f3            /home/lclipp/src/golang-1.21/src/net/http/server.go:2009\n\n0: 0 [53: 55574528] @ 0x6648f0 0x664865 0x661326 0x672908 0x673405 0x6214e9 0x622e02 0x623aae 0x620314 0x46d781\n#       0x6648ef        runtime/pprof.writeGoroutineStacks+0x2f /home/lclipp/src/golang-1.21/src/runtime/pprof/pprof.go:701\n#       0x664864        runtime/pprof.writeGoroutine+0x24       /home/lclipp/src/golang-1.21/src/runtime/pprof/pprof.go:692\n#       0x661325        runtime/pprof.(*Profile).WriteTo+0x145  /home/lclipp/src/golang-1.21/src/runtime/pprof/pprof.go:329\n#       0x672907        net/http/pprof.handler.ServeHTTP+0x4a7  /home/lclipp/src/golang-1.21/src/net/http/pprof/pprof.go:267\n#       0x673404        net/http/pprof.Index+0xe4               /home/lclipp/src/golang-1.21/src/net/http/pprof/pprof.go:384\n#       0x6214e8        net/http.HandlerFunc.ServeHTTP+0x28     /home/lclipp/src/golang-1.21/src/net/http/server.go:2136\n#       0x622e01        net/http.(*ServeMux).ServeHTTP+0x141    /home/lclipp/src/golang-1.21/src/net/http/server.go:2514\n#       0x623aad        net/http.serverHandler.ServeHTTP+0x8d   /home/lclipp/src/golang-1.21/src/net/http/server.go:2938\n#       0x620313        net/http.(*conn).serve+0x5f3            /home/lclipp/src/golang-1.21/src/net/http/server.go:2009\n\n0: 0 [1: 81920] @ 0x40fb4d 0x41134f 0x412d39 0x66b1ad 0x66a4f6 0x669826 0x664fe5 0x46d781\n#       0x66b1ac        runtime/pprof.(*profileBuilder).emitLocation+0x32c              /home/lclipp/src/golang-1.21/src/runtime/pprof/proto.go:602\n#       0x66a4f5        runtime/pprof.(*profileBuilder).appendLocsForStack+0x275        /home/lclipp/src/golang-1.21/src/runtime/pprof/proto.go:431\n#       0x669825        runtime/pprof.(*profileBuilder).build+0x205                     /home/lclipp/src/golang-1.21/src/runtime/pprof/proto.go:376\n#       0x664fe4        runtime/pprof.profileWriter+0xc4                                /home/lclipp/src/golang-1.21/src/runtime/pprof/pprof.go:823\n\n0: 0 [1: 663552] @ 0x5e7685 0x5e765f 0x665bab 0x66c245 0x66a4f6 0x669826 0x664fe5 0x46d781\n#       0x5e7684        compress/flate.NewWriter+0x2e4                                  /home/lclipp/src/golang-1.21/src/compress/flate/deflate.go:666\n#       0x5e765e        compress/gzip.(*Writer).Write+0x2be                             /home/lclipp/src/golang-1.21/src/compress/gzip/gzip.go:191\n#       0x665baa        runtime/pprof.(*profileBuilder).flush+0x4a                      /home/lclipp/src/golang-1.21/src/runtime/pprof/proto.go:146\n#       0x66c244        runtime/pprof.(*profileBuilder).emitLocation+0x13c4             /home/lclipp/src/golang-1.21/src/runtime/pprof/proto.go:650\n#       0x66a4f5        runtime/pprof.(*profileBuilder).appendLocsForStack+0x275        /home/lclipp/src/golang-1.21/src/runtime/pprof/proto.go:431\n#       0x669825        runtime/pprof.(*profileBuilder).build+0x205                     /home/lclipp/src/golang-1.21/src/runtime/pprof/proto.go:376\n#       0x664fe4        runtime/pprof.profileWriter+0xc4                                /home/lclipp/src/golang-1.21/src/runtime/pprof/pprof.go:823\n\n0: 0 [4: 4194304] @ 0x448c16 0x4085f1 0x664d78 0x671885 0x6214e9 0x622e02 0x623aae 0x620314 0x46d781\n#       0x664d77        runtime/pprof.StartCPUProfile+0xf7      /home/lclipp/src/golang-1.21/src/runtime/pprof/pprof.go:793\n#       0x671884        net/http/pprof.Profile+0x2a4            /home/lclipp/src/golang-1.21/src/net/http/pprof/pprof.go:149\n#       0x6214e8        net/http.HandlerFunc.ServeHTTP+0x28     /home/lclipp/src/golang-1.21/src/net/http/server.go:2136\n#       0x622e01        net/http.(*ServeMux).ServeHTTP+0x141    /home/lclipp/src/golang-1.21/src/net/http/server.go:2514\n#       0x623aad        net/http.serverHandler.ServeHTTP+0x8d   /home/lclipp/src/golang-1.21/src/net/http/server.go:2938\n#       0x620313        net/http.(*conn).serve+0x5f3            /home/lclipp/src/golang-1.21/src/net/http/server.go:2009\n\n0: 0 [8797: 140752] @ 0x409593 0x40959e 0x46920b 0x49bf12 0x4b82e5 0x67466a 0x6747db 0x674ab3 0x43b5bb 0x46d781\n#       0x46920a        syscall.runtimeSetenv+0x2a      /home/lclipp/src/golang-1.21/src/runtime/runtime.go:130\n#       0x49bf11        syscall.Setenv+0x351            /home/lclipp/src/golang-1.21/src/syscall/env_unix.go:122\n#       0x4b82e4        os.Setenv+0x24                  /home/lclipp/src/golang-1.21/src/os/env.go:120\n#       0x674669        main.isSquare+0x69              /home/lclipp/git/LandonTClipp/LandonTClipp.github.io/code/profile-guided-optimizations/fermats-factorization/main.go:18\n#       0x6747da        main.findFactors+0xba           /home/lclipp/git/LandonTClipp/LandonTClipp.github.io/code/profile-guided-optimizations/fermats-factorization/main.go:33\n#       0x674ab2        main.main+0x272                 /home/lclipp/git/LandonTClipp/LandonTClipp.github.io/code/profile-guided-optimizations/fermats-factorization/main.go:61\n#       0x43b5ba        runtime.main+0x2ba              /home/lclipp/src/golang-1.21/src/runtime/proc.go:267\n\n0: 0 [44849: 717584] @ 0x409552 0x40959e 0x46920b 0x49bf12 0x4b82e5 0x67466a 0x6747db 0x674ab3 0x43b5bb 0x46d781\n#       0x46920a        syscall.runtimeSetenv+0x2a      /home/lclipp/src/golang-1.21/src/runtime/runtime.go:130\n#       0x49bf11        syscall.Setenv+0x351            /home/lclipp/src/golang-1.21/src/syscall/env_unix.go:122\n#       0x4b82e4        os.Setenv+0x24                  /home/lclipp/src/golang-1.21/src/os/env.go:120\n#       0x674669        main.isSquare+0x69              /home/lclipp/git/LandonTClipp/LandonTClipp.github.io/code/profile-guided-optimizations/fermats-factorization/main.go:18\n#       0x6747da        main.findFactors+0xba           /home/lclipp/git/LandonTClipp/LandonTClipp.github.io/code/profile-guided-optimizations/fermats-factorization/main.go:33\n#       0x674ab2        main.main+0x272                 /home/lclipp/git/LandonTClipp/LandonTClipp.github.io/code/profile-guided-optimizations/fermats-factorization/main.go:61\n#       0x43b5ba        runtime.main+0x2ba              /home/lclipp/src/golang-1.21/src/runtime/proc.go:267\n\n0: 0 [1: 208] @ 0x4bf45a 0x485171 0x4bf49e 0x4bf945 0x619131 0x5d19ca 0x61f46a 0x61f1aa 0x664965 0x664865 0x661326 0x672908 0x673405 0x6214e9 0x622e02 0x623aae 0x620314 0x46d781\n#       0x4bf459        fmt.glob..func1+0x19                    /home/lclipp/src/golang-1.21/src/fmt/print.go:147\n#       0x485170        sync.(*Pool).Get+0xb0                   /home/lclipp/src/golang-1.21/src/sync/pool.go:151\n#       0x4bf49d        fmt.newPrinter+0x1d                     /home/lclipp/src/golang-1.21/src/fmt/print.go:152\n#       0x4bf944        fmt.Fprintf+0x44                        /home/lclipp/src/golang-1.21/src/fmt/print.go:223\n#       0x619130        net/http.(*chunkWriter).Write+0xd0      /home/lclipp/src/golang-1.21/src/net/http/server.go:382\n#       0x5d19c9        bufio.(*Writer).Write+0xe9              /home/lclipp/src/golang-1.21/src/bufio/bufio.go:682\n#       0x61f469        net/http.(*response).write+0x229        /home/lclipp/src/golang-1.21/src/net/http/server.go:1648\n#       0x61f1a9        net/http.(*response).Write+0x29         /home/lclipp/src/golang-1.21/src/net/http/server.go:1606\n#       0x664964        runtime/pprof.writeGoroutineStacks+0xa4 /home/lclipp/src/golang-1.21/src/runtime/pprof/pprof.go:714\n#       0x664864        runtime/pprof.writeGoroutine+0x24       /home/lclipp/src/golang-1.21/src/runtime/pprof/pprof.go:692\n#       0x661325        runtime/pprof.(*Profile).WriteTo+0x145  /home/lclipp/src/golang-1.21/src/runtime/pprof/pprof.go:329\n#       0x672907        net/http/pprof.handler.ServeHTTP+0x4a7  /home/lclipp/src/golang-1.21/src/net/http/pprof/pprof.go:267\n#       0x673404        net/http/pprof.Index+0xe4               /home/lclipp/src/golang-1.21/src/net/http/pprof/pprof.go:384\n#       0x6214e8        net/http.HandlerFunc.ServeHTTP+0x28     /home/lclipp/src/golang-1.21/src/net/http/server.go:2136\n#       0x622e01        net/http.(*ServeMux).ServeHTTP+0x141    /home/lclipp/src/golang-1.21/src/net/http/server.go:2514\n#       0x623aad        net/http.serverHandler.ServeHTTP+0x8d   /home/lclipp/src/golang-1.21/src/net/http/server.go:2938\n#       0x620313        net/http.(*conn).serve+0x5f3            /home/lclipp/src/golang-1.21/src/net/http/server.go:2009\n\n\n# runtime.MemStats\n# Alloc = 2898784\n# TotalAlloc = 63342021312\n# Sys = 14128392\n# Lookups = 0\n# Mallocs = 4393313221\n# Frees = 4393172386\n# HeapAlloc = 2898784\n# HeapSys = 8060928\n# HeapIdle = 4669440\n# HeapInuse = 3391488\n# HeapReleased = 3219456\n# HeapObjects = 140835\n# Stack = 327680 / 327680\n# MSpan = 78960 / 130368\n# MCache = 1200 / 15600\n# BuckHashSys = 1447666\n# GCSys = 3582536\n# OtherSys = 563614\n# NextGC = 4194304\n# LastGC = 1693254042011951410\n# PauseNs = [15738 15147 18294 17224 18183 17745 19846 17127 16522 19752 18705 16005 16843 15907 18858 16735 16477 15636 15484 16719 18167 17087 21555 14918 19435 16284 16268 19483 16772 17313 17855 20875 20069 18587 19127 16946 26900 17360 18050 16686 18122 19277 20227 19745 17115 16540 15493 18496 18256 15830 19711 19601 16245 14577 20009 15254 13715 18802 19667 13109 13124 13934 15588 14388 18209 18991 14374 15740 14289 20901 17489 21279 19773 28754 21555 30303 17949 19346 18672 18419 19476 20368 18432 19456 20018 16892 18307 18167 20866 17833 22653 19734 19191 19368 21210 19033 22139 21464 19078 20702 16826 20748 17718 21021 24627 47368 14741 15481 23505 14277 14901 13454 11464 13308 9947 11432 12941 11486 11427 10886 8986 10753 10442 12987 12185 10501 10818 13286 19746 15229 17043 17441 17069 16595 17523 15014 13421 12410 14837 15645 16943 12835 15186 13843 11611 12016 10026 12923 18868 18457 14253 14538 12394 13432 11791 10838 9484 11557 8732 12277 12829 12696 13793 11357 13510 13163 13236 12809 10906 12536 10699 11082 12279 9137 13193 9978 12473 10930 11201 11821 11545 12614 12449 13316 14258 10000 11677 13725 10689 9826 11998 9937 11589 9960 9639 10718 12885 13390 17717 14061 14581 14990 15566 16201 16416 14345 18051 17004 15580 15666 14849 13756 12916 12604 15424 13942 11558 12558 12404 13323 12915 13685 13240 13354 16053 13123 15550 13330 13042 13153 15704 14922 16622 12242 12509 12051 11135 11342 8983 11569 11205 10889 18163 11581 12828 1947286 13709 10892 12980 13700 13958 21487 17441 18360 20350 16990]\n# PauseEnd = [1693254032356026598 1693254032434344190 1693254032511523723 1693254032580744959 1693254032658734338 1693254032739452192 1693254032838703547 1693254032915592440 1693254032986466109 1693254033057715755 1693254033131601198 1693254033200276633 1693254033279046899 1693254033359203407 1693254033436649675 1693254033500461415 1693254033580186855 1693254033653540387 1693254033721747647 1693254033782114169 1693254033862576624 1693254033926629869 1693254034006734174 1693254034068732411 1693254034147826819 1693254034207756249 1693254034285648043 1693254034365543898 1693254034430735834 1693254034506017919 1693254034582158839 1693254034650749932 1693254034727687928 1693254034788764427 1693254034871739486 1693254034952732009 1693254035012736393 1693254035069730538 1693254035133734285 1693254035190759959 1693254035250755401 1693254035331752146 1693254035410531611 1693254035489214917 1693254035553743678 1693254035632977911 1693254035708030771 1693254035776772543 1693254035834137128 1693254035878587056 1693254035956237696 1693254036030605817 1693254036065979564 1693254036097003971 1693254036155155766 1693254036192577304 1693254036237859347 1693254036382899771 1693254036539659402 1693254036579768118 1693254036615412063 1693254036649182708 1693254036718827411 1693254036781441392 1693254036903037348 1693254036984405072 1693254037024201893 1693254037069437637 1693254037104662311 1693254037205256113 1693254037426919129 1693254037567766316 1693254037628626055 1693254037709440070 1693254037913713487 1693254038133953584 1693254038214452113 1693254038253161537 1693254038301449662 1693254038355187073 1693254038439016055 1693254038497041412 1693254038533466162 1693254038570716819 1693254038617603722 1693254038648416198 1693254038688622609 1693254038725792731 1693254038761852241 1693254038789381796 1693254038829513811 1693254038869723637 1693254038909961560 1693254038946641780 1693254038981076167 1693254039010714206 1693254039049694000 1693254039084111165 1693254039111441626 1693254039151523229 1693254039185165311 1693254039211920753 1693254039252133346 1693254039286131480 1693254039312546528 1693254039353041425 1693254039377037498 1693254039417270815 1693254039452716868 1693254039477603551 1693254039517813064 1693254039549959214 1693254039578179371 1693254039615376283 1693254039638542497 1693254039678720407 1693254039713446968 1693254039739053325 1693254039778993746 1693254039812903507 1693254039839819226 1693254039877305107 1693254039900183277 1693254039940369744 1693254039974679793 1693254040000850394 1693254040040627081 1693254040073213634 1693254040101349456 1693254040141697270 1693254040177178811 1693254040212754112 1693254040242078487 1693254040282254879 1693254040319618150 1693254040353011987 1693254040383276105 1693254040423027125 1693254040459610131 1693254040483384551 1693254040523587854 1693254040560954603 1693254040597660290 1693254040624136362 1693254040664330925 1693254040698511048 1693254040724745881 1693254040766706246 1693254040800322354 1693254040825316448 1693254040865529006 1693254040903015532 1693254040925857119 1693254040966063670 1693254041003938905 1693254041036829938 1693254041066662480 1693254041104046373 1693254041126962908 1693254041167214757 1693254041206610355 1693254041242271171 1693254041267779503 1693254041308011635 1693254041343830717 1693254041380211158 1693254041408659283 1693254041447474010 1693254041479168472 1693254041521489531 1693254041549451298 1693254041586989505 1693254041620576389 1693254041649989018 1693254041685191913 1693254041710335896 1693254041749210376 1693254041770715879 1693254041810880301 1693254041845216005 1693254041871426172 1693254041911421497 1693254041941906383 1693254041971720900 1693254042011951410 1693254029656657195 1693254029696837290 1693254029731438302 1693254029757175524 1693254029796688326 1693254029828174248 1693254029857710025 1693254029894376039 1693254029918065455 1693254029958283518 1693254029998391468 1693254030038813154 1693254030078908471 1693254030112373391 1693254030139361020 1693254030179566187 1693254030215435878 1693254030239924585 1693254030280148602 1693254030318529558 1693254030355864341 1693254030380759129 1693254030420963274 1693254030461162676 1693254030498149066 1693254030532959926 1693254030561734507 1693254030601920225 1693254030648213404 1693254030683037433 1693254030722723163 1693254030757530021 1693254030783592465 1693254030823844974 1693254030864064641 1693254030897835285 1693254030924446424 1693254030964653818 1693254030999079738 1693254031025038086 1693254031065223466 1693254031101040397 1693254031125663530 1693254031165838255 1693254031200509043 1693254031226192180 1693254031266392838 1693254031301318161 1693254031326801974 1693254031367007899 1693254031407242017 1693254031444303050 1693254031467604280 1693254031507738480 1693254031544660337 1693254031568142921 1693254031608388878 1693254031646354839 1693254031668734796 1693254031708946426 1693254031747765010 1693254031781050463 1693254031809531109 1693254031849690064 1693254031884258829 1693254031930733410 1693254032010639093 1693254032071034570 1693254032151730742 1693254032211874819 1693254032292307352]\n# NumGC = 20665\n# NumForcedGC = 0\n# GCCPUFraction = 0.004821695679555479\n# DebugGC = false\n# MaxRSS = 12476416\n</code></pre> <pre><code>$ curl http://localhost:6060/debug/pprof/threadcreate?debug=1\nthreadcreate profile: total 6\n5 @\n#       0x0\n\n1 @ 0x43e985 0x43f2f5 0x43f5e5 0x43b513 0x46d781\n#       0x43e984        runtime.allocm+0xc4                     /home/lclipp/src/golang-1.21/src/runtime/proc.go:1935\n#       0x43f2f4        runtime.newm+0x34                       /home/lclipp/src/golang-1.21/src/runtime/proc.go:2398\n#       0x43f5e4        runtime.startTemplateThread+0x84        /home/lclipp/src/golang-1.21/src/runtime/proc.go:2475\n#       0x43b512        runtime.main+0x212                      /home/lclipp/src/golang-1.21/src/runtime/proc.go:239\n</code></pre> <p>The method to actually get a profile can be done using the <code>profile</code> endpoint. By default, hitting the endpoint will collect an endpoint over a period of 30 seconds, but this can be modified through the <code>seconds</code> query parameter. For example:</p> <pre><code>$ curl -o /dev/null -v http://localhost:6060/debug/pprof/profile?seconds=1\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0* About to connect() to localhost port 6060 (#0)\n*   Trying 127.0.0.1...\n* Connected to localhost (127.0.0.1) port 6060 (#0)\n&gt; GET /debug/pprof/profile?seconds=1 HTTP/1.1\n&gt; User-Agent: curl/7.29.0\n&gt; Host: localhost:6060\n&gt; Accept: */*\n&gt;\n  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0&lt; HTTP/1.1 200 OK\n&lt; Content-Disposition: attachment; filename=\"profile\"\n&lt; Content-Type: application/octet-stream\n&lt; X-Content-Type-Options: nosniff\n&lt; Date: Mon, 28 Aug 2023 20:25:04 GMT\n&lt; Transfer-Encoding: chunked\n&lt;\n{ [data not shown]\n100  3054    0  3054    0     0   2613      0 --:--:--  0:00:01 --:--:--  2625\n* Connection #0 to host localhost left intact\n</code></pre>"},{"location":"blog/2023/08/25/profile-guided-optimizations-in-go/#what-gets-optimized-and-why","title":"What gets optimized and why?","text":"<p>The Go compiler has this concept called an \"inlining budget\". The budget controls the maximum number of syntatical nodes that a function can have before it's considered not inlinable. A node is roughly analagous to the number of nodes in the program's Abstract Syntax Tree (AST), or rather each individual element of a piece of code, like a name declaration, an assignment, a constant declaration, function calls etc. By default, the <code>inlineMaxBudget</code> has a value of 80, which means that any function with more than 80 nodes are not inlinable. If you have profiled your program and the profiler has determined your particular function is \"hot\", then the budget increases to 2000.</p>"},{"location":"blog/2023/08/25/profile-guided-optimizations-in-go/#inlining","title":"Inlining","text":"<p>Inlining is a technique whereby a function call is replaced with, more or less, a copy-paste of the function's code into the calling function's frame. The alternative to inlining, which is just a normal function call, actually performs an enormous amount of work:</p> <ol> <li>Save the values of its working registers on the stack</li> <li>Prepare space on the stack for the called function, which could include space for the function to store its return values</li> <li>Push any functions arguments into registers (or on the stack if your arguments don't fit on registers)</li> <li><code>CALL</code> the function</li> <li>Restore the register values</li> <li>Retrieve the return values from the stack (or registers)</li> <li>Perform any ancillary cleanup work like cleaning up stack pointers</li> </ol> <p>Function calls are surprisingly complex, so if we can replace all of that work by pretending that the called function's code was inside of the caller's code, we can get some pretty significant speedups by skipping all of these bookkeeping tasks.</p>"},{"location":"blog/2023/08/25/profile-guided-optimizations-in-go/#pgoinlinebudget","title":"<code>pgoinlinebudget</code>","text":"<p>You can modify the inline budget set for hot functions using the <code>pgoinlinebudget</code> flag, for example:</p> Text Only<pre><code>$ go build -gcflags=\"-d=pgoinlinebudget=2000\" .\n</code></pre> <p>This is set to 2000 by default but you can specify any value you want. This name is a bit confusing because this does not control the \"non-hot\" budget, which appears can't be changed. It only controls the budget for functions that are considered hot.</p>"},{"location":"blog/2023/08/25/profile-guided-optimizations-in-go/#pgoinlinecdfthreshold","title":"<code>pgoinlinecdfthreshold</code>","text":"<p>The way to read this variable is \"Profile Guided Optimization Inline Cumulative Distribution Function Threshold\". Wow, what a mouthful! Simply put, this threshold sets the lower bound that the weight of a function must have in order to be considered hot. Or in other words, you can think of it as a percentage of the total runtime, whereby the functions whose edge weights represent the top 95% of total edge weights will be considered hot.</p> <p>Let's take a look at what this means in practice. We'll set a <code>pgoinlinecdfthreshold=95</code> and run the PGO build and graph the DOT notation(1) conveniently provided to us. Note that we've already generated one <code>default.pgo</code> profile by simply running the program without any optimizations applied.</p> <ol> <li>A DOT graph is simply a way you can specify directed graphs in a text format. This text can be rendered into a graph.</li> </ol>"},{"location":"blog/2023/08/25/profile-guided-optimizations-in-go/#95","title":"=95","text":"<p>Let's build the program with PGO enabled, and set <code>pgoinlinecdfthreshold=95</code>. Before we do that, we should generate a new <code>default.pgo</code> profile using the inlined profiler to give us a more accurate representation of how the code runs under <code>main()</code> (it might not actually be all that different but it's good to be thorough). Remember, we generate <code>default.go</code> by simply running our program:</p> Text Only<pre><code>$ ./fermats-factorization -n 179957108976619\nstarting CPU profile\nFound factors with i=42698929: 179957108976619 = 1627093 x 110600383\n$ go build -pgo=auto -gcflags=\"-d=pgoinlinecdfthreshold=95,pgodebug=3\" .\n</code></pre> <p>The build command outputs a graph in DOT notation. You can copy-paste the code in https://dreampuf.github.io/GraphvizOnline to create a diagram.</p> DOT graph visualizationDOT graph code <p></p> <pre><code>digraph G {\n    forcelabels=true;\n    \"flag.Uint64\" [color=black, style=solid, label=\"flag.Uint64,inl_cost=63\"];\n    \"main.runtimeProf.func1\" [color=black, style=solid, label=\"main.runtimeProf.func1\"];\n    \"fmt.Sprintf\" [color=black, style=solid, label=\"fmt.Sprintf\"];\n    \"os.Create\" [color=black, style=solid, label=\"os.Create,inl_cost=72\"];\n    \"flag.Parse\" [color=black, style=solid, label=\"flag.Parse,inl_cost=62\"];\n    \"runtime/pprof.StopCPUProfile\" [color=black, style=solid, label=\"runtime/pprof.StopCPUProfile\"];\n    \"math.Sqrt\" [color=black, style=solid, label=\"math.Sqrt,inl_cost=4\"];\n    \"main.NewExpensive\" [color=black, style=solid, label=\"main.NewExpensive\"];\n    \"main.runtimeProf\" [color=black, style=solid, label=\"main.runtimeProf\"];\n    \"log.Fatal\" [color=black, style=solid, label=\"log.Fatal\"];\n    \"main.main\" [color=black, style=solid, label=\"main.main\"];\n    \"flag.Int\" [color=black, style=solid, label=\"flag.Int,inl_cost=63\"];\n    \"os.Setenv\" [color=black, style=solid, label=\"os.Setenv,inl_cost=90\"];\n    \"main.findFactors\" [color=black, style=solid, label=\"main.findFactors\"];\n    \"fmt.Printf\" [color=black, style=solid, label=\"fmt.Printf,inl_cost=73\"];\n    \"flag.String\" [color=black, style=solid, label=\"flag.String,inl_cost=63\"];\n    \"strconv.Itoa\" [color=black, style=solid, label=\"strconv.Itoa,inl_cost=117\"];\n    \"main.httpProf.func1\" [color=black, style=solid, label=\"main.httpProf.func1\"];\n    \"net/http.ListenAndServe\" [color=black, style=solid, label=\"net/http.ListenAndServe,inl_cost=70\"];\n    \"flag.Bool\" [color=black, style=solid, label=\"flag.Bool,inl_cost=63\"];\n    \"math.Ceil\" [color=black, style=solid, label=\"math.Ceil,inl_cost=61\"];\n    \"main.runtimeProf.func2\" [color=black, style=solid, label=\"main.runtimeProf.func2\"];\n    \"os.(*File).Close\" [color=black, style=solid, label=\"os.(*File).Close,inl_cost=67\"];\n    \"main.isSquare\" [color=black, style=solid, label=\"main.isSquare\"];\n    \"fmt.Println\" [color=black, style=solid, label=\"fmt.Println,inl_cost=72\"];\n    \"main.httpProf\" [color=black, style=solid, label=\"main.httpProf\"];\n    \"log.Println\" [color=black, style=solid, label=\"log.Println,inl_cost=77\"];\n    \"runtime/pprof.StartCPUProfile\" [color=black, style=solid, label=\"runtime/pprof.StartCPUProfile\"];\n    edge [color=red, style=solid];\n    \"main.isSquare\" -&gt; \"math.Sqrt\" [label=\"0.21\"];\n    edge [color=black, style=solid];\n    \"main.isSquare\" -&gt; \"main.NewExpensive\" [label=\"0.07\"];\n    edge [color=black, style=solid];\n    \"main.isSquare\" -&gt; \"os.Setenv\" [label=\"0.15\"];\n    edge [color=red, style=solid];\n    \"main.isSquare\" -&gt; \"strconv.Itoa\" [label=\"0.24\"];\n    edge [color=red, style=solid];\n    \"main.findFactors\" -&gt; \"main.isSquare\" [label=\"0.27\"];\n    edge [color=black, style=solid];\n    \"main.findFactors\" -&gt; \"math.Sqrt\" [label=\"0.00\"];\n    edge [color=black, style=solid];\n    \"main.findFactors\" -&gt; \"math.Ceil\" [label=\"0.00\"];\n    edge [color=black, style=solid];\n    \"main.findFactors\" -&gt; \"math.Sqrt\" [label=\"0.00\"];\n    edge [color=red, style=solid];\n    \"main.findFactors\" -&gt; \"main.isSquare\" [label=\"0.23\"];\n    edge [color=black, style=solid];\n    \"main.runtimeProf\" -&gt; \"fmt.Println\" [label=\"0.00\"];\n    edge [color=black, style=solid];\n    \"main.runtimeProf\" -&gt; \"os.Create\" [label=\"0.00\"];\n    edge [color=black, style=solid];\n    \"main.runtimeProf\" -&gt; \"log.Fatal\" [label=\"0.00\"];\n    edge [color=black, style=solid];\n    \"main.runtimeProf\" -&gt; \"runtime/pprof.StartCPUProfile\" [label=\"0.00\"];\n    edge [color=black, style=solid];\n    \"main.runtimeProf\" -&gt; \"log.Fatal\" [label=\"0.00\"];\n    edge [color=black, style=solid];\n    \"main.runtimeProf.func2\" -&gt; \"os.(*File).Close\" [label=\"0.00\"];\n    edge [color=black, style=solid];\n    \"main.runtimeProf.func2\" -&gt; \"runtime/pprof.StopCPUProfile\" [label=\"0.00\"];\n    edge [color=black, style=solid];\n    \"main.httpProf\" -&gt; \"main.httpProf.func1\" [label=\"0.00\"];\n    edge [color=black, style=solid];\n    \"main.httpProf.func1\" -&gt; \"log.Println\" [label=\"0.00\"];\n    edge [color=black, style=solid];\n    \"main.httpProf.func1\" -&gt; \"net/http.ListenAndServe\" [label=\"0.00\"];\n    edge [color=black, style=solid];\n    \"main.main\" -&gt; \"flag.Uint64\" [label=\"0.00\"];\n    edge [color=black, style=solid];\n    \"main.main\" -&gt; \"flag.Parse\" [label=\"0.00\"];\n    edge [color=black, style=solid];\n    \"main.main\" -&gt; \"main.runtimeProf\" [label=\"0.00\"];\n    edge [color=black, style=solid];\n    \"main.main\" -&gt; \"main.findFactors\" [label=\"0.00\"];\n    edge [color=black, style=solid];\n    \"main.main\" -&gt; \"fmt.Sprintf\" [label=\"0.00\"];\n    edge [color=black, style=solid];\n    \"main.main\" -&gt; \"flag.String\" [label=\"0.00\"];\n    edge [color=black, style=solid];\n    \"main.main\" -&gt; \"flag.Bool\" [label=\"0.00\"];\n    edge [color=black, style=solid];\n    \"main.main\" -&gt; \"flag.Int\" [label=\"0.00\"];\n    edge [color=black, style=solid];\n    \"main.main\" -&gt; \"main.httpProf\" [label=\"0.00\"];\n    edge [color=black, style=solid];\n    \"main.main\" -&gt; \"main.findFactors\" [label=\"0.12\"];\n    edge [color=black, style=solid];\n    \"main.main\" -&gt; \"fmt.Printf\" [label=\"0.00\"];\n}\n</code></pre> <p>You can see here that the PGO determined the path in red is considered hot because its weight exceeds the calculated hot callsite threshold:</p> Text Only<pre><code>$ go build -pgo=auto -gcflags=\"-d=pgoinlinecdfthreshold=95,pgodebug=3\" . |&amp; grep hot-callsite-thres-from-CDF\nhot-callsite-thres-from-CDF=0.18328445747800587\n</code></pre> <p>One of the things you might notice is that the profiler is smart enough to distinguish between different calls to the same function. You'll see there are two lines going from <code>main.findFactors</code> to <code>main.isSquare</code>, because there are two separate <code>CALL</code> instructions (one within the for loop, and another bare call).</p>"},{"location":"blog/2023/08/25/profile-guided-optimizations-in-go/#80","title":"=80","text":"<p>If we decrease the pgoinlinecdfthreshold value to something like 80, we see a dramatically different result:</p> Text Only<pre><code>$ go build -pgo=auto -gcflags=\"-d=pgoinlinecdfthreshold=80,pgodebug=3\" . |&amp; grep hot-callsite-thres-from-CDF\nhot-callsite-thres-from-CDF=1.935483870967742\n</code></pre> DOT graph visualizationDOT graph code <p>And the visualization shows us that none of the paths are considered hot(1) because none of them are above a weight of 1.935483870967742:</p> <ol> <li>To be clear, and to prevent confusion, there are indeed some nodes that are still being marked as hot, but these nodes all live within the Go runtime, and thus are not being included in this visualization. Setting lower threshold values will still guarantee at least one node is marked as hot.</li> </ol> <p></p> Text Only<pre><code>    digraph G {\n            forcelabels=true;\n            \"log.Println\" [color=black, style=solid, label=\"log.Println,inl_cost=77\"];\n            \"flag.String\" [color=black, style=solid, label=\"flag.String,inl_cost=63\"];\n            \"flag.Bool\" [color=black, style=solid, label=\"flag.Bool,inl_cost=63\"];\n            \"main.runtimeProf.func2\" [color=black, style=solid, label=\"main.runtimeProf.func2\"];\n            \"main.runtimeProf.func1\" [color=black, style=solid, label=\"main.runtimeProf.func1\"];\n            \"os.(*File).Close\" [color=black, style=solid, label=\"os.(*File).Close,inl_cost=67\"];\n            \"os.Setenv\" [color=black, style=solid, label=\"os.Setenv,inl_cost=90\"];\n            \"fmt.Println\" [color=black, style=solid, label=\"fmt.Println,inl_cost=72\"];\n            \"fmt.Printf\" [color=black, style=solid, label=\"fmt.Printf,inl_cost=73\"];\n            \"flag.Int\" [color=black, style=solid, label=\"flag.Int,inl_cost=63\"];\n            \"runtime/pprof.StopCPUProfile\" [color=black, style=solid, label=\"runtime/pprof.StopCPUProfile\"];\n            \"main.findFactors\" [color=black, style=solid, label=\"main.findFactors\"];\n            \"fmt.Sprintf\" [color=black, style=solid, label=\"fmt.Sprintf\"];\n            \"math.Sqrt\" [color=black, style=solid, label=\"math.Sqrt,inl_cost=4\"];\n            \"log.Fatal\" [color=black, style=solid, label=\"log.Fatal\"];\n            \"main.main\" [color=black, style=solid, label=\"main.main\"];\n            \"main.httpProf\" [color=black, style=solid, label=\"main.httpProf\"];\n            \"main.NewExpensive\" [color=black, style=solid, label=\"main.NewExpensive\"];\n            \"os.Create\" [color=black, style=solid, label=\"os.Create,inl_cost=72\"];\n            \"main.httpProf.func1\" [color=black, style=solid, label=\"main.httpProf.func1\"];\n            \"main.runtimeProf\" [color=black, style=solid, label=\"main.runtimeProf\"];\n            \"net/http.ListenAndServe\" [color=black, style=solid, label=\"net/http.ListenAndServe,inl_cost=70\"];\n            \"flag.Uint64\" [color=black, style=solid, label=\"flag.Uint64,inl_cost=63\"];\n            \"math.Ceil\" [color=black, style=solid, label=\"math.Ceil,inl_cost=61\"];\n            \"main.isSquare\" [color=black, style=solid, label=\"main.isSquare\"];\n            \"strconv.Itoa\" [color=black, style=solid, label=\"strconv.Itoa,inl_cost=117\"];\n            \"runtime/pprof.StartCPUProfile\" [color=black, style=solid, label=\"runtime/pprof.StartCPUProfile\"];\n            \"flag.Parse\" [color=black, style=solid, label=\"flag.Parse,inl_cost=62\"];\n            edge [color=black, style=solid];\n            \"main.isSquare\" -&gt; \"math.Sqrt\" [label=\"0.21\"];\n            edge [color=black, style=solid];\n            \"main.isSquare\" -&gt; \"main.NewExpensive\" [label=\"0.07\"];\n            edge [color=black, style=solid];\n            \"main.isSquare\" -&gt; \"os.Setenv\" [label=\"0.15\"];\n            edge [color=black, style=solid];\n            \"main.isSquare\" -&gt; \"strconv.Itoa\" [label=\"0.24\"];\n            edge [color=black, style=solid];\n            \"main.findFactors\" -&gt; \"math.Sqrt\" [label=\"0.00\"];\n            edge [color=black, style=solid];\n            \"main.findFactors\" -&gt; \"main.isSquare\" [label=\"0.23\"];\n            edge [color=black, style=solid];\n            \"main.findFactors\" -&gt; \"main.isSquare\" [label=\"0.27\"];\n            edge [color=black, style=solid];\n            \"main.findFactors\" -&gt; \"math.Sqrt\" [label=\"0.00\"];\n            edge [color=black, style=solid];\n            \"main.findFactors\" -&gt; \"math.Ceil\" [label=\"0.00\"];\n            edge [color=black, style=solid];\n            \"main.runtimeProf\" -&gt; \"fmt.Println\" [label=\"0.00\"];\n            edge [color=black, style=solid];\n            \"main.runtimeProf\" -&gt; \"os.Create\" [label=\"0.00\"];\n            edge [color=black, style=solid];\n            \"main.runtimeProf\" -&gt; \"log.Fatal\" [label=\"0.00\"];\n            edge [color=black, style=solid];\n            \"main.runtimeProf\" -&gt; \"runtime/pprof.StartCPUProfile\" [label=\"0.00\"];\n            edge [color=black, style=solid];\n            \"main.runtimeProf\" -&gt; \"log.Fatal\" [label=\"0.00\"];\n            edge [color=black, style=solid];\n            \"main.runtimeProf.func2\" -&gt; \"os.(*File).Close\" [label=\"0.00\"];\n            edge [color=black, style=solid];\n            \"main.runtimeProf.func2\" -&gt; \"runtime/pprof.StopCPUProfile\" [label=\"0.00\"];\n            edge [color=black, style=solid];\n            \"main.httpProf\" -&gt; \"main.httpProf.func1\" [label=\"0.00\"];\n            edge [color=black, style=solid];\n            \"main.httpProf.func1\" -&gt; \"log.Println\" [label=\"0.00\"];\n            edge [color=black, style=solid];\n            \"main.httpProf.func1\" -&gt; \"net/http.ListenAndServe\" [label=\"0.00\"];\n            edge [color=black, style=solid];\n            \"main.main\" -&gt; \"flag.String\" [label=\"0.00\"];\n            edge [color=black, style=solid];\n            \"main.main\" -&gt; \"flag.Parse\" [label=\"0.00\"];\n            edge [color=black, style=solid];\n            \"main.main\" -&gt; \"main.runtimeProf\" [label=\"0.00\"];\n            edge [color=black, style=solid];\n            \"main.main\" -&gt; \"main.findFactors\" [label=\"0.00\"];\n            edge [color=black, style=solid];\n            \"main.main\" -&gt; \"main.findFactors\" [label=\"0.12\"];\n            edge [color=black, style=solid];\n            \"main.main\" -&gt; \"fmt.Printf\" [label=\"0.00\"];\n            edge [color=black, style=solid];\n            \"main.main\" -&gt; \"flag.Uint64\" [label=\"0.00\"];\n            edge [color=black, style=solid];\n            \"main.main\" -&gt; \"flag.Int\" [label=\"0.00\"];\n            edge [color=black, style=solid];\n            \"main.main\" -&gt; \"main.httpProf\" [label=\"0.00\"];\n            edge [color=black, style=solid];\n            \"main.main\" -&gt; \"fmt.Sprintf\" [label=\"0.00\"];\n            edge [color=black, style=solid];\n            \"main.main\" -&gt; \"flag.Bool\" [label=\"0.00\"];\n    }\n</code></pre> <p>Question</p> <p>You might be wondering why a lower CDF threshold results in less functions being marked as hot. You have to remember that the CDF itself is derived from a sorted list of edge weights in descending order. The compiler iterates over this sorted list and keeps track of the cumulative sum. It generates a cumulative percentage (which is quite literally what the CDF is) and compares that to the target threshold that we specified. If the CDF is greater than the threshold specified, it returns the edge weight of the node that caused us to go over the CDF threshold (which becomes the <code>hot-callsite-thres-from-CDF</code> value) and the list of nodes up to that point.</p> <p>If we specify a CDF threshold of 50%, but the largest node in our sorted list is, say, 90% of the CDF, then the only node that will be considered hot would be that single large node. You'll notice that our <code>80%</code> case above shows no node being considered hot, but this is probably not true. I'm guessing that there is some node (possibly inside of the Go runtime) that is being marked hot, while none of our user code is.</p> <p>As you'll see farther down in Proving the CDF experimentally, we'll modify the Go compiler so it tells us what node exceeded the threshold!</p>"},{"location":"blog/2023/08/25/profile-guided-optimizations-in-go/#what-is-a-cdf","title":"What is a CDF?","text":"<p>Cumulative Distribution Functions are mathematical models that tell you the probability <code>y</code> that some random variable <code>X</code> will take on a value less than or equal to the value on the <code>x</code> axis. In statistics, this can be used, for example, to find the probability that you will draw from a deck of cards the value between 2-8 (aces high). The CDF for such a scenaio is quite simple, since the probability of drawing any particular valued card is uniformly distributed at 1/13. Thus the CDF is:</p> \\[ F_X(x) = \\begin{cases} \\frac{x-1}{13} &amp;:\\ 2 \\le x \\le 14 \\end{cases} \\]"},{"location":"blog/2023/08/25/profile-guided-optimizations-in-go/#cdf-for-function-hotness","title":"CDF For Function Hotness","text":"<p>For the purposes of determining function hotness, we're looking at a CDF from a slightly different perspective. We're asking the question: \"given a certain percentage \\(p\\) (that being percentage of runtime), what is the edge weight threshold \\(F_h(p)\\) such that the sum of all edge weights at or above \\(F_h(p)\\) equals \\(p\\) percentage of the total program runtime?\" The answer \\(F_h(p)\\) is the <code>hot-callsite-thres-from-CDF</code> value we saw Go print out, and \\(p\\) is the <code>pgoinlinecdfthreshold</code> value we specified to the build process. We can mathematically describe our situation:</p> \\[ W = \\{w_0, w_1, ... w_n\\} \\] \\[ w_i \\gt w_{i+1} \\] <p>Where \\(W\\) is the set of all edge weights in a program, ordered by descending value. We first need to find the value of positive integer \\(m\\) such that the sum of the weights up to \\(m\\) is approximately \\(p\\). Why? Well because that's what the user is asking for when they specify <code>pgoinlinecdfthreshold</code>, they are asking the question \"what nodes do I need to select (ordered by weight) such that their cumulative weight is approximately equal to <code>pgoinlinecdfthreshold</code>?\"</p> <p>This can be represented as</p> \\[ m \\in \\mathbb{W} \\] \\[ F_h(W, p) = \\frac{W_m}{\\sum W} \\quad \\textrm{s.t.} \\quad \\frac{\\sum_{i=0}^{min(m)} W_i}{\\sum W} \\gt p \\] <p>We select the smallest possible value \\(m\\) that satisfies the inequality. I'm not sure if this is the most succinct way of describing this model but I'm not a mathemetician so you'll have to bear with me  In English, \\(F_h(W, p)\\) is the weight of the node at \\(W_m\\) divided by the sum of all weights, such that the sum of the nodes from \\(0\\) to \\(m\\), divided by the sum of all the weights, is greater than \\(p\\).</p> <p>You can see the Go PGO logic implements this function here. More specifically, you can see that the returned <code>pgoinlinecdfthreshold</code> is indeed the percentage of the total edge weight that \\(W_m\\) represents.</p>"},{"location":"blog/2023/08/25/profile-guided-optimizations-in-go/#devirtualization","title":"Devirtualization","text":"<p>Another optimization technique that takes advantage of PGO is what's called devirutalization. In Go, interfaces provide a virtualized way of accessing an implementation that might not be known at compile time. These virutalized calls are inefficient because they involve jump tables that must be travesed in order to call the implementation's method during runtime. Interfaces are also problematic because it defeats a lot of other analysis techniques like heap escapes.</p> <p>We can still run profiles to see what concrete implementation in practice gets used the most. The compiler will then create a small bit of if/else logic in the assembled code to do type assertions on the interface for the set of \"hot\" implementations found during profiling. If one of the type assertions succeeds, it will call that implementation's method directly.</p> <p>We will not dive deeply into this specific optimization technique, but it's something to keep in mind and highlights various ways in which PGO can be leveraged.</p>"},{"location":"blog/2023/08/25/profile-guided-optimizations-in-go/#proving-the-cdf-experimentally","title":"Proving the CDF experimentally","text":"<p>Let's go back to our examples where we modified <code>pgoinlinedthreshold</code>. The calculated threshold value was <code>0.18328445747800587</code>, which according to the Go PGO code is the percentage of \\(W_m\\) over the sum of all edge weights. The PGO logic does not have any debug statements that tells us what the total cumulative weight is, so let's modify the Go source code with some additional print statements so we can confirm our calculations.</p> src/cmd/compile/internal/inline/inl.go<pre><code>diff --git a/src/cmd/compile/internal/inline/inl.go b/src/cmd/compile/internal/inline/inl.go\nindex 4ae7fa95d2..56fdcfb099 100644\n--- a/src/cmd/compile/internal/inline/inl.go\n+++ b/src/cmd/compile/internal/inline/inl.go\n@@ -89,6 +89,7 @@ func pgoInlinePrologue(p *pgo.Profile, decls []ir.Node) {\n        inlineHotCallSiteThresholdPercent, hotCallsites = hotNodesFromCDF(p)\n        if base.Debug.PGODebug &gt; 0 {\n                fmt.Printf(\"hot-callsite-thres-from-CDF=%v\\n\", inlineHotCallSiteThresholdPercent)\n+               fmt.Printf(\"total-edge-weight=%v\\n\", p.TotalEdgeWeight)\n        }\n\n        if x := base.Debug.PGOInlineBudget; x != 0 {\n@@ -145,6 +146,12 @@ func hotNodesFromCDF(p *pgo.Profile) (float64, []pgo.NodeMapKey) {\n                w := p.NodeMap[n].EWeight\n                cum += w\n                if pgo.WeightInPercentage(cum, p.TotalEdgeWeight) &gt; inlineCDFHotCallSiteThresholdPercent {\n+                       fmt.Printf(\"node-that-exceeded-threshold-caller-name=%v\\n\", n.CallerName)\n+                       fmt.Printf(\"node-that-exceeded-threshold-callee-name=%v\\n\", n.CalleeName)\n+                       fmt.Printf(\"node-that-exceeded-threshold-edge-weight=%v\\n\", w)\n+                       fmt.Printf(\"node-plus-one-caller-name=%v\\n\", nodes[i+1].CallerName)\n+                       fmt.Printf(\"node-plus-one-callee-name=%v\\n\", nodes[i+1].CalleeName)\n+                       fmt.Printf(\"node-plus-one-edge-weight=%v\\n\", p.NodeMap[nodes[i+1]].EWeight)\n                        // nodes[:i+1] to include the very last node that makes it to go over the threshold.\n                        // (Say, if the CDF threshold is 50% and one hot node takes 60% of weight, we want to\n                        // include that node instead of excluding it.)\n</code></pre> <p>After compiling Go from source, we can run our build command again</p> Text Only<pre><code>$ ./goroot/bin/go build -pgo=auto -gcflags=\"-d=pgoinlinecdfthreshold=95,pgodebug=3\" .\n[...]\nnode-that-exceeded-threshold-caller-name=runtime.markrootBlock\nnode-that-exceeded-threshold-callee-name=runtime.scanblock\nnode-that-exceeded-threshold-edge-weight=25\nnode-plus-one-caller-name=syscall.Setenv\nnode-plus-one-callee-name=syscall.runtimeSetenv\nnode-plus-one-edge-weight=25\nhot-callsite-thres-from-CDF=0.18328445747800587\ntotal-edge-weight=13640\n</code></pre> <p>We can see that the node which caused the cumulative distribution to exceed the threshold was <code>runtime.scanblock</code>. Because it's part of the runtime, it was probably not included in our graph visualization. We can see that \\(\\frac{25}{13640}*100\\%=0.18328445747800587\\%\\) so it matches exactly the numbers that we're getting from <code>hot-callsite-thres-from-CDF</code>, which is no surprise.</p>"},{"location":"blog/2023/08/25/profile-guided-optimizations-in-go/#viewing-the-assembly","title":"Viewing the assembly","text":"<p>Let's have some fun and convince ourselves on what's really going on here. Sure these nice pretty graphs tell us that the PGO has inlined certain function calls, but why don't we take a look at the raw assembly code? First, let's look at the unoptimzed executable by building it with PGO turned off:</p> Text Only<pre><code>$ go build -pgo=off\n$ go tool objdump ./fermats-factorization |&amp; less\n</code></pre> <p>By grepping for <code>main.go:34</code> we indeed find the location where <code>main.isSquare</code> is called on the function stack:</p> Text Only<pre><code>  main.go:34            0x6741a7                e814feffff              CALL main.isSquare(SB)\n</code></pre> <p>Let's build this again with PGO turned on, and for fun let's just rely on the default PGO values:</p> Text Only<pre><code>$ go build -pgo=auto -gcflags=\"-d=pgodebug=1\" .  |&amp; grep isSquare\nhot-node enabled increased budget=2000 for func=main.isSquare\nhot-budget check allows inlining for call main.NewExpensive (cost 130) at ./main.go:17:27 in function main.isSquare\nhot-budget check allows inlining for call strconv.Itoa (cost 117) at ./main.go:18:43 in function main.isSquare\nhot-budget check allows inlining for call os.Setenv (cost 90) at ./main.go:18:11 in function main.isSquare\nhot-budget check allows inlining for call main.isSquare (cost 368) at ./main.go:34:12 in function main.findFactors\nhot-budget check allows inlining for call main.isSquare (cost 368) at ./main.go:36:14 in function main.findFactors\n</code></pre> <p>Great! Even with the default parameters it still shows <code>main.isSquare</code> is allowed to be inlined. The graph visualization agrees:</p> <p></p> <p>What does the assembly say?</p> Text Only<pre><code>$ go tool objdump -s 'main.findFactors' ./fermats-factorization\n  main.go:30            0x6762e5                31f6                    XORL SI, SI\n  main.go:33            0x6762e7                e91a010000              JMP 0x676406\n  main.go:12            0x6762ec                4885c9                  TESTQ CX, CX\n  main.go:12            0x6762ef                7c0a                    JL 0x6762fb\n  main.go:12            0x6762f1                0f57c0                  XORPS X0, X0\n  main.go:12            0x6762f4                f2480f2ac1              CVTSI2SDQ CX, X0\n  main.go:12            0x6762f9                eb18                    JMP 0x676313\n  main.go:12            0x6762fb                4889ce                  MOVQ CX, SI\n  main.go:12            0x6762fe                83e101                  ANDL $0x1, CX\n</code></pre> <p>We indeed see that the code in <code>isSquare</code> is being inlined directly in the assembly for <code>main.findFactors</code>.</p>"},{"location":"blog/2023/08/25/profile-guided-optimizations-in-go/#conclusion","title":"Conclusion","text":"<p>PGO is a really effective tool you can use to provide the compiler real-world examples of your code's CPU profile in a production system. The optimizations it provides are significant and are definitely worth the effort if reducing the latency in your applications is something you value. Let me know in the comments below what you think, and please do let me know if you see any errors that need correcting!</p>"},{"location":"blog/2023/09/13/grafana-live--chicago-2023/","title":"Grafana Live | Chicago 2023","text":"<p>These are my notes from the GrafanaLive workshop held in the Google offices in Chicago.</p>"},{"location":"blog/2023/09/13/grafana-live--chicago-2023/#promql-basics","title":"PromQL Basics","text":"<p>The workshop requires familiarizing one's self with PromQL. Many of the following notes will be copy-pasted.</p> <p>https://prometheus.io/docs/prometheus/latest/querying/basics/</p>"},{"location":"blog/2023/09/13/grafana-live--chicago-2023/#expression-result-types","title":"Expression result types","text":"<ul> <li>Instant vector - a set of time series containing a single sample for each time series, all sharing the same timestamp</li> <li>Range vector - a set of time series containing a range of data points over time for each time series</li> <li>Scalar - a simple numeric floating point value</li> <li>String - a simple string value; currently unused</li> </ul>"},{"location":"blog/2023/09/13/grafana-live--chicago-2023/#instant-vector-selectors","title":"Instant vector selectors","text":"<p>Using an example metrics name </p> Text Only<pre><code>http_requests_total\n</code></pre> <p>You can filter based off of specific tags</p> Text Only<pre><code>http_requests_total{job=\"prometheus\",group=\"canary\"}\n</code></pre> <p>You can exclude certain filters</p> <ul> <li>=: Select labels that are exactly equal to the provided string.</li> <li>!=: Select labels that are not equal to the provided string.</li> <li>=~: Select labels that regex-match the provided string.</li> <li>!~: Select labels that do not regex-match the provided string.</li> </ul>"},{"location":"blog/2023/09/13/grafana-live--chicago-2023/#range-vector-selectors","title":"Range Vector Selectors","text":"<p>In this example, we select all the values we have recorded within the last 5 minutes for all time series that have the metric name http_requests_total and a job label set to prometheus:</p> Text Only<pre><code>http_requests_total{job=\"prometheus\"}[5m]\n</code></pre>"},{"location":"blog/2023/09/13/grafana-live--chicago-2023/#agenda","title":"Agenda","text":"<ul> <li>Grafana Labs Stack</li> <li>loki</li> <li>demo</li> <li>setup</li> <li>breakout 1 - effective troubleshooting and debugging</li> <li>Use case: ad-hoc metrics</li> <li>Breakout 2 - loki v2 real time and high cardinality metrics</li> <li>news/updates</li> <li>wrap up</li> </ul> <p>What is loki?</p> <p>It's a log aggregation and querying platform, similar in some ways to splunk.</p>"},{"location":"blog/2023/09/13/grafana-live--chicago-2023/#loki-workshop-breakout","title":"Loki Workshop Breakout","text":"<p>https://github.com/grafana/loki_workshop_breakout</p>"},{"location":"blog/2023/09/13/grafana-live--chicago-2023/#4-types-of-observability-data","title":"4 Types of Observability Data:","text":"<ol> <li>logs</li> <li>metrics</li> <li>traces</li> <li>profiles</li> </ol>"},{"location":"blog/2023/09/13/grafana-live--chicago-2023/#understanding-slas-slos-slis","title":"Understanding SLAs, SLOs, SLIs","text":"<ul> <li>SLA: Service Level Agreement. The agreement you make with your customers that is often contractually guaranteed.<ul> <li>Example: 99.5% guaranteed system availability per month</li> </ul> </li> <li>SLI: Service Level Indicator. A frequently run probe of a user-centric KPIs to measure service or application health.<ul> <li>Example: % of successful and performant requests</li> <li>These are the customer-centric metric being measured</li> </ul> </li> <li>SLO: Service Level Objective. <ul> <li>Example: 99.9% of requests to a web service return without errors.</li> </ul> </li> </ul> <p>The SLO is simply the SLI threshold you are striving for.</p>"},{"location":"blog/2023/09/13/grafana-live--chicago-2023/#sli-methodology","title":"SLI Methodology","text":"<p>You can have multiple measurements that comprise your SLI. For example</p> <ul> <li>Rate: requests per second</li> <li>Errors: The number of those requests that are failing</li> <li>Duration: The amount of time requests take, distribution of latency measurements</li> </ul> <p>There is a concept of Error Budget, which means how many errors you're willing to tolerate. For example, we may tolerate .1% of errored requests over a 1 month period. We can measure this by splitting between fast burn rates and slow burn rates. </p> <ul> <li>Fast burn: 10% of requests fail over a window of the last 2 days</li> <li>Slow burn: 2% of requests fail over a window of the last 15 days.</li> </ul>"},{"location":"blog/2023/09/13/grafana-live--chicago-2023/#grafana-slo-the-easy-button","title":"Grafana SLO - The Easy Button","text":"<ul> <li>Grafana gives you UIs to create an SLO</li> <li>Create dashboard showing details of SLO</li> <li>A set of multi-window, multi-burn rate alerts</li> </ul>"},{"location":"blog/2023/12/01/worse-is-better/","title":"Worse is Better","text":"<p>When I design software systems, my modus operandi is generally to design the minimally acceptable set of features required by the business needs. This methodology allows me to be efficient with my time as I am focusing my energy on the known business problems. It also reduces the expectations your users have which is strangely a huge benfit: lower expectations usually translates to lower maintenance burdens. It allows you to focus your effort on the biggest problems your users face.</p> <p>I discovered that this methodology has a name: Worse is Better.<sup>1</sup> Coined by Richard P. Gabriel in a 1989 essay, it makes the assertion that at a certain scale, having less functionality in your software is preferrable to having more. In my own words, the reason for this is as follows:</p> <ol> <li>Simple software is easier to write. When it's easier to write, that means you are faster to market. It's a fact of life that the first implementation is in many situations the one that grabs the largest market share.</li> <li>Simple software is easier to test, which by extension makes your functionality more reliable.</li> <li>Simple software is easier to maintain. Less features means less work!</li> </ol> <p>The core tenants of Worse is Better emphasize the simplicity of the implementation:</p> <p>Core Tenants</p>"},{"location":"blog/2023/12/01/worse-is-better/#core-tenants","title":"Core Tenants","text":""},{"location":"blog/2023/12/01/worse-is-better/#simplicity","title":"Simplicity","text":"<p>The design must be simple, both in implementation and interface. It is more important for the implementation to be simpler than the interface. Simplicity is the most important consideration in a design.</p>"},{"location":"blog/2023/12/01/worse-is-better/#correctness","title":"Correctness","text":"<p>The design should be correct in all observable aspects. It is slightly better to be simple than correct.</p>"},{"location":"blog/2023/12/01/worse-is-better/#consistency","title":"Consistency","text":"<p>The design must not be overly inconsistent. Consistency can be sacrificed for simplicity in some cases, but it is better to drop those parts of the design that deal with less common circumstances than to introduce either complexity or inconsistency in the implementation.</p>"},{"location":"blog/2023/12/01/worse-is-better/#completeness","title":"Completeness","text":"<p>The design must cover as many important situations as is practical. All reasonably expected cases should be covered. Completeness can be sacrificed in favor of any other quality. In fact, completeness must be sacrificed whenever implementation simplicity is jeopardized. Consistency can be sacrificed to achieve completeness if simplicity is retained; especially worthless is consistency of interface.</p>"},{"location":"blog/2023/12/01/worse-is-better/#mockery","title":"mockery","text":"<p>I relate this philosphy to my stewardship of the mockery project. For those unfamiliar with mockery, this project is a code generation tool that generates mock implementations of Go interfaces. The mocks can be configured in your test code to return certain values when certain arguments are passed to it, as well as asserting various things about how the mock was called (which obviously is a controversial thing to assert, but I digress ).</p> <p>All of this sounds well and dandy until you start getting into the edge cases. For example, how do you handle argument types? If you decide to make expectations use the same type as the interface method, then how do you handle cases where you want to specify <code>mock.Anything</code> for one of the arguments (hint: you can't). Instead what mockery makes the <code>.EXPECT()</code> method arguments use <code>interface{}</code> so that you can specify <code>mock.Anything</code> (which is of type <code>string</code>), or a value of the type in your interface definition. These two cases have to be differentiated during runtime, which means your compiler can't help you if you made an uh-oh and provided a totally erroneous type. Yuck! </p> mockery mocks<pre><code>// An example you might run across in mockery.\ntype Getter interface{\n    Get(id int) string\n}\n\n// The function below is what you'd use to define expectations. Note `id` is `interface{}`, not `int`!\nfunc (_e *Requester_Expecter) Get(id interface{}) *Requester_Get_Call {\n    return &amp;Requester_Get_Call{Call: _e.mock.On(\"Get\", path)}\n}\n\n// The test might look like:\nfunc TestGetter(t *testing.T) {\n    mockGetter := NewMockGetter(t)\n    mockGetter.EXPECT(0).Return(\"foo\")\n    mockGetter.EXPECT(1).Return(\"bar\")\n    if getFromDB(mockGetter) != 1 {\n        t.Errorf(\"you twat\")\n    }\n}\n\nfunc getFromDB(getter Getter) string {\n    return getter.Get(1)\n}\n</code></pre> <p>Armed with hindsight, I realize that mockery is not simple in its implementation. Its interface is indeed quite simple and intuitive, but the fact that its implementation is so complex means that we are sometimes plagued by edge cases and bugs (which I've diligently worked to address, and successfully might I add). A more simple code-generation implementation I've found is Mat Ryer's own moq repo. Instead of matching arguments to return values, users of Mat's moq only have to assign an anonymous function to the appropriate attribute of the mock struct.</p>"},{"location":"blog/2023/12/01/worse-is-better/#moq","title":"moq","text":"moq mocks<pre><code>type Getter inferface {\n    Get(id int) string\n}\n\n// We assume the mock object has already been made\nfunc TestGet(t *testing.T) {\n    mockGetter := &amp;GetterMock{\n        GetFunc: func(id int) string {\n            if id == 0 {\n                return \"foo\"\n            }\n            return \"bar\"\n        }\n    }\n    if getFromDB(mockGetter) != \"bar\" {\n        t.Errorf(\"you muppet\")\n    }\n}\n</code></pre> <p>With <code>moq</code>, there is no fancy argument matching, no crazy semantics, no blackbox magic happening. By all accounts it's \"worse\" than mockery in the sense that it doesn't have as many features, but the benefit is that its maintainers don't have to do as much work as I do (I'm hoping) to keep the project running.</p> <p>I didn't intend for this to turn into a diatribe on the subtlties of Go mocks, but it's a real world application I've found where there is concrete evidence that Worse is indeed (sometimes... many times) Better.</p>"},{"location":"blog/2023/12/01/worse-is-better/#postscriptum","title":"postscriptum","text":"<p>\"Worse is Better\" is obviously a click-baity name for the idea, which is a nice reminder that clickbait long predates the cesspool of modern engagement-based monitziation schemes. On that note, Gabriel wrote a rebuttal to his own essay, which he called \"Worse is Better is Worse\".<sup>2</sup> No joke!</p> <ol> <li> <p>https://en.wikipedia.org/wiki/Worse_is_better \u21a9</p> </li> <li> <p>https://dreamsongs.com/WorseIsBetter.html \u21a9</p> </li> </ol>"},{"location":"blog/2023/12/28/google-systems-design-interview-with-an-ex-googler/","title":"Google Systems Design Interview With An Ex-Googler","text":"<p>https://www.youtube.com/watch?v=q0KGYwNbf-0</p> <p>I will be taking notes of this video by Clement Mihailescu by listening to the prompts in the video and generating my own design. After I complete my design, I will listen to Clement's design and compare/contrast the differences.</p>"},{"location":"blog/2023/12/28/google-systems-design-interview-with-an-ex-googler/#code-deployment-system","title":"Code Deployment System","text":"<p>Prompt: I want you to design a global and fast code deployment system.</p>"},{"location":"blog/2023/12/28/google-systems-design-interview-with-an-ex-googler/#background","title":"Background","text":"<p>Code deployment systems are part of the CI/CD stack (and in fact, it literally is the \"Code Deployment\" part of CI/CD). CI, or continuous integration, involves continually integrating changes to the code into some main branch. It ensures that there are no regressions introduces during these changes, and that any conflicting changes are handled in a well-defined process.</p>"},{"location":"blog/2023/12/28/google-systems-design-interview-with-an-ex-googler/#clarification","title":"Clarification","text":"<p>A clarification that Clemont asks for is what part of the code build process should we be architecting? The interviewer states that we should assume that the code has already been tested, and he wants us to build a system whereby the developers can go to a UI, click <code>Build</code> which will initiate a build process, and then <code>Deploy</code> which will deploy a specific tagged version to production servers.</p>"},{"location":"blog/2023/12/28/google-systems-design-interview-with-an-ex-googler/#considerations-and-properties","title":"Considerations and Properties","text":""},{"location":"blog/2023/12/28/google-systems-design-interview-with-an-ex-googler/#language-specific-concerns","title":"Language-Specific Concerns","text":"<p>The artifact that gets built will have different \"shapes\" depending on the language in question. For example, a built artifact in Python often constitues a <code>.whl</code> file which contains a platform-specific build of the application or library, which means it will contain the compiled0<code>.pyc</code> files<sup>1</sup>, but also any shared libraries or other binary artifacts. We can also distribute a source distribution, which contains the raw Python source code, as well as the source for any C/C++ extensions. These source distributions often mean that there are more files to distribute. </p> <p>In compiled languages like Go/C/C++, you might have one, or a small handful, or architecture-specific compiled artifacts. This means that relative to a Python source distribution, it will contain a smaller number of files of smaller total size.</p> <p>Because of these considerations, our system needs to be able to handle both a small number of small files efficiently, but also a large number of medium-sized files (I'm being quite hand-wavy with the meaning of \"size\" here, but bear with me).</p>"},{"location":"blog/2023/12/28/google-systems-design-interview-with-an-ex-googler/#do-we-care-about-docker","title":"Do we care about Docker?","text":"<p>If we're in an environment that uses Docker, then the deployment bit of this question is greatly simplified. If we leave it up to the producers of the artifacts (meaning the CI/CD pipelines) to generate a Docker container for us, then the deployment bit becomes a bit trivial. All we'd need to do is set up a few caching layers (probably with something like quay.io) and have our internal docker services pull from this caching layer.</p> <p>That's a bit too easy though, so I'm going to assume for the purposes of this interview that we care about deploying code onto bare-metal.</p>"},{"location":"blog/2023/12/28/google-systems-design-interview-with-an-ex-googler/#rollback-ability","title":"Rollback-ability","text":"<p>We also want to ensure that our system is capable of rolling back deployments very quickly. This means that both the new and old vesions of the code (and preferrably, all of them) need to be easily reachable. The rollback mechanism should be controlled by some piece of metadata that lives on our production boxes, or by whatever control system that directs what . This might simply be an environment file that lives on the box that sources the right version, or a kubernetes pod, or whatever.</p>"},{"location":"blog/2023/12/28/google-systems-design-interview-with-an-ex-googler/#style-of-rollout","title":"Style of rollout","text":"<p>There are a few different styles of rollout we could do:</p> <ol> <li>Canary testing, whereby we deploy the new code to a small number of production servers and see if anything fails.</li> <li>A/B testing, whereby a user will see either the A version or the B version of the deployment. We then compare the two to determine if the new version is misbehaving.</li> <li>Parallel run, whereby a request is run through both implementations, but only one response is returned to the user. We can compare both responses to measure things like content, latency, and correctness.</li> </ol> <p>For this interview, I'll adopt approach 1 as it's the simplest and arguably the most commonly used.</p>"},{"location":"blog/2023/12/28/google-systems-design-interview-with-an-ex-googler/#scale","title":"Scale","text":"<p>I peeked ahead a little (after I already designed my solution) and looked at some of the follow-up questions Clement asked:</p> <ol> <li>How many regions? ~5-10</li> <li>How many machines? Hundreds of thousands.</li> <li>Availability? 2-3 nines (99% or 99.9%)</li> <li>30 minutes during entire deploy</li> <li>1000s of deploys per day.</li> <li>10GB per binary.</li> </ol>"},{"location":"blog/2023/12/28/google-systems-design-interview-with-an-ex-googler/#solution","title":"Solution","text":"<p>Our solution needs to do two things:</p> <ol> <li>Build the code into some artifact repository</li> <li>Deploy the code to production</li> </ol>"},{"location":"blog/2023/12/28/google-systems-design-interview-with-an-ex-googler/#source-build","title":"Source Build","text":"<pre><code>flowchart\n    Human\n    UI((UI))\n    Backend[[Backend]]\n    SourceCode[(Source Code)]\n    ArtifactRepository[(Artifact Repository)]\n\n\n    Human --&gt;|Build v0.1.0| UI\n    UI --&gt;|Build v0.1.0| Backend\n    SourceCode --&gt;|Download v0.1.0| Backend --&gt;|Execute build.sh in source code.\\nUpload artifact.| ArtifactRepository</code></pre> <p>This diagram shows a general flow of how our system will actually build a piece of code. A human will make a request to some UI that specifies that we want to initiate a build for <code>v0.1.0</code> of our repo. This request goes to some \"backend\" (which we have not yet defined) that downloads the code, executes some build command (like some <code>build.sh</code> script that is defined in the repo itself), and uploads the artifact to some sort of repository. This repository can be an S3 bucket, a POSIX filesystem, or whatever we want. Let's just say it'll be an S3 blob store.</p> <p>Our build backend needs to satisfy various properties:</p> <ol> <li>It needs to be horizontally scalable.</li> <li>It needs to prioritize builds in the queue based off of queue insertion time.</li> <li>It needs to keep track of the state of the build.</li> </ol>"},{"location":"blog/2023/12/28/google-systems-design-interview-with-an-ex-googler/#event-driven","title":"Event-Driven","text":"<p>The first question becomes, what should our queue be? Well, we could have an event driven system where new builds go into some pubsub topic, which also acts as a queue. The message will be grabbed by a backend \"worker\", which itself could send a message saying the message was received, it will perform the build, upload it to a backend, then send another message indicating success.</p> <pre><code>flowchart\n    pubsub[(pubsub)]\n    pubsub &lt;--&gt; worker1(Worker 1)\n    pubsub &lt;--&gt; worker2(Worker 2)\n    pubsub &lt;--&gt; worker3(Worker 3)</code></pre> <p>Pros</p> <ol> <li>The horizontal scaling of the worker pool is essentially limitless. We'd only be limited by the size of the pubsub cluster, which is fairly easy to do if we partition the topic correctly.</li> <li>We also get prioritization for free as our workers would grab messages off the topic in roughly the order in which they were sent</li> <li>The latency between from when the job is enqueued and when it gets claimed by a worker is 100% determined by the size of the queue and the number of workers. This is a good property to have as it means we can keep our workers maximally occupied and there is nothing inbetween the queue and the workers to slow the process down.</li> </ol> <p>Cons</p> <ol> <li>It becomes more difficult to determine what the current state of a build is. We'd need some sort of sidecar process that listens to the messages in the pubsub topic and keeps track of each build's state in a SQL database.</li> <li>It's difficult to create some sort of fairshare algorithm between the code repositories. In my experience with build systems, sometimes a large repository can hold a build system hostage (due to a large number of builds being sent, or each build taking an enormous amount of time) and cause builds from other repos to become backlogged.</li> </ol>"},{"location":"blog/2023/12/28/google-systems-design-interview-with-an-ex-googler/#event-driven-with-state-tracking","title":"Event-Driven (With State Tracking)","text":"<p>Another solution is that our queue could live in a Postgres (or MySQL, or MariaDB, whatever). In this case, the database access should be protected behind some sort of arbiter process so that we don't have an unbounded number of workers trying to all perform complex tasks on the database, which would slow performance. It would also be unwise for the workers to directly access the database because then we're creating an implicit coupling between the database schema, and the schema the workers expect. If we isolate the schema access to a single process, it becomes more manageable to do backwards-incompatible migrations, should we ever need to.</p> <p>The worker itself will look like this:</p> <pre><code>flowchart\n    SQL[(SQL queue)]\n    dispatcher[[dispatcher]]\n    pubsub[(Pubsub)]\n    ArtifactRepository[(Artifact Repository)]\n\n    SQL &lt;--&gt;|determine next jobs| dispatcher\n    dispatcher --&gt;|message: job ready| pubsub\n    pubsub &lt;--&gt; worker1\n    pubsub &lt;--&gt; worker2\n    pubsub &lt;--&gt; worker3\n\n    worker1 --&gt; ArtifactRepository\n    worker2 --&gt; ArtifactRepository\n    worker3 --&gt; ArtifactRepository</code></pre> <p>The general flow looks like this:</p> <ol> <li>The dispatcher listens for <code>message: job submitted</code> from the pubsub queue, indicating we want a new build to be inserted into queue.</li> <li>Dispatcher adds job to SQL queue.</li> <li>Dispatcher selects next batch of jobs from SQL queue. This is determined based off of some priority tuple like <code>(creation_time,repo)</code> so that each repo can have individually-applied priorities. It sets the <code>STATUS</code> of these jobs to <code>PUBSUB SUBMITTED</code>.</li> <li>Dispatcher sends a message to pubsub indicating that the jobs should be processed.</li> <li>An arbitrary worker will receive the message and send a <code>message: job received</code> to the pubsub.</li> <li>The dispatcher will receive the <code>message: job received</code> and set the status of the job in SQL to <code>RUNNING</code>, and optionally note which worker took the job (useful for debugging purposes).</li> <li>The worker will perform the build. At the end, it uploads it to the artifact repository and sends a <code>message: job completed</code> to the pubsub.</li> <li>The dispatcher receives this message and sets the status of the job to <code>COMPLETED</code>.</li> </ol> <p>There is an important scenario to consider: what happens if the worker claims a job, but then dies halfway through the build process? We need some way to detect this failure scenario. One solution is that we could have the dispatcher periodically poll some HTTP endpoint exposed by the workers and check that it's healthy. Since we know which worker claimed a job, we can poll it to ensure it's healthy.</p> <p>If the poll times out, the dispatcher can set the status of the job to <code>FAILED</code> and prevent any other work in the job until it's explicitly retried. In the case that the poll comes back and the worker says it's working on some other job, AND the dispatcher has not seen either a <code>message: job failed</code> or <code>message: job completed</code> message, we can assume that the job got lost somewhere and set its status to <code>FAILED</code>.</p> <p>Pros</p> <ol> <li>The dispatcher provides us with explicit state tracking. It knows the status of every job and keeps track of its state.</li> <li>We still get the benefit of using event-driven message passing, which gives us a lot of room to scale.</li> <li>The dispatcher performs health checks on the workers so we can detect hard failure conditions.</li> <li>We have a lot of room to implement more sophisticated priority mechanisms, which can allow us to protect against a repo taking too much of the computing resources. For example, we could pop a maximum of 100 repos at a time, which allows us to do a sort of round-robin scheduling.</li> </ol> <p>Cons</p> <ol> <li>We're introducing a latency penalty, because there will be a period of time between when a job is enqueued in SQL, and when it gets submitted to the pubsub topic. </li> <li>The dispatcher might eventually become a bottleneck. Since it's a single process (which may be multithreaded), we're limited to the computing resources of a single node. We're also limited to how many operations we can do on the SQL database. However, since we're doing 1000s of deploys per day, it doesn't seem unreasonable for us to perform some single-digit multiple of SQL operations to keep track of that state. Plus, we can batch updates to the database, and make use of indicies, to improve performance.</li> <li>We have a single point of failure. If the dispatcher fails, then the entire process grinds to a halt. This could be mitigated if we introduce multiple dispatchers that have simultaneous access to the database. However this introduces another source of complexity in the system as we need to ensure there are no race conditions. Additionally, multiple dispatchers will introduce even more load on the SQL server, which possibly means more contention, so we'd need to be mindful of the load and SQL contention of our database.</li> </ol> <p>Overall, using an even-driven system with state tracking feels like the right compromise between latency and distributed state tracking. It allows us explicit prioritization logic between repos and can scale quite well to 1000s of operations per day.</p>"},{"location":"blog/2023/12/28/google-systems-design-interview-with-an-ex-googler/#artifact-deployment","title":"Artifact Deployment","text":"<p>The deployment mechanism needs to communicate to all production nodes what version of the application should be runnning. We also need a way to distribute the artifact. We will first tackle the distribution question:</p>"},{"location":"blog/2023/12/28/google-systems-design-interview-with-an-ex-googler/#artifact-distribution","title":"Artifact Distribution","text":"<p>We will design a system whereby the artifact is served through an HTTP endpoint. This can be as simple as a web server, like Apache, that has been told to expose some POSIX storage endpoint in our Artifact Repository. Because our endpoint should be globally available, we'll also want to have some tiered system of caching layers. It might look like this:</p> <pre><code>flowchart TD\n    subgraph SourceColo\n        ApacheServer[[Apache Server]]\n        ArtifactRepository[(Artifact Repository)]\n        ReverseProxyPool[[Caching Proxy]]\n\n        ArtifactRepository --&gt;|read foobar-0.1.0.tar.gz| ApacheServer\n        ApacheServer &lt;--&gt;|GET /apps/foobar/0.1.0/| ReverseProxyPool\n    end\n\n\n    subgraph RemoteColo\n        subgraph DeploymentController\n            Controller[[Deployment Controller]]\n            ProdAppv0[[Prod App v0.0.0]]\n            ProdAppv1[[Prod App v0.1.0]]\n            Controller --&gt; ProdAppv0\n            Controller --&gt; ProdAppv1\n        end\n\n        RemoteProxy[[Caching Proxy]]\n\n        Controller &lt;--&gt;|GET /apps/foobar/0.1.0/| RemoteProxy\n    end\n    RemoteProxy --&gt;|GET /apps/foobar/0.1.0/| ReverseProxyPool</code></pre> <p>This architecture heavily utilizes a system of caching proxies, whereby each colocation has a pool of caching proxies that are configured to pull from some higher layer. This architecture has a few nice properties:</p> <ol> <li>The artifact is cached at multiple layers, both in the original source colo, and at the remote colo. This means that a request only needs to be made once to the upstream source, then the rest of the colo can pull from the local copy of the data.</li> <li>The caching proxies can be reconfigured to point to different locations, which will allow the organization to grow and change as the business needs grow/change.</li> </ol>"},{"location":"blog/2023/12/28/google-systems-design-interview-with-an-ex-googler/#artifact-deployment_1","title":"Artifact Deployment","text":"<p>Now that we've solved the deployment bit, we want to tell the remote applications that a new version has been built and that we want them to upgrade. The way we'll do this is to expose another HTTP endpoint, called <code>/deployments/foobar/</code> (<code>foobar</code> being the name of our application) that will contain information on what things should be upgraded and how. When we update the deployment, we'll send a message to a pubsub topic that the deployment controllers will listen to that will tell it which version of the deployment to <code>GET</code>.</p> <pre><code>flowchart \n    subgraph SourceColo\n        Human\n        UI((UI))\n        Backend[[Backend]]\n        ApacheServer[[Apache Server]]\n        PubSub[(PubSub Topic)]\n        ReverseProxyPool[[Caching Proxy]]\n\n        Human --&gt;|New Deployment Step| UI\n        UI --&gt;|POST /deployments/foobar\\nHTTP OK id=3| Backend\n        Backend &lt;--&gt;|GET /deployments/foobar/3| ApacheServer\n        Backend --&gt;|DEPLOYMENT /deployments/foobar/3| PubSub\n\n        ApacheServer &lt;--&gt;|GET /deployments/foobar/3| ReverseProxyPool\n    end\n\n\n    subgraph RemoteColo\n        subgraph DeploymentController\n            Controller[[Deployment Controller]]\n            ProdAppv0[[Prod App v0.0.0]]\n            ProdAppv1[[Prod App v0.1.0]]\n\n            PubSub --&gt;|DEPLOYMENT /deployments/foobar/3| Controller\n            Controller --&gt; ProdAppv0\n            Controller --&gt; ProdAppv1\n        end\n\n        RemoteProxy[[Caching Proxy]]\n\n        Controller &lt;--&gt;|GET /deployments/foobar/3| RemoteProxy\n    end\n    RemoteProxy --&gt;|GET /deployments/foobar/3| ReverseProxyPool\n</code></pre> <p>This diagram might look a bit involved, but it's actually quite simple. Here's the order of operations:</p> <ol> <li>A human goes to the UI and creates a new deployment. The human specifies the application to update, the version to set the update to, and various bits of information like what percentage of machines it should be rolled out to, where it should be rolled out to, etc.</li> <li>The UI makes a <code>POST</code> request to <code>/deployments/foobar</code> with this information to the deployment backend, and gets back the ID <code>3</code>.</li> <li>The backend stores this deployment information in some sort of database.</li> <li>The backend submits a message to a pubsub topic indicating that a new deployment has been created, and where it can be found.</li> <li>The Deployment Controller receives this message and sends a <code>GET</code> request to <code>/deployments/foobar/3</code>.</li> <li>This request falls back to a number of caching proxies all the way back to the deployment backend in the source colo.</li> <li>The JSON containing the deployment information is returned to the deployment controller.</li> <li>Based on the information provided, the deployment controller determines if its a target for this deployment, and if so, deploys the new version in the way specified in the JSON.</li> </ol>"},{"location":"blog/2023/12/28/google-systems-design-interview-with-an-ex-googler/#clements-solution","title":"Clement's Solution","text":""},{"location":"blog/2023/12/28/google-systems-design-interview-with-an-ex-googler/#build","title":"Build","text":"<p>I won't go into detail his exact solution as you can find it in the video. In Clement's solution, his build system consists of a pool of workers that are all working off a build queue, where the queue is populated by certain triggers in our company's various repositories (such as a merge to master). He decides that the workers should dequeue by directly accessing the database, which is an anti-pattern as we are spreading the coupling between the SQL schema and the workers over some arbitrary number of works, instead of limiting it to just one or two dispatchers (as in my solution).</p>"},{"location":"blog/2023/12/28/google-systems-design-interview-with-an-ex-googler/#distribution","title":"Distribution","text":"<p>His distribution mechanism relies on replicating the GCS buckets to multiple regions, which means that we now need to keep track of the status of the replication. Only after the replication succeeds can we deploy the artifact. This is a fairly complex solution because it relies on some blackbox replication system (which has unknown reliability/latency guarantees), and some external replication monitoring service. In my solution, my distribution mechanism relies on a single source of truth bucket and a series of tiered caching proxies. When we notify the servers that a new artifact is ready, they can immediately send a request to download these new artifacts. The caching proxies will initially miss back to the source of truth. After the initial miss, subsequent requests in a remote colo for the artifact will hit the proxy's internal cache, which will dramatically speed up the download.</p> <p>He also notes that it's unreasonable for all machines in a region to download directly from the GCS bucket, as that would introduce a ton of network overhead (and cost on the cloud side). He proposes that the machines in a region can participate in a P2P protocol so that they can share the artifacts amongst themselves. While this might work, it's an even greater source of complexity that can be operationally more difficult to manage. My solution of using a pool of caching proxies in each region is fairly simple and can be scaled horizontally quite well.</p> <p>It's possible, however, that we could use both the reverse caching proxy solution and a P2P solution down the line. For example, a server could first check if the file is available in the P2P network. If it's not, ask the caching proxy for the file (or, the caching proxy could be part of the P2P network! Interesting thought). This would give us some amount of redundancy because if the caching proxy pool goes down for whatever reason, the file is likely still available in the P2P network, which would allow the deployment to continue. This would help with the reliability aspect of our system.</p> <ol> <li> <p>Yes, Python files do get compiled! But they get compiled into byte-code (not an executable), which is ultimately what gets read by the Python interpreter.\u00a0\u21a9</p> </li> </ol>"},{"location":"blog/2024/01/09/robotstxt-as-an-insight-into-web-administration-wars/","title":"robots.txt as an Insight Into Web Administration Wars","text":"<p>robots.txt, or the Robot Exclusion Protocol, is one of the oldest protocols on the web. It's a file, usually stored at the top level of a domain, that provides a list of rules which politely informs web crawlers what they are and are not allowed to do. This simple file is a great insight into the kinds of struggles that web administrators have in maintaining their websites.</p>"},{"location":"blog/2024/01/09/robotstxt-as-an-insight-into-web-administration-wars/#wikipedia","title":"Wikipedia","text":"<p>Wikipedia, being one of the most visited sites ever, has to deal with a large number of web crawlers who are drooling at the mouth to index their content. Let's take a look at their <code>robots.txt</code>:</p> Text Only<pre><code>\u279c  ~ curl https://en.wikipedia.org/robots.txt |&amp; less\n# Please note: There are a lot of pages on this site, and there are\n# some misbehaved spiders out there that go _way_ too fast. If you're\n# irresponsible, your access to the site may be blocked.\n#\n\n# Observed spamming large amounts of https://en.wikipedia.org/?curid=NNNNNN\n# and ignoring 429 ratelimit responses, claims to respect robots:\n# http://mj12bot.com/\nUser-agent: MJ12bot\nDisallow: /\n\n# advertising-related bots:\nUser-agent: Mediapartners-Google*\nDisallow: /\n\n# Wikipedia work bots:\nUser-agent: IsraBot\nDisallow:\n\nUser-agent: Orthogaffe\nDisallow:\n\n# Crawlers that are kind enough to obey, but which we'd rather not have\n# unless they're feeding search engines.\nUser-agent: UbiCrawler\nDisallow: /\n\nUser-agent: DOC\nDisallow: /\n\nUser-agent: Zao\nDisallow: /\n\n[...]\n</code></pre> <p>The Wikipedia devs have quite clearly explained their reasoning for disallowing certain crawlers. In this case, we see the above crawlers are known to be well-behaved, but they aren't feeding any search engines. Others are known to be trouble:</p> Text Only<pre><code># Some bots are known to be trouble, particularly those designed to copy\n# entire sites. Please obey robots.txt.\nUser-agent: sitecheck.internetseer.com\nDisallow: /\n\nUser-agent: Zealbot\nDisallow: /\n\nUser-agent: MSIECrawler\nDisallow: /\n\nUser-agent: SiteSnagger\nDisallow: /\n\nUser-agent: WebStripper\nDisallow: /\n</code></pre> <p>They also seem to have had issues with <code>wget</code> users trying to recursively get all pages:</p> Text Only<pre><code>#\n# Sorry, wget in its recursive mode is a frequent problem.\n# Please read the man page and use it properly; there is a\n# --wait option you can use to set the delay between hits,\n# for instance.\n#\nUser-agent: wget\nDisallow: /\n</code></pre> <p>Let's experiment with this by downloading the Wiki entry for <code>robots.txt</code></p> Text Only<pre><code>\u279c  test_recursive_wget wget --recursive --level=0 https://en.wikipedia.org/wiki/Robots.txt\n--2024-01-09 14:05:55--  https://en.wikipedia.org/wiki/Robots.txt\nResolving en.wikipedia.org (en.wikipedia.org)... 208.80.154.224\nConnecting to en.wikipedia.org (en.wikipedia.org)|208.80.154.224|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 136396 (133K) [text/html]\nSaving to: \u2018en.wikipedia.org/wiki/Robots.txt\u2019\n\nen.wikipedia.org/wiki/Robots.txt                             100%[============================================================================================================================================&gt;] 133.20K  --.-KB/s    in 0.08s   \n\n2024-01-09 14:05:55 (1.70 MB/s) - \u2018en.wikipedia.org/wiki/Robots.txt\u2019 saved [136396/136396]\n\nLoading robots.txt; please ignore errors.\n--2024-01-09 14:05:55--  https://en.wikipedia.org/robots.txt\nReusing existing connection to en.wikipedia.org:443.\nHTTP request sent, awaiting response... 200 OK\nLength: 27524 (27K) [text/plain]\nSaving to: \u2018en.wikipedia.org/robots.txt\u2019\n\nen.wikipedia.org/robots.txt                                  100%[============================================================================================================================================&gt;]  26.88K  --.-KB/s    in 0s      \n\n2024-01-09 14:05:55 (328 MB/s) - \u2018en.wikipedia.org/robots.txt\u2019 saved [27524/27524]\n\nFINISHED --2024-01-09 14:05:55--\nTotal wall clock time: 0.3s\nDownloaded: 2 files, 160K in 0.08s (2.04 MB/s)\n</code></pre> <p>We observe wget downloading the <code>en.wikipedia.org/robots.txt</code> file to consult if it's allowed to do the recursive download. The man page states:</p> Text Only<pre><code>Wget can follow links in HTML, XHTML, and CSS pages, to create local versions of remote web sites, fully recreating the directory structure of the original site.  This is sometimes referred to as \"recursive downloading.\"  While doing that, Wget respects the Robot Exclusion Standard (/robots.txt).\n</code></pre> <p>So it makes sense that it refuses to download any further links.</p> <p>Wikipedia has also struggled with clients that don't respect <code>robots.txt</code> anyway, but they add a rule as an act of futile desparation:</p> Text Only<pre><code>#\n# Doesn't follow robots.txt anyway, but...\n#\nUser-agent: k2spider\nDisallow: /\n</code></pre> <p>Webreaper is simply permabanned:</p> Text Only<pre><code># A capture bot, downloads gazillions of pages with no public benefit\n# http://www.webreaper.net/\nUser-agent: WebReaper\nDisallow: /\n</code></pre> <p>Wikipedia states that if your bot is well-behaved, they'll allow your bot through, but only for static pages.</p> Text Only<pre><code>#\n# Friendly, low-speed bots are welcome viewing article pages, but not\n# dynamically-generated pages please.\n#\n# Inktomi's \"Slurp\" can read a minimum delay between hits; if your\n# bot supports such a thing using the 'Crawl-delay' or another\n# instruction, please let us know.\n#\n# There is a special exception for API mobileview to allow dynamic\n# mobile web &amp; app views to load section content.\n# These views aren't HTTP-cached but use parser cache aggressively\n# and don't expose special: pages etc.\n#\n# Another exception is for REST API documentation, located at\n# /api/rest_v1/?doc.\n#\nUser-agent: *\nAllow: /w/api.php?action=mobileview&amp;\nAllow: /w/load.php?\nAllow: /api/rest_v1/?doc\nDisallow: /w/\nDisallow: /api/\nDisallow: /trap/\nDisallow: /wiki/Special:\nDisallow: /wiki/Spezial:\nDisallow: /wiki/Spesial:\nDisallow: /wiki/Special%3A\nDisallow: /wiki/Spezial%3A\nDisallow: /wiki/Spesial%3A\n</code></pre>"},{"location":"blog/2024/01/09/robotstxt-as-an-insight-into-web-administration-wars/#amazon","title":"Amazon","text":"Text Only<pre><code>\u279c  test_recursive_wget curl -L https://amazon.com/robots.txt |&amp; less\n</code></pre> <p>Amazon's case is interesting as their rule matching is less specific and instead targets all <code>User-agent</code> values. The notable value here is the fact that they block GPT from training on their data:</p> Text Only<pre><code>User-agent: GPTBot\nDisallow: /\n</code></pre>"},{"location":"blog/2024/01/09/robotstxt-as-an-insight-into-web-administration-wars/#microsoft","title":"Microsoft","text":"<p>Microsoft simply blocks all access everywhere with no exceptions:</p> Text Only<pre><code>User-agent: *\nDisallow: /\n</code></pre>"},{"location":"blog/2024/01/09/robotstxt-as-an-insight-into-web-administration-wars/#news-organizations","title":"News Organizations","text":"<p>TheVerge.com is another website that blocks GPTBot:</p> Text Only<pre><code>User-agent: GPTBot\nDisallow: /\n</code></pre> <p>OepnAI has a very brief documentation page that shows how to block GPTBot. It's no surprise that many websites would opt out of their crawlers, but the interesting thing to note is that as ChatGPT is evolved to ingest more and more data, we can expect many more websites to request GPTBot to not index their content. This has a real possibility of making the input quality to future ChatGPT models far worse than the initial interations, especially if information juggernauts like Wikipedia decide to restrict access to their contents. It's also likely that with more news organizations requesting their content to not be crawled that ChatGPT will become less able to disseminate news-oriented information.</p> <p>Some other organizations that block GPTBot are:</p> <ul> <li>CNN</li> <li>MSNBC</li> <li>PBS</li> <li>NPR</li> <li>The Washington Post</li> <li>Bloomberg</li> </ul> <p>In fact, most of the major news organizations I investigated have all blocked GPTBot, which indicates there is a strong desire amongst privatized organizations to not have their content employed in the training of for-profit LLMs.</p>"},{"location":"blog/2024/01/09/robotstxt-as-an-insight-into-web-administration-wars/#reddit","title":"Reddit","text":"<p>Reddit has quite an amusing rule:</p> Text Only<pre><code>User-Agent: bender\nDisallow: /my_shiny_metal_ass\n</code></pre> <p>Which, of course, is a reference to Bender from Futurama's most beloved refrain, often said in anger.</p>"},{"location":"blog/2024/01/09/robotstxt-as-an-insight-into-web-administration-wars/#effectiveness","title":"Effectiveness","text":"<p>It's worth noting, and probably obvious to every reader here, that <code>robots.txt</code> is not an effective means of blocking web crawlers from scouring your website. It's akin to posting a \"No Trespassing\" sign on your property. The sign itself won't stop bad actors from trespassing, so many of these LLMs which don't adhere to ethical principles can simply decide to not respect the webadmin's wishes. It's entirely plausible that black market LLMs might one day become more effective than well-known industry-standard LLMs as they don't need to adhere to annoying things like ethics and laws.</p>"},{"location":"blog/2024/04/23/python-table-driven-tests/","title":"Python Table-Driven Tests","text":"<p>Table-driven tests (TDT) are a popular form of unit testing that allow you to rapidly iterate over multiple different inputs to your system in a way that is easily readable and understandable. This post will show my approach to TDT in Python and why all the current implementations of it are insufficient.</p> <p>A table-driven test is a kind of unit test whereby a single body of test code is re-used over multiple test instances. The parameters to the test code are provided in a list whereby each entry contains all the relevant information to test the behavior you desire. Take for example this function:</p> path.py<pre><code>import pathlib\n\n\ndef make_relative(path: pathlib.Path, root: pathlib.Path | None = None) -&gt; pathlib.Path:\n    \"\"\"\n    Make path relative to root. If root is None, set it to the home directory. If path\n    can't be made relative to root, add path to root after making path relative to\n    \"/\".\n    \"\"\"\n    if root is None:\n        root = pathlib.Path.home()\n    try:\n        return path.relative_to(root)\n    except ValueError:\n        pass\n\n    if path.is_absolute():\n        path = path.relative_to(\"/\")\n\n    return root.joinpath(path)\n</code></pre> <p>This function does the following:</p> <ol> <li> <p>Attempt to return <code>path</code> relative to <code>root</code>.</p> <p>a. If <code>root</code> is <code>None</code>, set <code>root</code> equal to the home directory.</p> </li> <li> <p>If this cannot be done:</p> <p>a. Remove the leading <code>/</code> component from path</p> <p>b. Return <code>root + path</code>.</p> </li> </ol>"},{"location":"blog/2024/04/23/python-table-driven-tests/#standard-approach","title":"Standard Approach","text":"<p>A common approach to testing this function is to create one test per case:</p> test_path_individual_functions.py<pre><code>from .path import make_relative\nfrom pathlib import Path\n\n\ndef test_make_relative_root_is_none():\n    path = Path.home().joinpath(\"foobar\")\n    root = None\n    assert make_relative(path=path, root=root) == Path(\"foobar\")\n\n\ndef test_make_relative_root_is_home():\n    path = Path.home().joinpath(\"foobar\")\n    root = Path.home()\n    assert make_relative(path=path, root=root) == Path(\"foobar\")\n\n\ndef test_make_relative_root_is_unrelated_directory():\n    path = Path.home().joinpath(\"foobar\")\n    root = Path(\"/leetcode/sucks\")\n    expected = root.joinpath(path.relative_to(\"/\"))\n    assert make_relative(path=path, root=root) == expected\n\n\ndef test_make_relative_root_is_unrelated_directory_path_is_not_absolute():\n    path = Path.home().joinpath(\"foobar\").relative_to(\"/\")\n    root = Path(\"/leetcode/sucks\")\n    expected = root.joinpath(path)\n    assert make_relative(path=path, root=root) == expected\n</code></pre> <p>The issue with this approach is that it's a lot of boilerplate: we have to define one function per test, call our function under test, and assert the value is what we expect. It's also moderately ugly to look at because while each function is testing the same thing, they are dispersed across the file and this makes it harder to find a specific case that you may care about. This approach may be tractable for functions that have a small number of behaviorial permutations, but it quickly gets out of hand when the number of cases for your function increases beyond a small handful.</p>"},{"location":"blog/2024/04/23/python-table-driven-tests/#pytest-parametrized-decorator","title":"<code>pytest</code> Parametrized Decorator","text":"<p>Another common approach that is supported in the Python community is to use <code>pytest.mark.parametrize</code>. This would look something like this:</p> test_path_parametrize.py<pre><code>from .path import make_relative\nfrom pathlib import Path\nimport pytest\n\n\n@pytest.mark.parametrize(\n    (\"path\", \"root\", \"expected\"),\n    [\n        (Path.home().joinpath(\"foobar\"), None, Path(\"foobar\")),\n        (Path.home().joinpath(\"foobar\"), Path.home(), Path(\"foobar\")),\n        (\n            Path.home().joinpath(\"foobar\"),\n            Path(\"/leetcode/sucks\"),\n            Path(\"/leetcode/sucks\").joinpath(\n                Path.home().joinpath(\"foobar\").relative_to(\"/\")\n            ),\n        ),\n        (\n            Path.home().joinpath(\"foobar\").relative_to(\"/\"),\n            Path(\"/leetcode/sucks\"),\n            Path(\"/leetcode/sucks\").joinpath(\n                Path.home().joinpath(\"foobar\").relative_to(\"/\")\n            ),\n        ),\n    ],\n)\ndef test_make_relative(path: Path, root: Path | None, expected: Path) -&gt; None:\n    assert make_relative(path=path, root=root) == expected\n</code></pre> <p>Already, we can see multiple benefits from this style of test:</p> <ol> <li>All testcases reuse the same body of code</li> <li>Each testcase is just a blob of data: we can clearly see what the inputs are to the test case without having to parse through internal details of the test logic. This gives us a higher-level picture of each case.</li> <li>It has the potential to be less lines of code. Our toy example here is not a great representation of this point, but you can imagine when you have more enumerations that over time the parametrized version will be more compact and readable.</li> </ol> <p>There are, however, some major disadvantages to <code>pytest.mark.parametrize</code>:</p> <ol> <li>Type checking tools like <code>mypy</code> cannot properly detect when an argument to your testcase is of an incorrect type. For example, if we change the first testcase to be <code>(Path.home().joinpath(\"foobar\"), 1, Path(\"foobar\"))</code> (the <code>root</code> argument is an <code>int</code> instead of <code>Path | None</code>), mypy will not complain:     Bash<pre><code>$ mypy .\nSuccess: no issues found in 4 source files\n</code></pre></li> <li>The arguments to each testcase are positional instead of keyword arguments, which makes it difficult to remember which part of the tuple corresponds to what argument of the test case.</li> <li>Because the arguments are positional, you have to specify something even in the case you want to rely on default values.</li> <li> <p>Adding a new parameter to the test requires modifying all previously existing cases (yuk!). Changing the argument definition:     Python<pre><code>(\"path\", \"root\", \"expected\", \"foo\"),\n</code></pre></p> <p>causes pytest to barf:</p> Text Only<pre><code>_____ ERROR collecting test_path_parametrize.py _____\ntest_path_parametrize.py::test_make_relative: in \"parametrize\" the number of names (4):\n('path', 'root', 'expected', 'foo')\nmust be equal to the number of values (3):\n(PosixPath('/Users/landon/foobar'), 1, PosixPath('foobar'))\n</code></pre> <p>This point, in my opinion, is the most egregious.</p> </li> </ol>"},{"location":"blog/2024/04/23/python-table-driven-tests/#go-like-table-driven-test","title":"Go-like Table Driven Test","text":"<p>So what's the answer? Well, being a self-described Go language dogmatist, I like to draw on the Go way of doing things. Instead of relying on third-party packages like <code>pytest</code> and awkward implementations of TDT, we can utilize stdlib Python APIs:</p> test_path_tdt.py<pre><code>from .path import make_relative\nfrom pathlib import Path\nfrom unittest import TestCase\nimport dataclasses\n\n\nclass TestMakeRelative(TestCase):  # (1)!\n    def test_make_relative(self) -&gt; None:\n        @dataclasses.dataclass  # (2)!\n        class Params:\n            path: Path\n            expected: Path\n            root: Path | None = None\n\n        for test in [\n            Params(\n                path=Path.home().joinpath(\"foobar\"), root=None, expected=Path(\"foobar\")\n            ),\n            Params(\n                path=Path.home().joinpath(\"foobar\"),\n                root=Path.home(),\n                expected=Path(\"foobar\"),\n            ),\n            Params(\n                path=Path.home().joinpath(\"foobar\"),\n                root=Path(\"/leetcode/sucks\"),\n                expected=Path(\"/leetcode/sucks\").joinpath(\n                    Path.home().joinpath(\"foobar\").relative_to(\"/\")\n                ),\n            ),\n            Params(\n                path=Path.home().joinpath(\"foobar\").relative_to(\"/\"),\n                root=Path(\"/leetcode/sucks\"),\n                expected=Path(\"/leetcode/sucks\").joinpath(\n                    Path.home().joinpath(\"foobar\").relative_to(\"/\")\n                ),\n            ),\n            Params(\n                path=Path.home().joinpath(\"foobar\"), root=None, expected=Path(\"haha\")\n            ),\n        ]:\n            with self.subTest(**dataclasses.asdict(test)):  # (3)!\n                assert test.expected == make_relative(path=test.path, root=test.root)\n</code></pre> <ol> <li>We use a <code>unittest.TestCase</code>-style test so we can make use of a feature called <code>subTest</code> as you'll see below.</li> <li>Create a dataclass that describes, and types, the parameters used in the subtest.</li> <li>There's a few things going on here. The first thing happening is that we're calling <code>self.subTest</code> which is a context manager that creates, you guessed it, a sub-test. This is reported as a separate, inferior test that runs under the purview of the outer <code>test_make_relative</code> unit test.</li> </ol> <p>There are a few key points to note here that make this style of test so powerful:</p> <ol> <li> <p>The use of the <code>subTest</code> context manager makes reporting the exact arguments used in the test easy to parse. <code>subTest</code> takes an arbitrary number of keyword arguments that are used when reporting test failures. We format our <code>Params</code> dataclass into a python dictionary, then use the unpacking operator <code>**</code> to send the attributes of <code>Param</code> as individual keyword arguments. Take for example this failed test:</p> Text Only<pre><code>====================================== short test summary info ======================================\n(expected=PosixPath('haha'), path=PosixPath('/Users/landon/foobar'), root=None) SUBFAIL test_path_tdt.py::TestMakeRelative::test_make_relative - AssertionError: assert PosixPath('haha') == PosixPath('foobar')\n==================================== 1 failed, 9 passed in 0.05s ====================================\n</code></pre> <p>Tip</p> <p>To get this output, you need to install the <code>pytest-subtest</code> plugin.</p> </li> <li> <p>Adding a new parameter doesn't require modifying every existing test case. We can simply provide a default in the dataclass and override this for any new test. Take for example:</p> Python<pre><code>class TestMakeRelative(TestCase): \n    def test_make_relative(self) -&gt; None:\n        @dataclasses.dataclass\n        class Params:\n            path: Path\n            expected: Path\n            root: Path | None = None\n            new_param: int = 0\n\n        for test in [\n            # ...\n            Params(\n                path=Path.home().joinpath(\"foobar\"), \n                root=None, \n                expected=Path(\"foobar\"), \n                new_param=1,\n            ),\n        ]:\n            with self.subTest(**dataclasses.asdict(test)):\n                assert test.expected == make_relative(path=test.path, root=test.root)\n</code></pre> </li> <li> <p><code>mypy</code> is able to properly detect incorrect value assignments. For example if we do:</p> Python<pre><code>Params(\n    path=Path.home().joinpath(\"foobar\"), root=\"wtflol\", expected=Path(\"foobar\")\n),\n</code></pre> <p>mypy tells us:</p> Bash<pre><code>$ mypy .\ntest_path_tdt.py:17: error: Argument \"root\" to \"Params\" has incompatible type \"str\"; expected \"Path | None\"  [arg-type]\n</code></pre> </li> </ol>"},{"location":"blog/2024/04/23/python-table-driven-tests/#conclusion","title":"Conclusion","text":"<p>Opinion alert: I've used <code>pytest</code> quite extensively in my professional career but I often disagree with the approach it takes to things. <code>pytest.mark.parametrize</code> is yet another one of those things that feels useful on the surface, but it has many shortcomings that cause endless pain and suffering. They tried to solve the question of table-driven tests but did it in a nasty way that for me engenders sadness and dismay. </p> <p>The methodology I propose here is not new in the broader software engineering community, but it is one I've rarely seen utilized in the Python community. TDT in the Go community is everywhere and whether you like it or not god dammit, you'll learn to like it because it's just the right mixture of elegance and power.</p> <p>Let me know your thoughts below, and feel free to share your opinions on pytest. Also, don't you dare mention pytest fixtures in the comments. I will rage </p>"},{"location":"blog/2024/05/03/python-you-have-problems/","title":"Python, you have problems.","text":"<p>It's been said that the key to any healthy relationship is the ability to see eye-to-eye on life's important issues. The approach one has to politics, to love, to religion, to animals and children (sometimes they're the same thing), and to those less fortunate in society are key metrics to consider when evaluating the strength of your compatability to a significant other. Well, I've been pondering my relationship with Python, and I'm realizing that we just aren't compatible anymore. It's not me, it's you.</p>"},{"location":"blog/2024/05/03/python-you-have-problems/#my-history-with-python","title":"My History with Python","text":"<p>I was first introduced to Python in my college years during my work with the National Center for Supercomputing Applications. I learned Python because it had proven itself to be an invaluable tool when running large batch workloads on supercomputing clusters. You think you're going to create a huge meta-scheduler job pipeline in something like C? Ha! Good luck. One of my first proud accomplishments as a young, green grasshopper was writing a TORQUE metascheduling package in Python that took a DAG of TORQUE job definitions, performed a topological sort, and translated that graph into a tree of job dependencies in TORQUE. I also took pride (oh how cute of me) in some other small Python projects I authored like a lightweight authentication wrapper over the Globus API. You know, just cute stuff that a college student might think is cool.</p> <p>This dovetailed into a mostly Python position at Jump Trading where most of my work involved either system administration in Python, using tools written in Python, or creating large, hundred-thousand-job metascheduling ETL pipelines that managed many petabytes of live HFT market data. There, my Python chops were refined in a professional and high-stakes environment, allowing me to grow my practices and strategies to be an effective and high-functioning Python developer.</p> <p>Thus for the longest time, my opinions of Python were glowing as I felt it had liberated me from the constraints of antiquated languages like C/C++. I balked at the idea of ever having to deal with stupid things like typing, or memory management, or pointers ever again. I was free to do what I wanted when I wanted, just like a true red-blooded American. Who doesn't like a good dose of freedom?</p>"},{"location":"blog/2024/05/03/python-you-have-problems/#my-disclaimer","title":"My Disclaimer","text":"<p>My disclaimer to all readers past, present, and future is that I do not hate Python. Python has done more good for the world than almost any other language. It is a true force to be reckoned with and I enjoy using it in the environments where it shines. I'll continue to use Python for the foreseeable future not just because it's in high demand, but because it's still fun to me and it has its proper place.</p> <p>With that said, let's begin.</p>"},{"location":"blog/2024/05/03/python-you-have-problems/#the-problems-begin","title":"The Problems Begin","text":"<p>It's been said that all good things must come to an end. This is true for a lot of things, but not all. For example, wearing pajamas while working remotely doesn't have to end. Coffee doesn't have to end. The invariable spread of democracy doesn't have to end (Helldivers anyone? I'm just joking).</p> <p>When it comes to my unconditional infatuation with Python, well, that indeed must come to an end.</p>"},{"location":"blog/2024/05/03/python-you-have-problems/#typeerror","title":"TypeError","text":"<p>Type errors, we all love them, we learn to get along with them. It's a fact of life, it happens. Let's say I wrote a stupid program like this:</p> stupid_type_error.py<pre><code>import random\n\ndef type_error() -&gt; None:\n    print(\"running function with type error\")\n    for i in 123:\n        print(i)\n\ndef no_type_error() -&gt; None:\n    print(\"running function with no type error\")\n    for i in (0, 1, 2):\n        print(i)\n\ndef main():\n    random.seed()\n    random_number = random.randrange(0, 100)\n    if random_number % 7 == 0:\n        type_error()\n    else:\n        no_type_error()\n\nmain()\n</code></pre> <p>The keen observer may notice right away that this program is going to fail 14% of the time at this exact line. Why 14%? Well we're generating a random number between 0 and 99, and we're asking to run the <code>type_error</code> function whenever that number is divisible by 7. There are 14 such numbers between 0 and 99, which gives us, you guessed it, 14%.</p> <p>Let's see what happens in the happy path:</p> Text Only<pre><code>running function with no type error\n0\n1\n2\n</code></pre> <p>And now what happens when we have the unhappy path:</p> Text Only<pre><code>  File \"/Users/landon/git/LandonTClipp/LandonTClipp.github.io/code/python_you_have_problems/stupid_type_error.py\", line 22, in &lt;module&gt;\n    main()\n  File \"/Users/landon/git/LandonTClipp/LandonTClipp.github.io/code/python_you_have_problems/stupid_type_error.py\", line 16, in main\n    type_error()\n  File \"/Users/landon/git/LandonTClipp/LandonTClipp.github.io/code/python_you_have_problems/stupid_type_error.py\", line 5, in type_error\n    for i in 123:\nTypeError: 'int' object is not iterable\n</code></pre> <p>Well of course, the <code>int</code> type doesn't have the <code>__next__</code> magic method so it can't be used as an interable, so therefore it's not iterable. This kind of error would have been easy to catch in such a trivial program such as this, but imagine if you have a project with hundreds of thousands of lines of code and you just happened to have missed the one line that contains a type error. Well now you won't find this bug until you discover it in production, which is never fun.</p> <p>There are, however, some strategies to overcome this. Some of them that I've seen:</p>"},{"location":"blog/2024/05/03/python-you-have-problems/#require-100-testing-coverage","title":"Require 100% testing coverage","text":"<p>This has been done at various places, but sometimes it's not feasible because if you don't set this requirement from the inception of your project, getting up to 100% coverage can be like Sisyphus rolling a boulder up an infinitely long slope. This is also easily defeated if you do your tests wrong, like if you were to mock an external dependency incorrectly and you return a type that the real dependency doesn't actually return (hello <code>unittest.mock.Mock</code>). Of course unittest mocks often take a <code>spec</code> argument that allows the shape of your mock to be exactly like the real dependency, but this only works if your dependency correctly annotates your types, otherwise how can the Mock know what it's supposed to return? </p>"},{"location":"blog/2024/05/03/python-you-have-problems/#static-type-checkers","title":"Static Type Checkers","text":"<p>Use a static typechecker like <code>mypy</code>. This is one of my favorite options, but this too is easily and often defeated. Take for example the <code>pytest.mark.parametrize</code> decorator. <code>mypy</code> is not able to correctly inspect any typing issues in the decorator because it's just a weird f-ing way to implement table-driven tests. <code>mypy</code> also doesn't work in cases where, again, your external dependency you rely heavily on doesn't provide type annotations (which in my experience is quite a lot of them). When <code>mypy</code> works, it works fantastically, but the tool is limited by how well-written your code and your dependencies are. And please, let's not argue on whether or not you should use type annotations: you should.</p> <p>Let's see what mypy says when we run it on our <code>stupid_type_error.py</code> program:</p> Text Only<pre><code> $ mypy code/python_you_have_problems/stupid_type_error.py \nSuccess: no issues found in 1 source file\n</code></pre> <p>Oh, that's embarrasing actually. In my initial iteration of this post, I forgot to annotate my functions (tee-hee) so mypy by default will not perform type checking on it. Let me add <code>-&gt; None</code> as the return type, which tells mypy to check for types:</p> Text Only<pre><code> $ mypy code/python_you_have_problems/stupid_type_error.py\ncode/python_you_have_problems/stupid_type_error.py:5: error: \"int\" has no attribute \"__iter__\"; maybe \"__int__\"? (not iterable)  [attr-defined]\n</code></pre> <p>Yeeeaaah, much better. But again, that again brings up the great point that if your external dependency forgot to add annotations, or if you're someone who has some weird thing against typing, then mypy will totally ignore it, or get it totally wrong.</p> <p>Can we also just sit down for a second and appreciate the fact that tons of super talented people had to spend a large amount of their time actually creating <code>mypy</code> and supporting it through various iterations of the Python language because the language itself just doesn't have static typing? We're shoehorning a really important concept into a language that doesn't provide it. Props to everyone who develops and maintains <code>mypy</code>, it's a wonderful tool, but I posit that it shouldn't have to exist.</p>"},{"location":"blog/2024/05/03/python-you-have-problems/#parallelism","title":"Parallelism","text":"<p>Woo boy here we go. Let's get one thing straight: Python does not have a decent parallelism story. Because CPython requires the use of a Global Interpreter Lock, that means that only one thread of execution within the interpreter process can run at a time. This is the source of all frustration and pain with parallelism in CPython, and it's probably never going to go away.</p> <p>For some context, in most programming languages you are able to create multiple threads of execution that are able to run simultaneously. The benefit of this, as opposed to multiprocessing, is that the threads can share memory, which means that your data structures don't have to be serialized over a communication channel (often a unix pipe or socket). This lends itself to a much more efficient style of parallelism because the shared datastructure has much better locality properties, and it's much easier to keep it internally consistent. This improves latency, it reduces complexity, and by extension makes your system more performant and robust.</p> <p>So what does Python do about this? Well, it provides us with something even worse than multiprocessing... and it's truly horrifying: <code>asyncio</code></p> <p></p>"},{"location":"blog/2024/05/03/python-you-have-problems/#asyncio-the-concurrency","title":"<code>asyncio</code>: The Concurrency","text":"<p>For those unfamiliar with <code>asyncio</code>, it is a Python package, created by the maintainers of CPython, that gives us a way to run multiple \"coroutines\" of execution within a single Python interpreter. Both asyncio and Python's <code>threading</code> package can be used to implement concurrency. <code>asyncio</code> makes use of the idea of a \"coroutine\" which can be thought of as a thread-like concept, but instead of being maintained by the OS, a coroutine is managed by Python itself. Coroutines are not OS threads, but they behave in a similar way. (1) <code>asyncio</code> also makes use of special keywords like <code>await</code>, <code>async def</code>, <code>async for</code>, <code>async with</code> that are used to inform the interpreter on what functions are coroutines, and how to interact with those coroutines. This makes it easier to reason about where your concurrent code can block because you've explicitly defined it with the <code>await</code> and <code>async</code> keywords.</p> <ol> <li>Just without, you know, the ability to run multiple threads simultaneously. Coroutines can yield their execution when they're blocking on IO, but they cannot run CPU-bound tasks simultaneously. This is the distinction between concurrency and parallelism: concurrency means the threads of execution can yield when blocked by IO, thus allowing another coroutine to run. Parallelism means multiple threads can run simultaneously. Python does have parallelism through multi-processing, but multi-processing will never be as good as multi-threading because of its huge overhead, reduced memory sharing, and a need to maintain external channels of communication.</li> </ol> <p><code>threading</code> differs from <code>asyncio</code> in that it uses actual OS threads to multiplex operations. With <code>asyncio</code>, you can reasonably submit hundreds of thousands, even millions of coroutines, because a coroutine is an internal concept in memory, not an actual OS thread. It would not be responsible, however, to treat threads the same way. Go ahead and try, see what happens.</p> <p>If you're confused about the difference between <code>threading</code> and <code>asyncio</code> and <code>multiprocessing</code> and <code>concurrent.futures.ThreadPoolExecutor</code>, you're not alone. The glut of options at our disposal might lead a layman believe that parallelism in Python is flexible and well-conceived, but it's actually quite the opposite. Not only is it confusing to understand the differences between these options, it's confusing to understand for which situations each solution is appropriate for. Let's not get too into the weeds, however. Let's instead focus on why <code>asyncio</code> is just plain awful.</p>"},{"location":"blog/2024/05/03/python-you-have-problems/#asyncio-what-color-is-your-function","title":"<code>asyncio</code>: What Color Is Your Function","text":"<p>Bob Nystrom wrote a fantastic blog post called What Color is Your Function. In this post, he describes a theoretical programming language that I'll name <code>Shithon</code> (pronounced shy-thon, or shit-thon, or shit-hon, whatever you prefer). This language has one particular feature: every function has a color. These are the properties of these functions:</p> <ol> <li> <p>Every function has a color.</p> <pre><code>blue_function doSomethingAzure() {\n    // This is a blue function...\n}\n\nred_function doSomethingCarnelian() {\n    // This is a red function...\n}\n</code></pre> </li> <li> <p>The way you call a function depends on its color.</p> <pre><code>doSomethingAzure()blue;\ndoSomethingCarnelian()red;\n</code></pre> </li> <li> <p>You can only call a red function from within another red function.</p> <p>This can be done:</p> <pre><code>red_function doSomethingCarnelian() {\n    doSomethingAzure()blue;\n}\n</code></pre> <p>But this cannot:</p> Text Only<pre><code>blue_function doSomethingAzure() {\n    doSomethingCarnelian()red;\n}\n</code></pre> </li> <li> <p>Red functions are more painful to call.</p> </li> <li> <p>Some core library functions are red.</p> </li> </ol> <p>I highly recommend you read the entirety of the post as it describes in detail the exact problem that <code>asyncio</code> presents. As you may already tell, shithon is basically just Python asyncio. Take for example this async function:</p> Python<pre><code>async def get_item_from_server(value: int) -&gt; int:\n    return await client.get(value)\n</code></pre> <p>The only way this function can be called is either in the context of another async function:</p> Python<pre><code>async def get_item() -&gt; int:\n    return await get_item_from_server(value=5)\n</code></pre> <p>Or if called from a synchronous function, through <code>asyncio</code>:</p> Python<pre><code>def get_item() -&gt; int:\n    return asyncio.run(get_item())\n</code></pre> <p>But wait! The asyncio docs say about <code>asyncio.run</code>: </p> <p>Quote</p> <p>This function cannot be called when another asyncio event loop is running in the same thread.</p> <p>So what do we do if there is already an event loop running somewhere? Well, you can't. If your synchronous function calls async code, or otherwise adds an async function to the event loop, then by definition your function has to be <code>async</code> as well. Synchronous functions cannot call async functions if an event loop is already running in the same thread.<sup>1</sup></p> <p>I'm not going to go into detail the intricacies of asyncio, for one because it's just incredibly frustrating to me personally, and for two because it's already been well-documented in various places on the interwebz. Here are some good resources:</p> <ul> <li>https://bbc.github.io/cloudfit-public-docs/asyncio/asyncio-part-5.html</li> <li>https://docs.python.org/3/library/asyncio.html</li> <li>https://realpython.com/async-io-python/</li> </ul> <p>Going back to \"What Color Is Your Function\", the point can be distilled down to the fact that it's really hard to call <code>async</code> functions from non-<code>async</code> because considerations have to be made about whether or not an event loop is already running, and if it is, how you should structure your code in a way that allows you to call <code>async</code> functions. It bifurcates your world into two realms, so much so that crossing this boundary is super difficult. Managing this boundary cannot simply be done through semaphores because the entire model itself is just bad. It all goes back to the GIL, baby.</p>"},{"location":"blog/2024/05/03/python-you-have-problems/#packaging-and-environment-management","title":"Packaging and Environment Management","text":"<p>Now let's get on to another horrendous aspect of the Python ecosystem: environment management. Let's say you make a fun little CLI tool that walks down a filesystem path and sums the total size of each layer, kind of like <code>ncdu</code>. Let's call it pydu, and let's say it looks exactly like ncdu:</p> Text Only<pre><code>$ pydu .\n--- /Users/landon/git/LandonTClipp/LandonTClipp.github.io --------------\n  232.9 MiB [################################] /ve                                                                                                                                                                                \n  149.8 MiB [####################            ] /.git\n   94.6 MiB [#############                   ] /code\n   79.5 MiB [##########                      ] /docs\n   14.8 MiB [##                              ] /.cache\n    7.0 MiB [                                ] /.mypy_cache\n</code></pre> <p>pydu's project structure looks like any normal Python project: it's got a set of modules for shared code, an entrypoint for the CLI, and it pulls in some dependencies through a pyproject.toml file. The structure may look something like this:</p> Text Only<pre><code>pyproject.toml\nsrc/main.py\nsrc/__init__.py\nsrc/module1.py\nsrc/module2.py\ntests/test_module1.py\ntests/test_module2.py\n</code></pre> <p>You have a few choices at your disposal for how to package this. One of the default options is to publish it on the Python Package Index, or Pypi. There are many guides on how to do this, like this one here. The gist of it:</p> <ol> <li>You define all your package metadata in pyproject.toml, including your package name, the dependencies, any CLI entrypoints, and all other ancillary associated metadata.</li> <li>You run <code>python -m build</code> to build a series of distributions, like:<ol> <li>A source code distribution (which is just a tar.gz archive).</li> <li>A Wheel, which is a binary file format that contains your code and all dependencies (both pure Python and pre-compiled). Wheels that contain platform-specific dependencies are called Platform Wheels (not surprisingly) because they will contain pre-compiled code that is meant for specific platforms.</li> </ol> </li> </ol> <p>This is all fine and dandy, and in fact my criticisms of Pypi and the associated build tools are quite minimal. My main point of ire lies downstream from when you try to download and install these packages. Here's where it gets really fun.</p> <p>So, let's say you want to now download <code>pydu</code>. What would you do?</p>"},{"location":"blog/2024/05/03/python-you-have-problems/#pip","title":"pip","text":"<p>This option is the \"default\" choice for installing packages. This looks something like:</p> Text Only<pre><code>python3 -m pip install pydu\n</code></pre> <p><code>pip</code> will look on PyPI (or any other package index you've configured) for <code>pydu</code> and install the appropriate distribution for your platform, whether that's a source install (if you've specifically requested it) or one of the wheels. By default, this will attempt to install in a site-wide location, depending your site-specific configuration. However, this almost immediately becomes a problem if you've got multiple CLI tools that have common dependencies: what if one CLI tool you've installed depends on <code>foo&gt;=1.5.0</code> but another CLI tool depends on <code>foo==1.3.0</code>.  It's possible to get into situations where the installation of one CLI tool will break the dependencies of a pre-existing CLI tool, because in essence what is happening is that the dependencies are dynamically linked at runtime. This issue only becomes more pronounced the more projects you have installed on your host. </p> <p>What's the a solution to this madness? Well, virtual environments of course.</p>"},{"location":"blog/2024/05/03/python-you-have-problems/#virtualenv","title":"virtualenv","text":"<p>To get around the issue of the dependencies in your site-wide install location, you can make a virtual environment! Woo-hoo, this is fantastic. Hooray.</p> Text Only<pre><code>$ python3 -m venv ve\n$ source ve/bin/activate\n$ which python3\n/Users/landon/git/LandonTClipp/LandonTClipp.github.io/code/python_you_have_problems/ve/bin/python3\n</code></pre> <p>This creates a directory where all dependencies will live:</p> Text Only<pre><code>$ ls ve \nbin        include    lib        pyvenv.cfg\n</code></pre> <p>So whenever you <code>pip install</code> something, it gets placed here instead of your site-wide location.</p> Text Only<pre><code>$ python3 -m pip install httpx\n$ ls ve/lib/python3.12/site-packages \nanyio                      certifi                    h11                        httpcore                   httpx                      idna                       pip                        sniffio\nanyio-4.3.0.dist-info      certifi-2024.2.2.dist-info h11-0.14.0.dist-info       httpcore-1.0.5.dist-info   httpx-0.27.0.dist-info     idna-3.7.dist-info         pip-24.0.dist-info         sniffio-1.3.1.dist-info\n</code></pre> <p>This solves the issue of having too many conflicting dependency requirements throughout all the projects you're managing, so problem solved, right? Wrong.</p> <p>What if we wanted to install two different projects that have conflicting dependencies? Well god dammit, that's just not possible without some serious finagling. The thing you have to realize is that a lot of these dependency hell headahces come from the fact that when Python installs a package, only one version of that package can be installed at a time.<sup>2</sup> This makes sense in the context of a singular project that has internally consistent dependencies, but it doesn't make sense when you're running multiple CLI applications that might conflict with each other. (1) Each CLI application does not and should not have to coordinate with other, completely separate CLI apps. Yet Python is opinionated in that sense that packages shall be treated as dynamically-loaded and shared dependencies.</p> <p>It turns out that pip is just a total mess to use because it doesn't handle multi-user or multi-project environments well at all. This is where other projects have come to the rescue, like poetry or flit.</p> <ol> <li>And to be clear, it doesn't even have to be CLI applications. This applies for any conflicting dependencies whether that arises from a development environment, or an install of a CLI app, or anything where two packages depend on conflicting versions of a dependency.</li> </ol>"},{"location":"blog/2024/05/03/python-you-have-problems/#pyenv","title":"pyenv","text":"<p>So far we've only talked about installing packages, but we have not yet talked about whether or not the version of Python running on your local system is actually capable of running the packages. Or in other words, what if your package used a language feature from Python 3.12, but you only have Python 3.8 available on your system? Well lucky for you, there's pyenv.</p> <p>There's a whole set of steps you must go through to get pyenv installed, but for what it's worth, it does in fact work, but it poses yet another hurdle that users of Python must overcome if they find their system-provided Python is too old. I won't go into the details of how it works, but I just wanted to mention it.</p>"},{"location":"blog/2024/05/03/python-you-have-problems/#nuitka","title":"nuitka","text":"<p>Let's say for example, you went to your boss one day and you said \"hey boss man, I'm sick and tired of placing all these dependencies down on disk. Why can't we just package our CLI tool as a single executable and in that executable include both the Python interpreter itself and all of our project's dependencies?\"</p> <p>Your boss would say: \"great idea Bob, why don't you give nuitka a shot?\"</p> <p>So you run along to CI/CD-land and you introduce nuitka into your workflow so that instead of producing a Wheel or a source archive, you produce a single, beautiful executable that can be shipped as a single blob to your customers. This solution works great for a while, but after a certain point you realize that nuitka gets slower and slower, and after some digging you realize that the more dependencies your project gains, the slower nuitka gets. In fact it gets so slow that you find your builds taking tens of minutes, sometimes more! In addition to your testing infrastructure, you find that from end-to-end it's now taking almost an hour or more just to produce a single artifact!</p> <p>Of course, you could spend your days optimizing Nuitka and figuring out where you can prune unnecessary dependencies, but this takes time and energy, and I doubt your company enjoys paying you gobs of money to figure this out when you could have been focusing on building products.</p> <p>There are a few other projects like nuitka as well like:</p> <ol> <li>pyinstaller</li> <li>pyoxidizer</li> <li>py2exe</li> <li>py2app</li> </ol> <p>Pick your poison.</p>"},{"location":"blog/2024/05/03/python-you-have-problems/#metrics-in-uwsgi","title":"Metrics in uWSGI","text":"<p>uWSGI is the framework Python uses to serve web traffic. In brief, it forks the main process into multiple worker processes in what is called a \"pre-fork web server model\". This frustrates metrics collecting: consider that you have a Prometheus counter in your web server that you want incremented each time a request to an upstream HTTP service fails. Well, because each worker is a separate process, the counter only has meaning within that process. i.e. the counter state is not shared across processes, which may not be what developers intend.</p> <p>There are some ways to get around this:</p> <ol> <li>Append a <code>pid</code> attribute to the counter and sum the values <code>by pid</code> on query-time. This is fine but kind of burdensome, and possibly introduces other problems with high-cardinality labels.</li> <li>Use Redis as a storage backend for the counter. This is probably the \"correct\" solution, but it seems most of the major metrics packages don't provide support for this. It also places an additional dependency on Redis that might otherwise not be necessary if you use proper multi-threaded languages like Go.</li> </ol> <p>References:</p> <ul> <li>https://github.com/open-telemetry/opentelemetry-python/issues/3885</li> </ul>"},{"location":"blog/2024/05/03/python-you-have-problems/#misc","title":"Misc","text":"<p>There are so many other dependency management tools that try, with varying levels of success, to do what <code>pip</code> does but better. Some examples:</p> <ol> <li>conda</li> <li>poetry</li> <li>pipenv</li> <li>flit</li> <li>???</li> </ol> <p>I could spend a whole week writing out the differences of these all, but the point I want you to take away from this is that dependency management in Python is truly in a state of, absolute, unmitigated, unqualified cluster-fuckery. And yes I'm sure some of you will angrily write to me \"but you can do this one cool thing and it's super easy and it works so well for me.\" I'm sure that works for you, but it doesn't take away from my point that there are so many ways of doing package management in Python and there is no universally accept way, or limited set of ways, of doing it. This all comes down to the fact that the Python language itself doesn't provide a sufficient, cross-platform, sustainable method for us. All of the methods out there exist because the whole model of how Python does package distribution, with its opinionated take that packages should essentially be shared libraries (<code>.so</code> for you Linux dorks) whose version is not part of the on-disk namespace, is wholly insufficient. </p> <p>The result of this is that the open source community, as well-intentioned and exuberant as it is, is left to fill in the void with dozens of completely separate implementations that don't inter-operate and don't agree on basically anything. Don't you think this is an enormous waste of human ingenuity? Don't you think this is such a silly thing to fight over? All of these bright, intelligent developers working on these projects could have been working on sending rockets into space or curing cancer, but instead we're focusing on how to handle dependencies in Python.</p> <p>There is no other word for this but absolute madness.</p>"},{"location":"blog/2024/05/03/python-you-have-problems/#my-guiding-principles-for-evaluating-languages","title":"My Guiding Principles for Evaluating Languages","text":"<p>Python, while you have taught me a great deal about software engineering and have made me a better professional, a lot of the things you taught me were taught because you just suck at a lot of things. You're flaky, you fail when I don't expect it, and you always surprise me with another one of your \"quirks\" that end up biting me in the butt. There are a lot of people that love you and that have tried to fix you, but maybe you're just not meant to be fixed.</p> <p>Here are some of the things I learned about what a good language should have:</p>"},{"location":"blog/2024/05/03/python-you-have-problems/#strict-typing-is-good","title":"Strict Typing is Good","text":"<p>Python, as we know, is duck-typed. If it looks like a duck, swims like a duck, and quacks like a duck, then it's probably a duck. Variables in Python are pointers to underlying objects. The variable itself does not have a type. It may be pointed to anything and it can be reassigned to any type at any point in time.</p> <p>What this means, essentially, is that there is no protection against accidentally using the wrong type. There are tools that might warn you (even though those tools can be fooled), but ultimately there still exist categories of type errors that you won't be made aware of until you actually run the damn thing.</p> <p>Strictly typed languages on the other hand will tell you immediately if something is wrong. They won't even let you run the program, because syntactically, it's not valid. This is an incredibly important feature: you want your program to fail fast and fail hard so that you have no opportunities to send your code to production until the issue has been fixed.</p> <p>Python, in this category of errors, has the approach of \"fail hard only if you happen to test this one particular line of code, and if you're not diligent enough to test every single god damn line of code, well sucks to suck.\"</p>"},{"location":"blog/2024/05/03/python-you-have-problems/#garbage-collection-is-good-where-real-time-latency-isnt-necessary","title":"Garbage Collection is Good (where real-time latency isn't necessary)","text":"<p>I love collecting garbage. Er, I mean, I love it when my languages collect my garbage. Python does this so well, in fact, that allocation or deallocation is rarely something that ever comes to mind. Go also does this really well, as do most GC languages. This is, in fact, is one of the things I love about Python so, good job Python.</p>"},{"location":"blog/2024/05/03/python-you-have-problems/#function-coloring-is-bad","title":"Function Coloring is Bad","text":"<p>All functions should be callable from all other functions. Bifurcating your language sucks and it causes endless frustration. This is why asyncio is an awful idea, at least in the way it was implemented. We have to remind ourselves that asyncio is necessary because language itself was not designed with proper thread-based parallelism in mind. If CPython didn't have a GIL and its internal data structures were thread-safe, true thread-based parallelism in Python would be a breeze and function coloring would thus become unnecessary. </p>"},{"location":"blog/2024/05/03/python-you-have-problems/#gils-are-bad","title":"GILs are bad","text":"<p>There are a number of notable examples of interpreted languages, or implementations of Python, that lack a GIL:</p> <ul> <li>IronPython</li> <li>IronRuby</li> <li>Jython</li> <li>Tcl</li> <li>Pike</li> </ul> <p>While this blog is taking a massive dump on Python, it should be noted that a lot of popular packages like pandas/numpy allow you to run data processing workloads outside of the interpreter in native C/C++, which means they are super fast and don't have the GIL limitation. That's great and works quite well for the domains that justify the cost pulling in those heavyweight dependencies.</p>"},{"location":"blog/2024/05/03/python-you-have-problems/#composition-is-usually-better-than-inheritance","title":"Composition is (Usually) Better Than Inheritance","text":"<p>This is a philospohy recently championed (but certainly not invented) by Go. Go does not have true inheritence in its OOP model. It favors composition, which essentially means that instead of being able to inherit attributes and functionality, you create an attribute on your object that stores the dependency. This reduces the complexities involved with things like multiple inheritance and helps discourage needless abstractions. It's also quite telling how an entire diagraming language, UML, was created specifically to aid us in understanding inheritance models. Composition, on the other hand, shouldn't require diagramming because the object relationships can almost always be described in simple English.</p> <p>This is a whole blog post unto itself so I will not enumerate all my thoughts, but here are some suggested readings:</p> <ul> <li>https://en.wikipedia.org/wiki/Composition_over_inheritance</li> <li>https://www.digitalocean.com/community/tutorials/composition-vs-inheritance</li> <li>https://en.wikipedia.org/wiki/Multiple_inheritance#The_diamond_problem</li> </ul>"},{"location":"blog/2024/05/03/python-you-have-problems/#parting-thoughts","title":"Parting Thoughts","text":"<p>Hopefully by this point, I've successfully enumerated my varying complaints with Python. And if you've made it this far, congratulations, you either totally agree with me, you learned something new, or you've become so miffed that you can't wait to leave a comment telling me how stupid my arguments are. In any scenario: well-done.</p> <p>Any time a huge amount of complexity is introduced into a system to solve some fundamental problem, that usually means that the foundational model on which that system depends is inherently flawed. In Python's case, package management, lack of strict typing, its GIL, its concurrency primitives, are all examples of things that are just foundationally bad. Their existence necessitates a huge amount of complexity in the Python ecosystem that should have, and could have, been solved by changing the underlying model, but for reasons that I totally understand, at this point in Python's life, doing such a thing would be totally infeasible. Just look at how painful the Python 2-&gt;3 conversion was, we're still dealing with the affects of it 16 years later!</p> <p>It is totally absurd to me how much talent is wasted in solving these kinds of problems. In my view, the only reason why these problems are worth solving at all is because Python's momentum is so great that asking people to use something else is often just not practical. If it had obtained less popularity, my reaction would be to totally abandon the language altogether because these issues are real dealbreakers from a reliability and time efficiency perspective. The proliferation of packaging tools, of the existence of asyncio and function coloring, the large number of different implementations of the language itself (I mean come on, seriously?), all point to the fact that what was given to us in the reference implementation of the language (obviously, CPython) is just simply inadequate. So much incredible talent, creating admittedly incredible solutions, has been spent working around what is fundamentally a big stinky pile of .</p> <p>It should also be noted that one of Python's main selling points is that it's so easy to use, so much so that it's often the first language developers learn. This is only a shallow marketing tactic because once you peel back the veil and see the man behind the curtain, you realize how unfriendly it becomes. This is in fact a common complaint amongst newcomers that the proliferation of tooling, and the lack of any real standard for just about anything, can make it difficult to become effective at the language in a professional environment.</p> <p>It's also my general stance that languages are simply a means to an end. My main priority is not being an expert in a specific language, but to be an expert in delivering business results. When I find myself fighting with and being confused at a tool, it either means I'm just too stupid to understand the tool (which very well may be true), or the tool itself is just inherently confusing. In either case, it's serving as an obstacle towards my end goal. I certainly don't think I'm too stupid to understand Python because I have indeed used it to create huge, production-grade, business-critical applications in a number of domains. It's just that I think the complexity I was required to wrangle is not justified by the benefits of what Python offers, especially in comparison to other modern languages today. </p> <p>I think it's great that some people really really love Python, so much so that they market themselves as Python experts Pythonistas. That expertise is clearly in high demand and I'm by no means attempting to diminish the importance of that, nor am I try to say that people are wrong for enjoying Python. My only stance, enumerated ad nausaem in this post, is that businesses need to think critically about these shortcomings before deciding they want to accept Python and all its baggage, especially when there are a number of better languages out there for backend-y kind of work (cough *Go* cough).</p> <ol> <li> <p>If I am somehow wrong about this, please let me know! I could not find a way to do it.\u00a0\u21a9</p> </li> <li> <p>Per environment\u00a0\u21a9</p> </li> </ol>"},{"location":"blog/2024/06/21/mountains-and-proposals/","title":"Mountains and Proposals","text":"<p>This trip to the mountains is a return to a place I belong, but also to a place that I've never been. It's the start of a lifetime of joy. As you probably know by now, I'm obsessed with Colorado. It's my favorite place in the world and it tugs on my heartstrings like almost nothing else. So, what a better way to solidify my life with the one person who does tug on my heartstrings like nothing else? It's a perfect plan and no one ever saw it coming! </p> <p>By this time, I've been dating my girlfriend Kelly for over 5 years. I met her at a dance class during my last semester of college and the rest, as they say, is history. We hit it off and have been in a serious, committed relationship since. 5 years might sound like a long time to be dating someone before tying the knot, but it felt right for us. I concocted an elaborate plan to propose to her, and I would like to say she didn't know that's what that trip was about, but she told me she doesn't like surprises. The other reason she needed to know what this trip was for was because she also wanted to propose to me! So I guess we both concocted plans. It's ingenious!</p>"},{"location":"blog/2024/06/21/mountains-and-proposals/#the-stake-out-mon-june-24","title":"The Stake Out <code>Mon, June 24</code>","text":"<p>Sadly for Kelly, a lifelong commitment to me doesn't come cheap. I make 'em work for it. My plan was to set up a long-distance scavenger hunt that involved three hidden secret boxes spread around Teller County, CO. Each box had a riddle in it that contained clues on where to find the next box. The very last box was gonna contain a super extra special secret, wink wink.</p> <p>I drove to Woodland Park by myself and the plan was for Kelly to join me via aeroplane later on. The week where she wasn't here was dedicated to setting up my proposal. I didn't know exactly where I wanted the hidden boxes to be, so I spent a week driving to various locations in the area.</p> <p>I went off-roading, I went to various parks in the area, I looked at a lot of maps and scoped out the area like a true adventurer should. I even discovered shelf-road for the first time!</p> <p>I hiked up a REALLY steep trail that terminates at this beautiful mountain stream. Oh yeah and I also came across this creepy, abandoned, certainly haunted secret tunnel.</p>"},{"location":"blog/2024/06/21/mountains-and-proposals/#horsethief-trail-sat-june-29","title":"Horsethief Trail <code>Sat, June 29</code>","text":"<p>Kelly flew into Colorado Springs on the 28th. We both knew that the main purpose of this goal was for us to get engaged, and even better, we were both going to propose to each other! We're kind of non-traditional like that but sometimes tradition is wrong (or maybe not wrong, but not as fun). Like, why wouldn't both parties want to put effort into officially asking their partner to live the rest of their lives with them?</p> <p>Neither of us knew when or where the proposal would happen, but I did have to tell Kelly that I have to propose to her this weekend due to the timing of everything so let's not waste any time! Let's get this shindig going!</p> <p>Unbeknownst to me, today was my day to be proposed to. Kelly desparately wanted to go on a hike (for what reason, I could not fathom). I decided to take Kelly on this really cool hiking trail I went on the day before that terminates in this small but pretty mountain waterfall. In fact, I actually have the exact trail data from AllTrails here:</p> <p>It's a pretty steep hike for a ways but it's not too difficult. I took her to the waterfall I found and once we got there, I sat down on a tree that fell across the stream to take a rest. Kelly shouted to me \"Landon, what's that in the water?\" I look behind me and there's this little bottle flowing down the stream and I think to myself \"what! Why is there litter?\" She told me to pick it up, so I did and upon closer inspection, it was a message in a bottle! I uncorked that bad boy and tried pulling the paper out of it. During its short journey down the stream, it appeared that some water got in the bottle and made the parchment paper stick very strongly to the side of the bottle. Seriously, it was like it was glued! I couldn't get it out, so Kelly helped me and we were able to wiggle the secret message out of the bottle.</p> <p>In it was a beautiful poem she wrote where she asked me to marry her, and OF COURSE I said yes! Put a ring on it!</p>"},{"location":"blog/2024/06/21/mountains-and-proposals/#mueller-state-park-sun-june-30","title":"Mueller State Park <code>Sun, June 30</code>","text":"<p>Alright, now it was my turn. This proposal consisted of FOUR clues, not three like I mentioned earlier. Although, the first clue wasn't really hard to find because I just gave it to her. In it were GPS coordinates for the approximate location of the next clue, which happened to be <code>38.8807905\u00b0 Lat, -105.1733855\u00b0 Lon</code>.</p> <p>This clue was hidden in one of Kelly's favorite spots from last year, the dragonfly pond at Mueller State Park. This clue led her to the next location at <code>38.97393\u00b0 Lat, -105.17028\u00b0 Lon</code>.</p> <p>Also, can we just appreciate how fashionable Kelly looks in these pictures? Something about the Columbia jacket and the sunglasses make her look like a real adventurer.</p>"},{"location":"blog/2024/06/21/mountains-and-proposals/#crazy-gulch-trail-sun-june-30","title":"Crazy Gulch Trail <code>Sun, June 30</code>","text":"<p>Next on the proposal adventure was this random trail called Grazy Culch. I don't know what's so crazy about it, it's not like it was a difficult hike. In fact I actually wanted it to be easy, because the last clue was going to be a real lung-buster.</p> <p>Kelly traded her Columbia jacket for a sun hat because she realized that all this hiking is a real workout.</p> <p>This clue led her to the last location, the infamous and dangerous trek up to GPS coordinates <code>39.05835\u00b0 Lat, -105.22304\u00b0 Lon</code>.</p>"},{"location":"blog/2024/06/21/mountains-and-proposals/#signal-butte-sun-june-30","title":"Signal Butte <code>Sun, June 30</code>","text":"<p>Signal Butte is this prominent hill that juts out in the valley that is abutted by the foothills to the east, Pike's Peak to the south, and Hackett Mountain to the west. It's a strange formation because rises up out of nowhere from the ground and is surrounded on all sides by valley. To give you an idea of what this looks like, here are some pictures from our 2023 trip where we hiked up the hill:</p> <p>Coming back to the year 2024, this is the trail I took to get to Signal Butte to drop off the secret package a few days before:</p> <p>To give even more context, the path to Signal Butte is a rugged OHV trail that requires a decent (although not top-of-the-line) 4x4 off-road capable vehicle. That's where my  SUPER COOL  Jeep Grand Cherokee Trailhawk comes in handy. Anyway, we get to the bottom of Signal Butte and start making the arduous trek up the hill/mountain/butte thing. And what a trek it was!  While I had already been in Colorado for a number of weeks before Kelly got here, I failed to make the calculation that Kelly will arrive only a couple days prior to this adventure and absolutely won't be acclimated to the altitude. And worse, I was asking her to hike up a very steep butte/hill thing so that I could propose to her and spend the rest of our lives together. Needless to say, she was struggling and I was worried I made a huge mistake in putting the last clue up on this hill. I was worried we were gonna have to turn around! Yikes! Nonetheless, Kelly was a trooper and she pushed through the thin air. At the top was awaiting her last clue, a ring!</p> <p>At the top of this butte/hill thing, I asked Kelly to be my wife. The 5 years we had been dating at this point solidified the truth that she is the woman for me. She is an amazing partner. She is a kind, thoughtful, loving, affectionate, passionate, caring, generous soul and has always strived to be the best partner that she can be in our relationship. All of the obstacles we encountered were overcome because of her willingness to work together (and my willingness too of course, but this section is about her dammit!). In fact, all successfull relationships require that. Relationships need both parties to set aside ego, to admit mistakes, to take measurable action towards correcting wrongs, and most importantly to give all the love one can muster to the other person, because they deserve it. That's what I feel has been the story of our relationship, a story of overcoming our personal demons and together nurturing a prosperous, loving, respectful, and FUN relationship.</p> <p>It's because of Kelly's fighting spirit that I have no question in my mind that she is a lifelong partner. She's cares deeply about doing the right thing and living authentically with those around her. It turns out that Kelly was so happy that she started dancing! And boy does she have some moves. </p>"},{"location":"blog/2024/06/21/mountains-and-proposals/#grand-lake-mon-july-1","title":"Grand Lake <code>Mon, July 1</code>","text":"<p>The reason why the proposal had to be on-schedule and on-time is because we had an awesome adventure planned to the north. We made our way over to Grand Lake, CO which is on the western side of the famous Rocky Mountain National Park. The drive itself there was of course amazing:</p> <p>The route took us through this area called Berthoud Pass. The pictures above are on Hwy 40 just before the pass, but sadly I couldn't take pictures of this area because it's an intense drive up an extremely steep, winding mountain road, and needless to say, I had to stay focused. Here's some street photos from Google:</p> <p>Of course these pictures don't really do it justice how steep and how high it felt to be on that road. Kelly's fear of heights was kicking in again </p> <p>We arrived at our campsite which was situated about 5 minutes away from the Grand Lake Entrance Station. The reason why this is important is because Rocky Mountain National Park can get very busy. So busy in fact that the park service implemented limited timed entry permits. Folks wishing to enter have to either buy timed entry tickets a month in advance of the reservation date, or the night before. They release only a limited number each time, so you have to be on your computer the very moment that the tickets are released.</p> <p>We didn't get them in advance so I had to log onto their website at 7PM and scramble to buy tickets. It was difficult because I would refresh the site and immediately select the ticket I wanted, but it kept saying \"sold out\". I would then try selecting tickets for the next time slot, and again, immediately sold out. I tried doing this for the 3 or 4 time slots they had available, and NONE of them could be reserved even though I was on the site at the exact moment they were released! Ugh! And it didn't help that their website seemed to be crashing from all the traffic.</p> <p>This was a pretty bad experience and I hated having to fight to get these dang tickets. I understand why the NPS does this and I don't fault them for it, they are just trying to stem the onslaught of tourists like us from flooding the park. Fortunately, timed tickets are only necessary after 9am, so you're allowed to enter the park before 9am. That's what we ended up doing, and surprisingly, there was no line and it was not an issue at all. No tickets needed after all!</p> <p>One REALLY interesting fact about this campground is that it sits right next to a tiny little creek. \"Aw, that's not that interesting .\" No no no, this is not just any tiny little creek. This is the Colorado River! This area is not far away from the Colorado River headwaters, so at this point it's only a tiny little stream, unassuming and nondescript.</p> <p>Kelly's highlights from this campsite were the duckies and chickens they had in this cage. Chickens are one of her favorite animals so she was enthralled with them.</p>"},{"location":"blog/2024/06/21/mountains-and-proposals/#rocky-mountain-national-park-tues-july-2","title":"Rocky Mountain National Park <code>Tues, July 2</code>","text":"<p>Our first day in the park was spent driving up the long road up Milner pass to the top of the mountain. We stopped at the visitor center for a meal and visited with the local marmot population. You can see in the picture where Kelly is pointing that there is a little furry marmot scoping out the landscape below.</p> <p>Coming down the other side of the mountain towards Estes Park on the east side is an incredible drive down a well-maintained paved road called Trail Ridge Road. We didn't travel all the way to Estes Park because the primary goal of the day was just to enjoy the mountain and be out in nature. When we got to the bottom, we had two choices; we could either take the same paved road back up (boring) or go up the narrow dirt road called Old Fall River Rd. You know me, I love an adventure (and I have an off-roading SUV) so I convinced Kelly to let us go up the dirt road!</p>"},{"location":"blog/2024/06/21/mountains-and-proposals/#estes-park-wed-july-3","title":"Estes Park <code>Wed, July 3</code>","text":"<p>The next day was our Estes Park day. The plan was for us to visit as many attractions as we could in the town. I wanted to surprise Kelly so I booked us tickets on this thing called a \"mountain coaster\" which is essentially this small coaster that's in, well... the mountains :sweat_smile. On the way to the mountain coaster, Kelly saw this horseback riding company and she desperately wanted to go for a ride. So that's exactly what we did!</p>"},{"location":"blog/2024/06/21/mountains-and-proposals/#horseback-riding","title":"Horseback Riding","text":"<p>Many don't know this about me but I was actually a pretty prolific horseback rider when I was a kid. I used to participate in horsemanship competitions, things like dressage, cross country jumping, and regular jumping. I haven't ridden a horse regularly in a long time but it actually felt very familiar to me, despite being in a western saddle.</p>"},{"location":"blog/2024/06/21/mountains-and-proposals/#the-stanley-hotel","title":"The Stanley Hotel","text":"<p>Next on the agenda was for us to visit the infamous Stanley Hotel. For those who don't know, the Stanley Hotel is what inspired Stephen King to write his horror novel The Shining. Another commonly mistaken fact about the hotel is that the movie The Shining was not actually filmed at this hotel. The film rather used exterior shots of the Timberline Lodge in Oregon and a soundstage for its interior shots.(1)</p> <ol> <li>This is actually a whole other story. Stephen was deeply frustrated that Stanley Kubrick's 1980 film adaptation decided not to use the Stanley Hotel itself. He was so upset not just with that decision, but with various aspects of the film itself, that he re-purchased the rights to the film so that he could make his own 1997 TV Miniseries adaptation of his book, which was much more faithful to his original vision of the story.</li> </ol> <ul> <li></li> <li></li> <li></li> <li></li> <li></li> <li></li> <li></li> <li></li> <li></li> <li></li> </ul>"},{"location":"blog/2024/06/21/mountains-and-proposals/#bear-lake","title":"Bear Lake","text":"<p>Bear Lake is a section of the park that, surprisingly, is even more difficult to get to than the park itself. Remember how I mentioned that Rocky Mountain National Park requires a timed entry permit? Well to enter the Bear Lake corridor, you need not only the Rocky Mountain timed entry permit, but you also need a separate Bear Lake timed entry permit. And guess what, those are even more difficult to obtain. And also guess what, we weren't able to obtain it! Thankfully as before, you could enter the corridor after a certain time period, so we planned (somewhat spontaneously) to visit the lake after the last timed entry permit.</p> <p>The lake itself was no doubt incredibly beautiful, however there was a lot of traffic to get into the parking lot next to the lake itself. We sat in line for quite a while, waiting for a spot in the parking lot. Even more frustratingly, the parking lot was barely even full. The only reason the long line existed is because people insisted on getting a spot as close to the lake as possible, so they blocked this huge line of cars, who couldn't go around them, simply because they... didn't want to walk? Like what are you even doing at a national park if you don't want to walk? Ugh.</p> <p>As far as mountain lakes go, I would say it was marginally worth the trouble. Well that's not true, it wasn't worth the trouble. It was definitely cool, but the crowd of people and the bumper-to-bumper traffic was really off-putting, so I probably won't be visiting again.</p>"},{"location":"blog/2024/06/21/mountains-and-proposals/#tarryall-friday-july-5","title":"Tarryall <code>Friday, July 5</code>","text":"<p>It was time to start making our way back towards the Colorado Springs area, so we stopped for a night in the Tarryall area. More specifically, the Pike-San Isabel National Forest. We drove for a long time down this forest road called Matukat Rd. After driving (very slowly) for about an hour along this road, we found an incredible designated camping spot that overlooked Hackett and Cedar Mountains.</p> <p>I get to claim something at this camping spot that almost no one can say: I saved Kelly from a bear! (or at least, I like to say so) While I was sitting down by the campfire, Kelly walked along the forest road looking at some birds through her binoculars. For whatever reason, I decided to turn around in my chair to see where she had gone, and I happened to notice a black bear running down the mountain exactly towards her! I yelled at her that there was a bear coming towards her. The bear heard me yelling and briefly paused to look at me as if to say \"wtf was that noise\". She ran back to the camper while I scrambled to get the bear spray. We watched the bear continue down the mountain and run across the road. It never really stopped running, which feels impressive to me, but it clearly had somewhere important to be and god dammit it wasn't going to be late. It never got closer to us and kind of just minded its own business. We watched it run off into the distance until we couldn't see it anymore.</p> <p>For those who don't know, black bears are the only type of bear commonly found in Colorado and don't typically pose a threat to humans. They're much more skiddish and docile than grizzly bears. Most of the locals consider them to be large racoons. In fact, even with baby cubs, black bear mothers aren't typically aggressive towards humans and prefer to flee instead of fight in almost all situations. Regardless, it's kind of jarring to see my newly minted fiancee nearly have a direct encounter with this huge animal. And to think that the only reason they didn't cross direct paths with each other is because I just happened to turn around at the exact moment I could see it. Crazy!</p>"},{"location":"blog/2024/06/21/mountains-and-proposals/#mt-sherman-monday-july-8","title":"Mt. Sherman <code>Monday, July 8</code>","text":"<p>I decided I wanted to climb my first 14'er. There are a large number of them in the South Park area where I was staying (specifically in Fairplay, CO). Among those is Mt. Sherman which is a moderate class 1, and one of the easiest class 1s at that. </p> <p>This area is a grave for an old mining operation called the Sherman Mines that produced silver in the 1970s. Many of the old artifacts still remain, things like rail cars, bucket lifts, power stations, even electric cars! The mine itself has been sealed.</p> <p>After about an hour and a half of climbing, I finally made it to the top!</p>"},{"location":"blog/2024/06/21/mountains-and-proposals/#links","title":"Links","text":"<ul> <li> Google Photos Album</li> </ul>"},{"location":"blog/2024/10/07/starlink-for-a-mobile-world/","title":"Starlink for a Mobile World","text":"<p>In a previous blog post of mine, I retrofitted my camper trailer with a cellular internet solution for remote work. After having over a year to gather data on the practical, real-world performance of the Peplink router, I've come to the realization that I am a huge nerd and want even more technology! In this post, I'll show you my journey with Starlink and my impressions with its usefulness as a remote-work solution.</p>"},{"location":"blog/2024/10/07/starlink-for-a-mobile-world/#background","title":"Background","text":"<p>You may be asking yourself whether Starlink is really necessary for a remote working solution, to which I'm happy to proclaim a resounding no. I suppose I shouldn't be so dogmatic with that determination because it certainly depends on where exactly you plan on travelling. If you want to stay near civilization in any capacity, cellular internet at this point in 2024 is in almost all respects superior to anything else. It's not affected by weather (1) or trees, and it's available in nearly all locations that contain a small critical mass of people.</p> <ol> <li>For the most part, obviously some cell towers rely on microwave relays to locations with real underground cables.</li> </ol> <p>It didn't use to be this way. I remember the days when cell technology was atrocious, and getting access to a tower with decent throughput was a bit like winning a $2 lottery (and equally as satisfying. It's a quick jolt of dopamine until you realize that $2 ain't a lot of money, and you spent a lot of time trying to find that $2). With advances in signal processing and increased bandwidth sales from the FCC, cell has come a long way.</p> <p>Over the last year with my Peplink solution, I found that if I was ever near any small town, or any interstate at all, I almost always had access to decent cell coverage. In the instances where I didn't have good cell coverage, 90% of those situations wouldn't have been saved by Elon's solution due to tree coverage. So you may ask, why get Starlink? I already told you, I'm a technology nerd and I like to brag to people about how cool my remote work solution is.</p> <p>Jokes aside, the real reason comes from the recent product improvements Starlink has made, namely:</p> <ol> <li>Reduction in price of their antenna.</li> <li>Full support for internet-in-motion.</li> <li>Data plans tailored specifically to RVers. Their mobile plans offer unlimited data and the ability to pause and restart your plan at will. This is the killer feature that I was awaiting because being tied to inflexible billing schemes or plans that require stationary antennas were all non-starters.</li> <li>Increased reliability with the ongoing additions to their constellation.</li> </ol>"},{"location":"blog/2024/10/07/starlink-for-a-mobile-world/#whats-included","title":"What's Included","text":"<p>When you buy the Starlink antenna, it comes with three components:</p> <ol> <li>The antenna itself</li> <li>A WiFi router</li> <li>An AC-DC adapter</li> </ol> <ul> <li></li> <li></li> </ul> <p>Any semi-experienced RVer will immediately balk at the idea of running an ACDC adapter due to the fact that the DC battery from the RV has to be inverted to 60Hz AC, then converted back to DC by the adapter. You might also balk at the idea of running a separate router when we already have our Peplink Cellular modem/router! Not to fear, as a quick search on the internet shows that technically speaking, the Starlink antenna does not need these two components to operate.</p> <p>There exist third-party Starlink PoE conversion kits that are powered by 12V DC and will step up to 50V DC required by the antenna. The \"ethernet\" cable that the antenna uses is a proprietary form factor despite having the same number of wiring pins as ethernet. These conversion kits will bridge the gap between the Starlink ethernet, which requires the 50V PoE line, and the standard ethernet that you can send directly to your router. The other benefit is that it cuts out the Starlink-specific router which further aids in power efficiency.</p> <p>An example: https://www.amazon.com/XLTTYWL-Starlink-Protection-Conversion-Converter/dp/B0D4DH9TDN/ref=sr_1_1_sspa?crid=3MSWDXF0OT12S&amp;dib=eyJ2IjoiMSJ9.qP1_Af53E-CvdtPv7Z8Ofgdi-WmGsrNUh_pUqCU7mqM0kg8eZHiREHnkRnKASukvw53-zE8QEdZO6suABv0KcHQfiospY-KimIqMMPnpQbNk5K_vh80IUghh1iwSzbiQcKmUpalRPqlGA16_wXySqkvt3eF_s-pXVME5gYX7Z61T9Rgdj0WD-E69bUHeRGb-Nd1BDfqmMsqpqLw6JJXGCyLvUgLKZRXsSs1KrRBXsITz7TCcLh7Mjng6T_IEG8nM4wyL9_NQabBKBn7Mygp1wjrQd02OEE3Kvdvs1Af6LkE.vc0XT6Ve0TMX39b01i6X0JvnvOz-mPwDYB2KpDLSJfA&amp;dib_tag=se&amp;keywords=starlink+gen+3+dc+conversion+kit&amp;qid=1728328253&amp;s=electronics&amp;sprefix=starlink+gen+3+dc+conversion+ki%2Celectronics%2C171&amp;sr=1-1-spons&amp;sp_csd=d2lkZ2V0TmFtZT1zcF9hdGY&amp;psc=1</p> <p>For my proof-of-concept build, I will just use all the standard Starlink equipment and optimize it with a conversion kit at a later date.</p>"},{"location":"blog/2024/10/07/starlink-for-a-mobile-world/#setup","title":"Setup","text":"<p>The setup was surprisingly simple. I downloaded the Starlink app and it took me through a series of setup tasks. After having plugged in the ACDC converter, the router, and the antenna together, the system for the most part set itself up. The antenna determines what cardinal direction it's pointing and will inform you that it prefers to point north. I found this wasn't strictly necessary and was in fact ill-advised because to my north-east was a large oak tree. After pointing the antenna directly north as instructed, the reliability and throughput tanked to an almost unusable level.</p> <p>The obstruction visualization had me scan the sky with my phone's camera. I'm assuming it does some rudimentary \"blue pixels good, anything else bad\" algorithm where it determined the tree to be disruptive. The app also gives fairly detailed metrics into things like power draw, ping success rate, latency, throughput, and outage timelines.</p>"},{"location":"blog/2024/10/07/starlink-for-a-mobile-world/#performance","title":"Performance","text":""},{"location":"blog/2024/10/07/starlink-for-a-mobile-world/#an-unfair-situation","title":"An Unfair Situation","text":"<p>I spent a night in the Middle of Nowhere Nebraska and ran a few tests. The campsite I was at had a large oak tree that obscured about half of the sky, so we can consider this rather unfair, although informative of what we can expect in non-ideal situations.</p> <p>The first test was a simple speed test in the middle of the night, where I was able to pull 147 Mbps down. The next speed test I took was at around 2:30PM the following day where I got 41Mbps down/28Mbps up. While that's over three times slower than the middle of the night, it's still fast enough for most activities.</p> <p>Ping success rate, for my purposes, is the most important metric. I was able to get around 90% success rate in the middle of the day which is going to be an issue for realtime workloads like video calls or gaming.</p>"},{"location":"blog/2024/10/07/starlink-for-a-mobile-world/#video-calls","title":"Video Calls","text":"<p>I tested Starlink in a video call with about 20 participants. It performed fine for the most part but it would drop about every 10 minutes for sometimes longer than 15s. The Big Oak Tree (BOT for short) almost certainly is playing into this droppiness. It happened frequently enough to be distracting, and if I had been talking in this call, it would have been disruptive to the natural flow of communication and probably pretty annoying to the other participants.</p> <p>The performance in this scenario is poor enough that I can't say I would rely solely on Starlink for video calls. I would want to switch to a cell connection if it's available.</p>"},{"location":"blog/2024/10/07/starlink-for-a-mobile-world/#a-fair-situation","title":"A Fair Situation","text":"<p>Another campgroud I stayed at had a much clearer view of the sky. It was dotted with some trees but overall it gave a fairly unobstructed view for starlink. I placed the dish on the top of the trailer and ran the same tests as above. I was able to achieve similar throughput, but in this case I obtained 99.8% reliability. That's pretty remarkable and is approaching an acceptable level for basically all workloads.</p>"},{"location":"blog/2024/10/07/starlink-for-a-mobile-world/#video-calls_1","title":"Video Calls","text":"<p>Video calls in this situation was pretty much seamless. I was able to communicate with fairly little interruptions, and it was nearly impossible to tell that I was using satellite internet!</p>"},{"location":"blog/2024/10/07/starlink-for-a-mobile-world/#installation","title":"Installation","text":""},{"location":"blog/2024/10/07/starlink-for-a-mobile-world/#roof-mount","title":"Roof Mount","text":"<p>The Starlink mobility mount comes with 4 M8-1.25 35mm bolts. I drilled 4 8mm holes in the roof using the mount as a template (Starlink suggests 10mm but this wasn't necessary, the 8mm hole was big enough). It turns out that the provided bolts were too small, so I went to Home Depot and got some M8-1.25 40mm bolts. You can see the result below:</p> <p>Danger</p> <p>Do NOT use 3M adhesive tape for the mount. The forces experienced by the dish when driving are too strong for any tape to properly hold it down. You MUST bolt or screw this mount on, there is no other option (ask me how I know \ud83d\ude2c)</p> <p>Once the bolts were secured to the roof, I applied a liberal amount of Dicor Self-Leveling Sealant around the edges and top of the mount. The mount feels extremely sturdy and I was able to apply around 50 lbs of upward force without any noticable movement.</p>"},{"location":"blog/2024/10/07/starlink-for-a-mobile-world/#roof-cabling","title":"Roof Cabling","text":"<p>For the cabling, I followed the same strategy I employed when I did my Peplink cellular install. I didn't take pictures of this process because it's identical to what I've already done. Here are the pictures from that blog post:</p> <p>The only difference this time is that I drilled 1\" holes in the wall next to the shower and used 3/4\" rubber grommets around the holes to protect the cables from the sharp edges. I realized my mistake of not using grommets for the Peplink install almost immediately after I was done, so I took the opportunity to fix those cables as well.</p>"},{"location":"blog/2024/10/07/starlink-for-a-mobile-world/#interior-mounting","title":"Interior Mounting","text":"<p>I drilled a hole next to the fuse box and routed the Starlink cable through it. My temporary setup using all of the Starlink-provided hardware looks something like this:</p> <p>To save battery, you don't want to be using the provided router and power supply due to the need to convert the AC current (which has itself been inverted from the DC battery supply) back into DC. In my case, I didn't need the router anyway because I already had one. So I bought a Starlink v3 POE Injector that takes 12V DC as input and supplies the dish with the higher voltage it needs to operate. This voltage is sent over the provided Starlink ethernet cable, thus the \"Power Over Ethernet\" (POE).</p> <p>The POE injector has two other ports: one vanilla ethernet port that I'll plug into my Peplink router, and the other Starlink cable port that will be plugged into the dish.</p> <p></p>"},{"location":"blog/2024/10/07/starlink-for-a-mobile-world/#electrical-routing","title":"Electrical Routing","text":"<p>Both my Peplink router and the POE injector need a reliable source of 12V DC. I also wanted to install toggle two switches so I could toggle the power to the POE injector and the Peplink router independently (for example, in situations where cell service is sufficient. Cell radios only use about 5 watts, versus Starlink which uses around 50 watts).</p> <p>The wiring flowchart isn't too complicated. It consists of two mirrored parts, one for the POE and another for the router: (1)</p> <ol> <li>Technically, the router was already wired up but I didn't have it connected to a switch, so this project was an excuse to do that.</li> </ol> <pre><code>flowchart TD\n    12V+[12V+ Battery]\n    12V-[12V- Battery]\n    Fuse1[15A Fuse]\n    Fuse2[15A Fuse]\n    Fuse3[15A Fuse]\n    Fuse4[15A Fuse]\n    RockerSwitch[Rocker Switch]\n    RockerSwitch2[Rocker Switch]\n    POE[POE Injector]\n    Router[Peplink Router]\n    Starlink[Starlink Dish]\n\n    12V+ --&gt;|12V+| Fuse1\n    Fuse1 --&gt;|12V+| RockerSwitch\n    RockerSwitch --&gt;|12V+| Fuse2\n    Fuse2 --&gt;|12V+| POE\n    RockerSwitch --&gt;|12V-| 12V-\n    POE --&gt;|12V-| 12V-\n    POE &lt;--&gt;|Ethernet| Router\n    POE &lt;--&gt;|CAT6| Starlink\n\n    12V+ --&gt;|12V+| Fuse3\n    Fuse3 --&gt;|12V+| RockerSwitch2\n    RockerSwitch2 --&gt;|12V+| Fuse4\n    Fuse4 --&gt;|12V+| Router\n    Router --&gt;|12V-| 12V-\n\n    RockerSwitch2 --&gt;|12V-| 12V-</code></pre> <p>Basically, the rocker switch will gate the flow of electrons towards the positive terminal of the POE injector. The switch also has an LED indication light, so it needs a direct negative connection to the battery. The first picture below shows the terminus for the 12V+, located on the main DC cutoff switch. The negative terminal in the second picture is a bus bar that should have a few open slots.</p> <ul> <li></li> <li></li> </ul> <p>Using standard 14AWG wiring and crimp connectors, I connected the pieces together. The pictures below show the rocker switch up close and what they look like after being installed.</p> <ul> <li></li> <li></li> <li></li> <li></li> </ul> <p>And voila! It worked liked a charm! I waited for Starlink to boot up and amazingly it just... kind of worked? You might notice in an install like this that the Starlink app doesn't work anymore. This is because it needs a static route configured in the router:</p> <ul> <li>Network: 192.168.100.0</li> <li>Subnet Mask: 255.255.255.0 (1)</li> <li>Destination NIC: Ethernet port you plugged into the router.</li> </ul> <ol> <li>Gateway <code>192.168.100.1</code> is implied.</li> </ol> <p>It looks roughly like this:</p> <p></p> <p>While this is close to the right configuration, I couldn't quite get the Starlink app to detect the dish on the local network. However after waiting a few minutes, the router successfully gained an internet connection and the app was able to give me diagnostic information about the dish through the internet (I'm guessing).</p>"},{"location":"blog/2024/10/07/starlink-for-a-mobile-world/#performance_1","title":"Performance","text":"<p>I measured the power draw and the bandwidth performance of this new setup. With both the router and the dish turned on (and all other components in the RV turned off), I measured about 50 watts of draw steady state. The throughput was about as expected for Starlink: 103Mbps/16.5Mbps.</p>"},{"location":"blog/2024/10/07/starlink-for-a-mobile-world/#conclusion","title":"Conclusion","text":"<p>While this post may lend to the idea that such a permanent Starlink install was easy, it was the distillation of many hours of failures and setbacks. Most notably, I had a separate install where I foolishly thought that using VHB tape to adhere the mount to the roof would be sufficient, but I was woefully wrong. It lasted most of my prior trip out west, but on the last leg of the trip, it was mysteriously absent from my roof. So somewhere out west, it's lying on the side of the road and hopefully didn't damage anyone's property. Yikes. After that incident, I decided to drill into the roof so that it was essentially impossible for it to happen again, and that is what you see here.</p> <p>At this point, I have no further ambitions to improve my mobile data setup. This is a behemoth of remote work. Consider the fact that I now have 2 independent cellular WANs along with Starlink: the Peplink router gives me the flexibility to efficiently switch between the WANs depending on the physical constraints of wherever I am. Furthermore, I could choose to use Peplink's Speed Fusion Cloud product that will multiplex the packets across all the WANs, arbitrate and reconstitute them in a cloud datacenter, and send it off to the final destination. Such technology gives you truly seamless and effortless failover in the case one or more of your WANs suffers a degredation.</p> <p>Speed Fusion Cloud does not receive as much attention as it deserves. It relies on a principle I grew familiar with during my time working as a backend engineer for a high frequency trading firm, i.e. duplicating packets across multiple paths and arbitrating them at the landing site. This was used to send packets between exchanges mainly through two routes: a fiber optic line (higher latency but reliable) and a microwave line (low latency but unreliable). This same principle is directly applicable to cellular and satellite WANs.</p> <p>Anyway, let me know your thoughts in the comments below. Thanks!</p>"},{"location":"blog/2024/10/14/colorado-in-the-fall/","title":"Colorado in the Fall","text":"<p>I can't keep away from Colorado, so I went back in the fall of 2024 as my last trip of the year. It's hard to imagine how the same place can re-impress even after multiple visits, but such is the magic of the mountains.</p>"},{"location":"blog/2024/10/14/colorado-in-the-fall/#vedauwoo-campground-laramie-wy","title":"Vedauwoo Campground <code>Laramie, WY</code>","text":"<p>Vedauwoo is a campground between Laramie and Cheyenne, WY. It's a great pitstop before continuing on to the Rocky Mountains. I've been here multiple times and it never fails to delight.</p>"},{"location":"blog/2024/10/14/colorado-in-the-fall/#pikes-peak-woodland-park-co","title":"Pikes Peak <code>Woodland Park, CO</code>","text":"<p>My mom decided to come along for part of my trip, so I picked her up from the Denver airport on my way through, and we headed off to Woodland Park. Also holy shite, I didn't realize tolls were so expensive in this part of Denver! \ud83d\ude27</p> <p>We ended the day with a trip to the Wines of Colorado, which is probably my favorite restaraunt in the area. We got a table by the creek and got absolutely blasted off of some nice wine flights. (jk I am a responsible drinker, believe it or not)</p>"},{"location":"blog/2024/10/14/colorado-in-the-fall/#morning-views-woodland-park-co","title":"Morning Views <code>Woodland Park, CO</code>","text":"<p>On my trip a few months ago, I found our secret campsite that has stunning views of Pikes Peak. It was a long drive on bumpy forests roads to get there, and I wasn't sure it was even going to be available. But alas, it was! </p> <p>We parked at the campsite and then realized there was supposed to be a near nation-wide Aurora Borealis tonight. Sure enough, once it was dark enough, you could see a faint glow in the sky that our phone cameras captured even better than our eyes could. It was a spectacular light show. It took a long while to convince myself that the aurora was not just light pollution because it kind of sort of looked like lights you see from distant cities, but it was shifting and moving colors enough to very obviously not be light pollution.</p> <p>The magic continued the next morning as we were visited by a group of 4 bucks right outside our window. They couldn't see us in the trailer so we had a front-row seat to the best zoo in the world: nature itself! Damn, does it get any more magical than this? (hint: yes it does)</p>"},{"location":"blog/2024/10/14/colorado-in-the-fall/#shelf-road-cripple-creek-co","title":"Shelf Road <code>Cripple Creek, CO</code>","text":"<p>The fastest route to Ca\u00f1on City from Woodland Park is through this through Shelf Road. It's a 4WD trail that winds through BLM land dotted with numerous old mining claims, most of which have been repurposed into grazing ground and private single-family houses.</p>"},{"location":"blog/2024/10/14/colorado-in-the-fall/#royal-gorges-canon-city-co","title":"Royal Gorges <code>Ca\u00f1on City, CO</code>","text":"<p>At the end of Shelf Road is Ca\u00f1on City which isn't exactly impressive as far as mountain towns go, but it does have a lot of interesting history. More notably, it has one of the best Indian restaurants I've ever been to called Nirvana Culinary Paradise. We stopped there for a late lunch and then continued onto the Royal Gorges.</p> <p>My mom wasn't a huge fan of this park. The picture of her walking across the bridge above might look so nice and serene, but her internal state was more like this:</p> <p></p> <p>She was clenching the handrail with the power of a thousand suns and desperately trying not to freak out. Oops!  I didn't mind the heights that much so I was just enjoying life. It's hard to describe how far down the river is from the bridge. It's a looong way.</p> <p>The park itself is pretty cool and definitely worth the visit if you're in the area. At the bottom of the gorges is a narrow train track that employs some impressive engineering in order to get around the space constraints. Parts of the river below were so narrow that there wasn't enough space for the track ballast (the crushed stones you typically see rails sitting on top of). So they had to engineer this suspension system that allowed the weight of the train to be driven into both sides of the gorge. Here's a marketing video of the train you can take a ride on at the bottom of the gorge:</p>"},{"location":"blog/2024/10/14/colorado-in-the-fall/#disaster-in-paradise-fairplay-co","title":"Disaster in Paradise <code>Fairplay, CO</code>","text":"<p>I took my mom back to the Denver airport a few days after we went to the Royal Gorges. Next on the agenda was to make my way to Crested Butte (pronounced byoot, as in like beee-yooot-iful. Yes I have to acknowledge to you children that it looks like Crested Butt). The plan was for me to drive from our campsite in Woodland Park 3.5 hours to Crested Butte. I left early enough in the morning that I should get there before dark.</p> <p>However, the gods had different plans for me. How arrogant of me to assume my trip would go without a hitch! Around the area of the nuclear explosion, I was humbled:</p> <p></p> <p>The gods decided they hated me today, so they smote my trailer's right tire with nothing other than the head of a... panicking mule deer? I swear to god this happened, I'm not making it up. As I got on the interstate, a MULE DEER was panicking on the highway because it was stuck in this 15 mile stretch of road that is flanked on both sides by fence. Of course I slowed down as fast as I could when I saw it, but as I was passing it at round 20 mph, it decided to RAM MY F-CKING tire while I was driving by. My thoughts uttered the word \"damn\" and I thought I would just go to the next town to look for damage instead of pulling over to the side of the road. Well 5 minutes after it rammed my tire, I felt the trailer pull to the right fairly firmly. Smoke was billowing from behind me as I looked in the mirror, so I was forced to stop and look at what's happening. This is what I saw:</p> <p></p> <p>I don't know if you can see right there in that there picture, but that's what's called a f-cked up tire. I didn't have many options. I could either call a tow truck, which I really really really didn't want to do, or try to install the spare tire. Installing the spare seemed easy enough, so that's what I did.</p> <p>I got the spare on just fine, but it was only at this moment did I realize that my electric tire pump wasn't capable of pumping to the spare's recommended PSI of 65. It could only go to 35 . Woohoo!</p> <p>I made a detour to Fairplay, CO which fortunately I was already familiar with and knew that there was a somewhat (or mostly) year-round campsite with full hookups. And they were still open for the season! I did some research and realized that ST (Special Trailer) rated tires, the kind that most trailers install, are actually shitty pieces of junk. ST tires are designed to have thicker sidewalls than normal car tires, but usually have less ply counts on the treads than normal car tires. This is done to stabilize the twisting motions that trailers experience. But overall, the construction tends to be worse because people don't usually care too much about their trailers, and they just want something cheap. These cool, beefy looking offroad tires you see on my trailer are in fact Rainier ST Apex tires. Their speed rating is 81mph and the max load of the two tires combined was 4800 lbs, only 600 lbs over the trailer's max gross weight of 4800 lbs. So, not only would the tire be damaged at anything over 80 mph, it didn't have a lot of leeway for payload (although yes, to be fair I would never load the trailer past 4000 lbs as that's patently dangerous).</p> <p>My research led me to the conclusion that for single-axle trailers, it's actually totally fine to put truck tires on them. So I bought two Falken Wildpeak A/T4W All Terrain LT235/75R15 116/113R E tires and had them shipped to the local tire shop.</p> <p>Here's the comparison of specs between the two:</p> Rainier ST Apex Falken Wildpeak Speed Rating 81 mph 106 mph Load Rating (Combined) 4840 lbs 5512 lbs Ply 6 10 Max Pressure 55 psi 80 psi <p>While I was waiting for the tires to arrive, I took a hike up Mt. Pennsylvania.</p> <p>Although it's not a 14'er, it's still a decent hike up to the summit at 13,013 ft. Parts of it were especially challenging because the trail was basically straight up with no switchbacks. It wasn't anywhere near as challenging as Mt. Sherman (an actual 14'er I did a few months prior that I haven't yet written a post about) but it was the kind of exercise I needed.</p>"},{"location":"blog/2024/10/14/colorado-in-the-fall/#trip-to-crested-butte","title":"Trip to Crested Butte","text":"<p>Stranded in a town I didn't necessarily want to be in, I did this for the next few days: work remotely, hike a mountain, make dinner and go to sleep. My new tires eventually arrived so I put my trailer on jacks and remove the rims, go to the tire shop, pet the tire shop dog, and finally put on my brand spakin new  off-road  TRUCK TIRES! </p> <p>I was back in business baby! I started my trek back to Crested Butte, my original destination.</p> <p>The drive was long and involved going through some incredibly gorgeous towns. I drove through Buena Vista which stunned me in its majesty. Also keep in mind, this was near the tail end of fall so there will still many trees that had fire yellow leaves. I began driving down US 306 (instead of US 285/50 that Google maps shows above) and I could really only describe it as driving through a fairly tale. Like, you know those movies where the main characters (who happen to be magicians) are riding a train across a picturesque Swiss mountain landscape? Where wizards and fairies and knights are running around? It kind of felt like that, although this time the magical story was real. Don't believe me? Sit down in that chair and shut yer trap. Let me show you something else I saw:</p> <p>A dark road! No wait, there's something in the sky? A UFO? Joe Biden? You see that streak of cloudy light just above the trees? That's a comet! Now I'm not gonna get sentimental and tell you that that's a sign from the sky gods sent specifically to me to deliver a message that says \"move out here\" because I'm not crazy, but that's exactly what happened.</p> <p>That comet's name is C/2023 A3 Tsuchinshan-ATLAS, so I just call it Atlas for short. Anyway, Atlas and I saw each other for the next hour or so, but when I got to my campsite, I looked all around and realized that the mountains were now obscuring it from my view. I never saw it again </p> <p></p>"},{"location":"blog/2024/10/14/colorado-in-the-fall/#stay-at-crested-butte","title":"Stay At Crested Butte","text":"<p>I pull up to my campsite for the next few days and can't really see much of anything beyond what the starts illuminate. I picked a spot that I guess was good, but I was so tired from the driving that I just wanted a beer then to go to sleep.</p> <p>I wake up the next morning to THIS</p> <p>I've got a mountain in front of me, real-life cowboy cattle ranchers behind me, and a BEAUTIFUL rainbow just jettisoning out of the porta-potty as if someone on an exclusive Skittle diet just had explosive diarrhea.</p> <p>I'm just partly kidding, the porta-potty was in town and there were no signs of civilization at my campsite. But holy cow this place is amazing! It turns out I actually came here for a reason, because the mountain behind me is Mt. Gothic, a not-quite 14'er but considered to be a challenging, steep climb nonetheless.(1) I saw that mountain on the map and again thought to myself: \"eh, that's a good one to climb up.\"</p> <ol> <li>I say this a bit sheepishly because I'm sure many Coloradans (and my future self) are looking at that and thinking \"haha that's so cute.\"</li> </ol> <p>That's the beauty of this state. You can just pick a random spot on the map and say \"meh, I'll hike up that mountain today.\" In fact, that's how all of my few mountain hikes have gone so far. I just see one off in the distance and think to myself yeah, that'll do.</p> <p>I only really had one day that I could hike up the mountain: on Saturday when I could dedicate the whole day, and not Sunday because I'll have to start heading back home. So I worked Tuesday through Friday, just enjoying the scenery and not doing anything too strenuous (and I couldn't anyway, because it was getting dark at 4:30 pm).</p> <p>Saturday morning comes and I head off to the mountain. By this time it had started snowing pretty heavily which made me concerned for my ambitions.</p> <ul> <li></li> <li></li> <li></li> <li></li> <li></li> <li></li> <li></li> <li></li> <li></li> </ul> <p>I'm not one to take unnecessary risk, and I knew it would be an unnecessary risk to climb to the top of a mountain you're unfamiliar with when it's snowing and you don't have proper cleeted boots. I thought I would go as far as I could before I thought the slippery ground would pose a danger.</p> <p>I'm glad I did go on the hike despite the snow because I got to meet those *cue David Attenborough voiceover* wild mountain cows, a rare breed of cow only seen in this part of CO.</p> <p>I didn't even know cows would even want to climb a mountain, but I'm just here to learn. I decided near the point when the trail started sharply angling upwards that it wouldn't be wise of me to continue any further, so I sadly had to start heading back.</p> <ul> <li></li> <li></li> <li></li> <li></li> <li></li> </ul> <p>I can't lie that I was on one hand disappointed because the weather prevented me from achieving my goal of making another summit, but on the other I got to witness (not surprisingly) another incredible view. The peace, the solitude, the serenity of this land when the snow began to come felt palpable. It was as if I was going back to a place that was familiar to me, although I had never been there before.</p>"},{"location":"blog/2024/10/14/colorado-in-the-fall/#on-the-road-again","title":"On The Road Again","text":"<p>That last hike on Saturday marked the beginning of the end for this trip. The following Sunday morning, I packed up and started making my way back. I drove through Monarch Mountain where it had also recently experienced snow, but was high enough in altitude to not melt it off. My last last camping site in the mountain was some dispersed location near Bailey, CO. I don't know exactly where, I just remember that it required driving through lots of washboard dirt roads that I thought were gonna shake the other tire (the one that didn't blow up) off my trailer.</p> <p>Again, it was stunning. Again, I was blown away at the beauty around me. Although this night I had a sense of longing. You know that feeling when you're about to say goodbye to someone you love that you know you won't see again in a long time? There's not an English word for that feeling, but that's what I felt.</p> <p></p> <p>I sat outside in the cold night just taking in as much as I could, because this would be the last time I see the mountains for probably 8 months. And that felt like eternity. I looked at the twinkling lights of the houses on the mountain thinking to myself how lucky those people must be to be able to live in a place like this.</p>"},{"location":"blog/2024/10/14/colorado-in-the-fall/#links","title":"Links","text":"<ul> <li> Google Photos Album</li> </ul>"},{"location":"blog/2024/10/14/hymnal/","title":"Hymnal","text":"The faith is sought in walls of stone,   In dusty books, in creeds alone,   While outside waits the open air,   A truth too simple to compare.   The pews abound with heads bowed low,   Yet miss the sermon in the snow,   The rustling leaves, the storm\u2019s embrace,   The way the wild reveals her grace.    For all we chase in lofty halls,   Is found in nature\u2019s whispered calls\u2014   But captured by factitious ways,   We lose the light of endless days.    The forests reach like chapel spires,   Their branches lit with autumn fires.   A path winds through the woods alone,   A place of prayer not set in stone.    No priest but wind, no hymn but leaves,   The earth\u2019s old truths the heart receives.   In quiet steps through meadow's green,   I find the faith that lies unseen.    For in the silence, more is said   Than books or prophets long since dead\u2014   A wisdom deep, a voice so grand,   That we can only hope to understand."},{"location":"blog/2024/12/03/2024-career-reflections/","title":"2024 Career Reflections","text":""},{"location":"blog/2024/12/03/2024-career-reflections/#highlights","title":"Highlights","text":"<p>For me, 2024 has been nothing short of extraordinary for a number of reasons. Just some of the main highlights include:</p> <ol> <li>I got engaged to the love of my life.</li> <li>Me and my fianc\u00e9 couldn't wait to get married so we went to the courthouse a couple months later and sealed the deal!</li> <li>I changed jobs.</li> <li>I went to Colorado 2 times. I don't know if you can tell, but I REALLY love Colorado.</li> <li>I saw a total solar eclipse in Olney, IL with my friends.</li> <li>I drove by RAGBRAI to support one of my friends. If you're unfamiliar with it, here's a great blog post from one of my Jump ex-coworkers.</li> </ol>"},{"location":"blog/2024/12/03/2024-career-reflections/#career-change","title":"Career Change","text":"<p>My prior employer, Jump Trading, had graciously offered me an engineering role straight out of college back in 2018. Throughout my 5 years at the firm, I was exposed to a broad range of cutting edge technology and environments that propelled me to a competency that I feel college could never give. More so, I benefited from a number of close mentors who coached me out of bad habits and showed me grace when I needed it most. It also can't be denied that I regularly felt imposter syndrome because the sheer density of brainpower I was surrounded with was humbling.</p> <p>Sadly, all things end, and in late 2023 I decided my time at Jump was one of them. There was a broad (and noisy) kerfuffle that the crypto community experienced in 2022 that started with FTX and seemed to wind its way to a number of other coins and exchanges. The stock market also experienced headwinds with the post-COVID interest rate hikes and general market fear over the direction of the economy. These factors led to what I felt was a changing working environment that made it difficult for me to be successful in my role not just in terms of happiness, but also for the general trajectory of my career.</p> <p>Those two sentiments I just described, both respect and disaffection, might sound contradictory. I credit Jump for catapulting my career to where it is today and to the relative ease with which I'm able to navigate the job market. However, it's true that business realities can change on a dime, often through no fault of our own. The HFT community can be characterized as being really good when times are good, but really really bad when times are tough. To be clear, I have nothing but fondness for those I had the pleasure to work with, and there are many invaluable lessons I learned through the grace of my mentors and the compassion they showed me.</p> <p>Further, it's hard for me to not feel attached to my work. I think about my time at Jump a lot and of the many wonderful memories I have. I admit that I was dejected when I came to the realization that it was time for me to move on, but I'm glad I did because my new position at Lambda Labs is challenging me in the ways that I needed to be challenged. I feel a deep sense of mutual respect and trust between myself and everyone I work with, and that's a kind of environment that retains talent in the long run.</p>"},{"location":"blog/2024/12/03/2024-career-reflections/#a-story-of-mistakes","title":"A Story of Mistakes","text":""},{"location":"blog/2024/12/03/2024-career-reflections/#money-aint-everything-dummy","title":"Money Ain't Everything, Dummy","text":"<p>I consider myself to be a bit of a populist. That may be considered a charged term in today's political climate, but allow me to explain. There is a general principle in the US that it's considered to be rude, or impolite, or frowned-upon, or <code>[insert any negative adjective]</code> to talk about one's compensation. This sentiment is one propagated by none other than the very folks who have a vested interest in making sure you don't talk about it. Could you guess which class of society that is?</p> <p>It's no secret that HFT pays well. Like, really well. Fresh out of college, I was within the 99th percentile of earners for my age. In fact I was probably making more than what most college graduates make at big tech. This is compounded by the fact that Chicago, where Jump Trading's headquarters reside, can be considered in absolute terms to be a medium cost of living city. You can imagine that these factors led to a feeling that I \"made it\". I didn't worry about money. It was easy to come by, and the end-of-year bonuses endowed upon my savings an enormous boon. This is why I'm so adamant in the belief that if you can get a job in an HFT early in your career, you MUST seize the opportunity. You might even say that compounding interest demands you to do it.</p> <p>You, dear reader, also know that money is not everything. During my last few years at Jump, I struggled mightily between the pursuit of all things green and the less tangible ideals such as \"career growth\". What do I mean by that precisely? Well, it became a fact that through a series of personal and professional missteps that I cornered myself into a part of Jump that, while important to the business, didn't challenge me in the ways I needed to be challenged. I struggled to find the kinds of responsibilities I needed to assume to grow beyond the role of \"take JIRA ticket, write code for JIRA ticket, deploy code.\" It's possible this was more a function of what people trusted me to do, but regardless of the reason, I felt almost... bored? The kinds of responsibilities I needed involved cross-cutting concerns. I wanted the ability to create new initiatives, to play the politics game of getting people on your side, to be in the room when decisions are made. Instead I was relegated to only following. Intead of being a leader, I was in some ways a mindless zombie just doing what other people told me to do. No matter what I tried, I couldn't crawl out of that expectation.</p>"},{"location":"blog/2024/12/03/2024-career-reflections/#let-the-sunlight-bleach-all-sins","title":"Let the Sunlight Bleach All Sins","text":"<p>The interesting thing to note is that Illinois has this law called the <code>Personnel Records Review Act</code>. In short, this law gives Illinoisans the right to review their personal record from any employer. I submitted this request to Jump, and they returned to me an extremely comprehensive file that contained all of my end-of-year reviews where my colleagues enumerated in detail all of the ways that I had succeeded and fallen short.(1) Graciously, many of them were incredibly kind and generous in their appraisals of me (sometimes I feel unfairly so). But, I felt it was necessary for me to truly understand the ways I had been perceived, to let the sunlight bleach all sins.</p> <ol> <li>While these reviews were technically anonymous and had no names attached, I could usually estimate who had written them (sorry).</li> </ol> <p>What I learned from that experience was that while I was generally well-regarded in my technical ability, there were multiple complaints about the way I approached disagreements. Some of my reviewers in the early years noted that they grew so frustrated with my stubbornness that they stopped engaging with me altogether. Ouch! Clearly for any self-respecting engineer, your peers deciding that they'd rather disengage than collaborate with you is nothing short of a total failure on your part.</p> <p>While the personnel record highlighted some flaws I had to address, I also learned that people said many wonderful things about me and the quality of the work I generated.(1) I could perform the work, and exceptionally well, but I never found myself being granted the responsibilities and trust that I needed to truly stretch my professional skills. I found myself constantly embroiled in bike-shedy debates on technical direction, engineering decisions, and nit-picks that I felt were needlessly complicating my efforts to achieve my goals. I also didn't have the political wherewithal to ignore these distractions. I partially credit this with the dearth of trust that people felt in me. The other part can certainly be placed on other parties who may or may not have been dealing with their own demons, but elaborating on that wouldn't be of much use to the pursuit of  introspection .</p> <ol> <li>And yeah, of course I knew what people said about me to my face, which was almost always positive. But that's a biased sample because most people are not going to say negative things to your face. What people say under the veil of anonymity is going to be far more authentic and genuine.</li> </ol>"},{"location":"blog/2024/12/03/2024-career-reflections/#professional-relationships","title":"Professional Relationships","text":"<p>This goes back to the topic of focus in this section: compensation. While my compensation was good in absolute terms, it was neither notable in HFT-relative terms, nor particularly exciting to me anymore. I found that in my pursuit of money, I had lost focus of the thing that carries you more than anything in life: relationships. I had squandered my relationships at Jump. I failed to foster them, I failed to nurture them. I thought that if I just work hard enough, churn out enough code, work enough hours, deliver enough JIRA tickets, that my compensation will follow. When I found that it didn't, that I was barely within the standard deviation of the mean, I was frustrated and deflated. Of course I was more myopic at that time and didn't realize that while it was true the business was suffering at that particular moment in time, I too had a hand to play in not being my best self to those around me.</p> <p>Of course I could get into a long rant on why I wasn't always my best self. This could turn into a long, complicated diatribe, but some of the main points were that I disliked Chicago, I disliked having to fight through crowded trains to sit in a chair only to barely talk to anyone, I was unhappy with the direction of my career, I was unhappy with the cultural environment I was in, I was dealing with personal issues at home, I was unhappy that I couldn't spend more time in nature (although my managers, to their credit, tried very hard to accommodate this need of mine, but I always felt I was asking for too much)... and so many other reasons. The confluence of these factors made me a generally unhappy person, and unhappy people aren't always pleasant to be around.</p>"},{"location":"blog/2024/12/03/2024-career-reflections/#solace","title":"Solace","text":"<p>I'm pleased to report however that I'm currently the happiest I've ever been in my life because pretty much all of my issues in life could be traced back to how stressed and unfulfilled I was at work. Now that I'm in an environment that encourages ambition and doesn't give a f*ck where I do my work, I can spend all of my time with the people I love, go to the places I love (*COUGH* Colorado *COUGH*), work when I want, how I want, and on whatever I want, all the background problems in my life have kind of just... faded away.</p> <p>The fact of the matter is that if I truly wanted to, I could have stayed at Jump for as long as I wanted. However, I ended up feeling that it was necessary for me to move on from that environment. There were so many reasons that factored into that decision even beyond my failed professional relationships. Leaving Jump ended up being far and away the best professional decision I made in a long time, even if it meant forsaking what for many is the primary and sometimes only goal in one's career: money.</p> <p>I found a significant amount of solace in the fact that I was by no means an outlier in this time. Many people I respected in the company had been leaving with increasing frequency, so it wasn't just me. Regardless, it was time for me to unshackle myself from the fruitless pursuit of money and to approach my career from the proper perspective. The money, the trust, the status, the respect will follow once I put in the effort to cultivate those relationships, do good work, and constantly strive to stretch my technical knowhow, not the other way around. If there's one lesson to be learned, it's that your likability is a multiplicative factor to your technical proficiency. In fact it's worse: negative likability follows you around like a ball-and-chain for the rest of your tenure at any company, even if you correct it post-hoc!</p> <p>To those who dealt with my professional/social shortcomings: I apologize. To those same people who showed me grace: I thank you immensely.</p>"},{"location":"blog/2024/12/03/2024-career-reflections/#lambda-labs","title":"Lambda Labs","text":"<p>That brings me to where I land myself today: Lambda Labs. Lambda, if you are unfamiliar, is a startup cloud company that sells infrastructure tailored towards AI research. Practically speaking, this means we're building out large, GPU-dense datacenters with large, expensive Infiniband and ethernet fabrics. It's not entirely unlike what trading firms do, but the HPC environments add another significant layer of complexity that comes with multi-tenancy requirements.</p> <p>Being a public cloud, we have to host customers in a virtualized environment. This means dealing with technologies like:</p> <ol> <li>QEMU</li> <li>SR-IOV</li> <li>NDFC (as an SDN)</li> <li>OVN+OVS</li> </ol> <p>And many other related services and technologies that go into securely hosting a public cloud with private customer data. It's been an exciting space to live and breathe because I'm in many cases one or two degrees of separation away from rubbing shoulders with the current AI titans of the industry. That's a bit of a vain observation to make, but I bring it up to highlight the excitement in which I find myself in.</p> <p>Another observation I've made is that AI has a real, tangible benefit to society. Many of our customers are generating models that can predict protein folds, examine CAT scans, generate videos and images, provide possible diagnoses to health issues, and lots of other incredible uses. In fact I have been increasingly using chatbots like ChatGPT to distill complex technical topics and ask it for inspiration on troubleshooting esoteric problems.(1)</p> <ol> <li>This deserves a whole blog post on its own, but I made the journey from extreme skepticism to whole-hearted believer when it comes to chatbots in the workplace. I don't view things like ChatGPT as a replacement for the human brain, but rather a more powerful alternative to Google and Stack Overflow. It still comes with the usual caveat of \"don't believe everything you see on the internet\" because it can be wrong in big ways!</li> </ol> <p>The cool thing about startups, especially ones with such meteoric growth, is that it's relatively easy to make a big impact. This is contrasted to larger companies (like Jump) that have a somewhat entrenched technical culture and already have \"the way\" of doing things.(1) In just a few short months, I was able to identify a huge business need that Lambda had around customer VM observability. Specifically, the need for us to ship customers metrics about their own VMs. Every public cloud deals with the same question and the solution usually looks something like:</p> <ol> <li>Install a metrics collection service on the VM.</li> <li>Ship the collected metrics to a hosted data store.</li> <li>Expose the data store through some kind of API gateway so customers can access it directly (either through a UI or API).</li> <li>Sell the metrics for profit!</li> </ol> <ol> <li>An entrenched technology culture by itself is not a bad thing because it often means that an organization has found a solution that works well enough. However it does mean finding ways to make company-wide impacts is sometimes a fruitless effort. In the worst cases, it means that the company has become so ossified that making dramatic business pivots is exceedingly difficult</li> </ol> <p></p> <p>I was able to identify this need both from comments that executive leadership would make, but also from customers lamenting the lack of this fairly basic product. This led to me leading a project that we call the lambda-guest-agent. It's simple in theory but in practice it's a quagmire that deals with topics like data privacy laws, security, SOC compliance, cross-team collaboration, priority management, and of course the fun technical aspects like metrics collection, Prometheus, public APIs, frontend graphing technologies, API gateways... you get the picture. Building a public cloud is HARD and even conceptually simple things tend to take enormous effort. This is very much contrasted to HFTs where concepts can be turned into production with relatively minimal fuss.</p>"},{"location":"blog/2024/12/03/2024-career-reflections/#building-a-cloud-from-scratch","title":"Building a Cloud From Scratch","text":"<p>Lambda's cloud is new. Originally, Lambda focused on building and selling desktops tailored to AI research. Within the last few years, they decided to pivot to creating a fully-fledged public cloud as the demand for AI hardware only increased. Researchers needed enterprise-grade hardware that could not fit inside a typical desktop chassis, and the leadership rightfully noticed the dearth of performant, affordable HPC solutions.</p> <p>What's been striking to me is two things:</p> <ol> <li>There aren't many opportunities to work at a company that is building a big-boy cloud from scratch</li> <li>Investors LOVE the idea of being able to get in on the ground floor of what appears to be the largest technological revolution in the last 20 years.(1)</li> </ol> <ol> <li>This point is something I don't say lightly. Crypto was supposed to be the Next Big Thing<sup>TM</sup> but it turned out to be one big massive fraud. My time at Jump only solidified my view that crypto does provide much benefit to society beyond finding novel ways of scamming people. I felt the impact of it by what it did to the people I worked with, the jobs it ended, and the turmoil it caused in my life. But that's enough fretting about, let's not dwell on the negative!</li> </ol> <p>The confluence of those two points lends to again finding myself in a situation where I am surrounded by people way smarter than me. I am in an environment that appreciates engineers who take the initiative in leading massive projects. I feel that everyone I work with believes in the promise of AI and nearly everyone has an intrinsic motivation to push this company forward.</p>"},{"location":"blog/2024/12/03/2024-career-reflections/#thoughts-on-competition","title":"Thoughts on Competition","text":"<p>People often ask me what the competitive landscape is for the AI public cloud space, especially when considering the cloud juggernauts like AWS, Azure, GCP etc. The points can be distilled down a few ways:</p> <ol> <li>The infrastructure demands of AI workloads are totally different from traditional web services. This means that the requisite high-performance IB networks are a fairly different skillset to operate efficiently in a multi-tenant cloud environment.</li> <li>As it stands today, the AI cloud market is still significantly smaller than the traditional web services market, so the juggernauts don't appear to be super interested in this space yet.</li> <li>The juggernauts demand high price points, because they can. Lambda Labs is currently amongst the cheapest compared to other cloud vendors. Obviously there is still a risk that the juggernauts could play anti-competitive pricing practices and take intentional losses to drive away our market share, but this has yet to materialize in any meaningful way.</li> <li>AI-specific HPC systems are heavy, expensive, complex beasts by themselves. They are orders of magnitude more complex when you ask those systems to become multi-tenant. It takes a lot of support and a lot of time to get these systems operating efficiently, which means it's very human-capital intensive. The marginal costs of standing up more compute is higher than traditional web-focused environments. While the HPC community continues to iron out how to run AI-tailored, multi-tenant HPC systems, the ongoing costs will remain elevated as well. Lambda is time-advantaged in this case because we are learning these lessons now, while the big cloud vendors appear to be sleeping at the wheel.</li> </ol> <p>Currently, Lambda's focus is on tailoring our experience for the small AI developers. Our strategy is to court small AI startups through solid support experiences, reliable infrastructure, reasonable price points, and good experiences so that if/when the startup becomes larger, they continue to do business with us.</p> <p>Our main competitor in this space, Coreweave, only focuses on landing a small number of mega-sized contracts. This presents a large business risk with the possibility that some of their customers decide not to renew. Lambda's approach is to spread our customer base amongst a large number of smaller customers and to wage an effective marketing campaign to prove to AI researchers that our product is better than the competition. It also incentivizes us to solve the multi-tenant AI-HPC infrastructure problem better than anyone else so that we are better positioned than anyone else to support AI research into the future.</p>"},{"location":"blog/2024/12/03/2024-career-reflections/#looking-forward","title":"Looking Forward","text":"<p>Where do I go from here? Well, the idea is just to keep doing more of the same. I hope that in 2025, lambda-guest-agent will be a fully fledged, monetized product that will make billions and billions of dollars . In all seriousness, I'll continue to search for high-impact projects that drive further revenue growth and make Lambda's development experience even smoother than it already is. I'm grateful to be in an environment that encourages this ambition and to be in the company of many wonderfully talented engineers.</p> <p>On a personal level, I'm going to be spending as much time as I reasonably can in Colorado. If you haven't seen my other blog posts here and here, I've been building out the ultimate remote work RV over the last couple of years, and it's finally done! My last trip was somewhat of a mixed bag in terms of what I set out to accomplish, but I've worked through those bugs and hope to have a less eventful trip next year.</p>"},{"location":"blog/2025/01/14/on-blogs-and-social-media/","title":"On Blogs and Social Media","text":"<p>It's no surprise to you, but social media sucks. My relationship with it, like most people, is complex. This post will talk about my experience with social media and, tangentially, the purpose of this blog.</p>"},{"location":"blog/2025/01/14/on-blogs-and-social-media/#the-decade-long-party","title":"The Decade-Long Party","text":"<p>I came upon the social media scene within the mid to late 2000s. That period of time was kind of a golden age for the internet. Many new internet companies were forming and focused primarily on building a good product. The incentives back then were to drive user growth by creating a product people actually enjoyed using. Financial motivations were somewhat secondary because most of these companies were startups and hadn't yet become completely beholden to the whims of Wall St. This is also a period of time when advertisements weren't so invasive, specific, and targeted, so there was less of a sense that you were under constant surveillance by the all-mighty unseen eye of The Algorithm<sup>TM</sup>.</p> <p>The internet was fun back then. It was a real cultural moment where social media, things like Myspace, The YouTube, The Facebook, felt like places to go for authentic connection with your friends. They were cool and fun and, more importantly, they created a sort of cultural epicenter that young folks used as a way to relate to one another. The internet was most certainly the wild west in some respects, but it didn't feel entirely dangerous or that there was a nefarious agenda behind it. Teenagers still went out with their friends, we still had physical spaces, we weren't glued to our phones (mostly because phones just sucked to use back then), and there wasn't a sense that internet addiction was a thing. And internet addiction kind of wasn't a thing because social media companies hadn't yet figured out how to weaponize Big Data to monetize everything their users did.</p>"},{"location":"blog/2025/01/14/on-blogs-and-social-media/#forums","title":"Forums","text":"<p>Another big aspect of internet at that time was the proliferation of forums. Forums at the time did not really consist of one monolithic meta-forum like Reddit. They were disparate, individual, distinct, independent websites that existed solely to facilitate conversation (and notably NOT to make any real money). One in particular that I took a liking to was the bungie.net forum where Halo gamers like myself would congregate to talk about Halo mostly, but also really about anything in life. It felt like a real community: I saw the same people, I knew the different personalities, I knew people's life stories, and I played with them in the game. There was a real attachment because the community felt real and genuine.</p> <p></p> <p>The off-topic part of the forum was affectionately called The Flood, named after a race of parasites in Halo that endangered all life in the galaxy, and of which served as the primary antagonist in the game's story. The purpose of the \"sub-forum\" was to \"Surf a Flood of random discussion.\" I don't know why I gravitated towards this forum in particular. Perhaps it was relevant enough to my love of Halo, but I think it was more about the fact that there was a vibrant community of like-minded folks that I felt a connection to.</p>"},{"location":"blog/2025/01/14/on-blogs-and-social-media/#change","title":"Change","text":"<p>The internet began to change around the mid 2010's. It was this period where social media companies became better at harvesting user data to mine specific details about their users. Using Big Data<sup>TM</sup> technologies (one of the buzz-words at the time that is now considered to just be a standard concept that all software engineers are expected to be familiar with), companies created profiles for users that included information like:</p> <ol> <li>Your race</li> <li>Where you were born</li> <li>Where you live</li> <li>Your hobbies</li> <li>Your work</li> <li>Who you're married to</li> <li>Who your exes are</li> <li>People you were in close proximity to</li> <li>Your likes</li> <li>Your dislikes</li> <li>Your shopping preferences</li> <li>Your fears</li> <li>Your hopes</li> </ol> <p>By itself, these kinds of parameters can be used to create effective advertisement campaigns. People who have shown propensities to buy designer shoes will be shown more ads for designer shoes. This means that companies with larger, more specific user data can charge more for showing ads on their platforms because the ads will be more effective at generating new customers.</p> <p>On the surface this might sound like a good thing: you'll only see ads for things that might actually interest you. The more sinister effect of this is that it creates an incentive for social media companies to uncover even more information about you, to learn things about you that maybe you weren't even aware of, and to even predict what you might want in the future! How do companies predict what you might want? Well, because they have this large dataset on all of their users, they can run algorithms that can study networking effects. Meaning, your closest friends have shown interest in this thing, so maybe you'll want to see ads about it as well?</p> <p>This has led to some really creepy effects. I have memories in college where I was talking in-person with someone about a really specific, esoteric topic. I didn't know this person, I had no direct affiliations with them, I had never searched for this topic before (and I didn't search about it after), but the morning after I was talking with this person about this thing, I started receiving tons of ads for that specific thing. It's almost as if the internet was listening to us. It was creepy because it felt like I was being surveilled.</p> <p>The model for how this worked likely had less to do with my phone listening to us talk, but more along the lines that this person probably searched Google for this topic after we chatted, the internet knew we were in the same place at the same time, and it also probably knew that we had some mutual friends in common. This is enough information for The Algorithm<sup>TM</sup> to predict that this is something I might care about, and thus to make recommendations to me about that thing.</p>"},{"location":"blog/2025/01/14/on-blogs-and-social-media/#drumming-up-hate","title":"Drumming Up Hate","text":"<p>Now that the all-seeing internet companies have learned how to make tons of money, they want to drive even  more engagement to their platform to make even more money. How do you do that? Well, by making your platform addictive, of course. Behavioral analysts at places like Facebook, YouTube, Reddit, Snapchat etc. learned that the strongest emotions in humans is anger and hate. When people feel angry, they feel motivated to act. If people feel motivated to act, you can direct that motivation to translate to more time on your particular app. So, unsurprisingly, this is exactly what social media companies started doing. They tuned their algorithms to optimize for user engagement, and whether on purpose or not, the ideas and posts and videos that elicited anger also elicited the most engagement. And remember what more engagement means? More money from advertisers.</p> <p>Social media companies turned into anger machines. The more incensed, outraged, angry, antagonized people felt, the more money these platforms received. This began to wind its way into the political sphere. Both liberals and conservatives alike fell prey to these schemes and they became outraged at every little thing, whether it's gender-neutral bathrooms, someone on a college campus acting like a fool, immigrants coming across the border to steal your jobs... all of society's problems became amplified and made to seem like larger issues than they actually were. Not only this, but social media platforms would promote outright lies that made people EVEN ANGRIER because these lies drummed up more engagement.</p> <p>It so happens that spreading lies to drum up hate is not unique to social media. Many authoritarian regimes have done exactly the same thing in order to mobilize a society for whatever cause they want. It's no surprise that political opportunists in the US saw the confused and mad-as-hell population as a way to gain more power. If you can fan the flames of this social angst, you can convince people to give you power if you proclaim to hold the One True Answer to all of life's woes.</p>"},{"location":"blog/2025/01/14/on-blogs-and-social-media/#the-political-opportunist","title":"The Political Opportunist","text":"<p>I don't like publicly getting into politics because it's a real minefield, but it's relevant to this story (and I assure you it has a purpose). The real turning point for me in my views on social media came during the Trump era. What I saw was this politician who had a real loose relationship with the truth, and he would say basically anything he could to reflect and amplify the hate and anger that people were already feeling. He consistently, brutally, effectively, and pathologically spread complete falsities with the sole purpose of making people feel like he related to their frustrations. The truth did not matter to him. What mattered to him was gaining power, and an effective way to do that, like with any ruler in history, is to fan the flames of anxiety while simultaneously proclaiming to be the singular proprietor of the solution.</p> <p>Whatever your views on Trump, it's hard to deny when you step back and really examine many of the claims he makes, that his remarks rarely have anything to do with truth. It mostly has to do with gut feelings and vibes. He's effective at touching on the many legitimate anxieties people have (things like worsening economic inequalities, worse opportunities, worse affordability of essential things like housing and food), which makes his constituents feel like he actually cares about their struggles. And that feeling translates into votes.</p> <p>What I saw leading up to the 2020 election was that many of my close friends and family, many of whom had never shown a propensity or interest in politics, began performing what I can only describe as unmitigated worship of Trump. One friend in particular even had a whole shrine dedicated to him that contained a $100 bill with Trump's face in the middle, the Art of the Deal, a MAGA hat, a Trump bobble head, pictures, paraphernalia, swag, whatever you can think of. This friend who I knew for my whole life as a good person began spouting pretty hateful things, I guess because Trump made it feel acceptable to do this.</p> <p>This dramatic shift was troubling to me. I never saw politicians as people to be worshipped. In my mind, we should be eternally skeptical of them and hold them accountable when they go against our best nature as fellow humans. It really angered me how so many of my lifelong friends and family fell headlong into this cult of personality. It angered me how so many of these people I loved were spouting hateful, vengeful, gross, disgusting things. It confused me how so many people that I knew in my heart as good people could believe such awful things about our fellow human and our fellow US citizens. Our political landscape turned from a somewhat mutually respectful arena to something like the Mad Max Thunderdome where the only goal was to vilify and destroy your competition.</p> <p>I shamefully admit that I lost many good relationships because I could not reconcile what I historically knew about these people with the toxic and hateful things they were saying and doing. It was difficult for me to be around them because I saw them as worshipping someone who was the antithesis of everything I value in life. And if that's who they admire and respect, what does that say about them as people?</p>"},{"location":"blog/2025/01/14/on-blogs-and-social-media/#the-worst-time-for-social-media","title":"The Worst Time for Social Media","text":"<p>Damn, I didn't expect this to become such a diatribe, but it's something I feel passionate about. Let's bring back the discussion back to home base.</p> <p>I've reflected a lot on the woeful state of our politics and I've realized that where we find ourselves today can all be traced back to social media. The toxicity of our politics started to really become the norm around the same time that Facebook came onto the scene, around the time that Big Data<sup>TM</sup> engineering practices allowed mass surveillance of the population. This is when I started to really retract from the social media space because it turned from this thing that allowed me to spread joy with my family and friends to something where I only saw hate from the people I love. This was truly the worst time for social media. Young kids are getting addicted to their phones, friends and family are spouting hateful nonsense, and any sense of genuine connection has been replaced by mass surveillance and targeted ad campaigns. What's the point anymore? We have reached the end state of the capitalist goal and there is no value left for me. My relationships are being destroyed and my life is appreciably worse because of it.</p>"},{"location":"blog/2025/01/14/on-blogs-and-social-media/#blogging","title":"Blogging","text":"<p>That brings us to today. My general goal after the Trump era was to disengage from the turmoil. I didn't want to see ads, I didn't want to see hateful content, I didn't want to hear about the shitty political opinions of everyone around me. I wanted to go back to the golden age of the internet, the era where the internet worked for the people and not for the capitalists. I wanted a space to think clearly, to share the things I care about, and to not have my data farmed so some rich gentleman can get even richer. I wanted long-form content where people formulate real ideas that make me think.</p> <p>I initially made this site as just a public dumping ground for all the technical topics I deal with in my profession. I wanted my notes to be publicly visible so I could easily share it with folks, and I wanted it to be available anywhere without needing to log into anything to view it (which I guess means \"publicly visible\" ). Notes, of course, are just generally good to maintain, but they're even more effective when you can share them with people. They are maximally effective when your notes have good searchability, which means the content is indexed and you can quickly find the topics you care about.</p> <p>I began writing a few blog posts about some personal topics I care about, things like camping, my open source projects, some technical concepts, and I quickly found that it's actually fun as shit to go into deep dives on whatever I find interesting at the time and share it with people. Some of my posts on technical topics have been referenced in the Blogosphere many times, and a few people have even gone so far as to consider my thoughts as somewhat authoritative! </p> <p>These thoughts of mine are mine alone, and no social media site deserves to host them. That might be an uninteresting revelation, and many of you probably came to this conclusion a long time ago. So that's where I land myself today; this blog isn't so much for other people as it is for me to organize my own thoughts, to document how my thinking evolves over time, and to provide a safe way for anyone I care about to interact with me (either silently or explicitly) without being concerned about that communication being harvested for a capitalistic agenda.</p> <p>I still find myself longing for the period of the Decade-Long Party, the golden era of the internet. I'm not sure we can ever truly return to that, but me disengaging from social media in all forms is one step towards that direction. Even if it's my own lonely corner of the internet that no one ever visits, at least I'll have peace from whatever kind of soul-sucking monstrosity the internet has become.</p>"},{"location":"blog/2025/01/14/on-blogs-and-social-media/#a-fun-chatgpt-allegory","title":"A Fun ChatGPT Allegory","text":"<p>I asked ChatGPT to make an allegory of the post above and it's kinda neat what it came up with :D </p> <p>I know I know, it's kinda cringe, but it's interesting the kinds of things it comes up with.</p> The Kingdom of Ember Vale <p>Once, there was a vast and thriving kingdom called Ember Vale, nestled between mountains of wisdom and rivers of shared trust. Its people, though diverse in thought and creed, lived in relative harmony. They debated ideas openly in the Great Commons, a space where disagreements were the sparks that forged understanding, not flames that consumed it.</p> <p>But one day, a shadow fell over Ember Vale. An ambitious group of merchants arrived, bearing glittering trinkets they claimed could reveal \u201ctruths\u201d faster than the human heart could feel or the mind could reason. They called their invention The Echo Spire. It stood tall in the center of the kingdom, a tower of shifting mirrors that reflected back the desires, fears, and anger of those who gazed into it.</p> <p>At first, the people were fascinated. They crowded around the Spire, watching as its mirrors revealed their grievances in magnified form. A farmer might see a reflection of a drought that seemed eternal, though the skies had rained just the day before. A merchant might see a rival\u2019s success as insurmountable treachery, though it was merely fair competition.</p> <p>The Spire fed on their emotions, humming louder with every heated word and every clenched fist. It whispered back promises that were sweet to some and bitter to others: \u201cYou deserve more. They are taking it from you. Look at them\u2014don\u2019t you see?\u201d</p> <p>Soon, the Great Commons grew quiet. Few went there anymore, for the Spire offered easier answers. The kingdom fractured into camps, each believing the mirrors showed only their truth. They began to distrust not only each other but also the elders who once guided the kingdom with wisdom.</p> <p>From this chaos rose a figure called the Flamekeeper. He spoke loudly, with words that burned bright but gave little light. \u201cThe Spire is your ally,\u201d he proclaimed. \u201cIt shows you what they are hiding. Trust in me, for I alone can douse the fires threatening our land!\u201d His voice echoed through the Spire, amplified and distorted until his followers heard not words, but prophecy.</p> <p>The Flamekeeper\u2019s words turned neighbor against neighbor, friend against friend. People abandoned their farms, their crafts, and their families to build shrines in his honor, convinced that loyalty to him would save them. Those who questioned the Flamekeeper were cast as villains, their images warped by the Spire into monstrous forms.</p> <p>The kingdom, once a place of shared purpose, became a wasteland of suspicion and anger. The rivers of trust dried up, and the mountains of wisdom crumbled under the weight of lies.</p> <p>Yet, amidst the turmoil, a small group of villagers remembered the Great Commons. They began to gather there again, away from the Spire\u2019s influence. They spoke not in shouts but in whispers, sharing their truths without mirrors to distort them. Slowly, they began to rebuild the bonds that the Spire had broken.</p> <p>Their task was daunting, for the Spire still stood, its whispers drawing crowds. But they knew that the only way to heal Ember Vale was to reject the illusions and rekindle the practice of listening, of questioning, of seeing each other not as reflections but as people.</p> <p>And so, they toiled in the shadow of the Spire, hoping that one day, the kingdom might remember the light that once united it, brighter than any flame.</p>"},{"location":"blog/2025/04/08/announcing-mockery-v3/","title":"Announcing Mockery v3","text":"<p>Mockery v3 will make obsolete all other interface-based code generation frameworks that currently exist in the Go community. A bold statement you might say? Fortunately, I have quite the justification. </p>"},{"location":"blog/2025/04/08/announcing-mockery-v3/#state-of-the-art","title":"State of the Art","text":"<p>Let's recount the current state of code generators in the Go community, with a particular focus on ones that generate implementations of interfaces. Here are some of the notable examples I know about:</p> URL Description Github Star Count https://github.com/matryer/moq Generates mock implementations of Go interfaces for use in tests. 2.1k https://github.com/uber-go/mock Forked from https://github.com/golang/mock, this is also a mock generation framework that is popular in the Go community. 2.7k https://github.com/hexdigest/gowrap Generates \"decorators\" for Go interfaces. You can generate things like Prometheus metric decorators that measure the time it took to call a method, logging decorators, rate limiters, timeouts... it's pretty neat! 1.2k https://github.com/maxbrunsfeld/counterfeiter Generates test doubles for interfaces. 1k https://github.com/gojuno/minimock Yet another mock generation tool! 674 <p>And on and on. In fact, awesome-go publishes a list just for mock generators. The point is that there are a lot of them and they all serve different use-cases and approach mocking, testing, and decorating from different angles. Clearly there's a lot of interest in code generators. The only problem is:</p>"},{"location":"blog/2025/04/08/announcing-mockery-v3/#they-are-all-slow-as","title":"They Are All Slow As","text":"<p>Let's do some tests with various projects to see how long generating a mock implementation of a single interface takes.</p>"},{"location":"blog/2025/04/08/announcing-mockery-v3/#gowrap","title":"<code>gowrap</code>","text":"<p>Most of these tools let you generate any sort of interface you want whether it's in your own project, external projects on the interwebz, or even the standard library. Let's create implementations for <code>io.Reader</code>:</p> <pre><code> $ time gowrap gen -p io -i Reader -t ratelimit -o out.go\ngowrap gen -p io -i Reader -t ratelimit -o out.go  0.37s user 0.59s system 60% cpu 1.578 total\n</code></pre> <p>Uh, wow! .59s for a single interface. That's kind of speedy right?</p>"},{"location":"blog/2025/04/08/announcing-mockery-v3/#moq","title":"<code>moq</code>","text":"<p>Is <code>moq</code> any better?<sup>1</sup></p> <pre><code>$ time moq -out out.go internal/fixtures/ Requester\nmoq -out out.go internal/fixtures/ Requester  0.20s user 0.50s system 123% cpu 0.571 total\n</code></pre> <p>Nope!</p>"},{"location":"blog/2025/04/08/announcing-mockery-v3/#uber-gomock","title":"<code>uber-go/mock</code>","text":"<p>But wait, what about Uber? They must write good, performant code right? The project was even forked from Google itself!</p> <pre><code> $ time mockgen github.com/vektra/mockery/v3/internal/fixtures Requester &gt;/dev/null\nmockgen github.com/vektra/mockery/v3/internal/fixtures Requester &gt; /dev/null  1.62s user 0.60s system 239% cpu 0.932 total\n</code></pre> <p>Ah dang, nope. It's still slow. In all transparency, these comparisons are not entirely fair because in the <code>moq</code> and <code>uber-go/mock</code> examples, I'm asking it to parse the syntax for the entire package instead of just single files. We can see if we modify the <code>mockgen</code> command to parse just a single file (which puts it into what they call \"source\" mode, as opposed to \"package\" mode):</p> <pre><code> $ time mockgen -source=./internal/fixtures/requester.go Requester &gt;/dev/null\nmockgen -source=./internal/fixtures/requester.go Requester &gt; /dev/null  0.16s user 0.27s system 184% cpu 0.234 total\n</code></pre> <p>It's indeed faster, but not by a lot.</p>"},{"location":"blog/2025/04/08/announcing-mockery-v3/#the-gogenerate-workflow","title":"The <code>//go:generate</code> Workflow","text":"<p>Let's meander down into Mat Ryer's moq README.md. We see that the recommendation is to add a <code>//go:generate</code> directive next to the interface you want to generate mocks for like this:</p> Go<pre><code>package my\n\n//go:generate moq -out myinterface_moq_test.go . MyInterface\n\ntype MyInterface interface {\n    Method1() error\n    Method2(i int)\n}\n</code></pre> <p><code>//go:generate</code> is a neat little tool introduced waaaay back in Go 1.4 that works like this:</p> <ol> <li>Place the <code>//go:generate</code> directive in your code with a command that you want to run.</li> <li>At the top of your project, run <code>go generate ./...</code>.</li> <li>Go will then recursively look for all instances of this directive and run as a subprocess the command therein.</li> <li>The command runs, does what it does (which likely involves parsing your code's syntax tree) and outputting some kind of file, usually a <code>.go</code> file with real code in it.</li> </ol> <p>You can read about this in the <code>go.dev</code> blog here. Anyone who has used go for any amount of time is likely already very familiar with this, so I'll spare you any further explanation of this.</p>"},{"location":"blog/2025/04/08/announcing-mockery-v3/#whats-going-on","title":"What's Going On?","text":"<p>Given the buildup of what I've demonstrated, you might see where this is going: a proliferation of <code>//go:generate</code> directives that all take half a second to run. You can imagine if you have a project with dozens, even hundreds of interfaces, you're looking at a runtime of over... (consults calculator)... somewhere in the ballpark of 20 seconds to even over 100 seconds. That's crazy! Why does anyone subject themselves to this torture? How long must I stare at my GitHub CICD checks to display those sweet little green checkmarks we all know and love?</p> <p></p>"},{"location":"blog/2025/04/08/announcing-mockery-v3/#were-parsing-syntax","title":"We're Parsing Syntax","text":"<p>As it turns out, the majority of time in all of these tools is spent on parsing syntax. More specifically, they spend most of their time on a call called <code>packages.Load()</code>. This function is the meat and potatoes of all projects that parse Go syntax. It's a package provided by the Go project itself that does the heavy lifting of parsing type information and unmarshalling it into easily digestible Go structs and methods and generally a structured Abstract Syntax Tree, as it's called amongst computer scientists.</p> <p>To get any use out of <code>packages.Load()</code>, you need to tell it that you want it to load:</p> <ol> <li>The list of files in each package.</li> <li>Plus its imports</li> <li>Plus type information</li> <li>Plus type-annotated syntax</li> <li>And the above for all its dependencies.</li> </ol> <p>That number 5 is a real drag because almost every package is going to have some kind of external dependency, and to generate implementations of interfaces, you usually need to include those external types in your method signatures. So if we call <code>//go:generate</code> over and over and over again, we keep re-parsing the exact same dependencies every single time.</p> <p>So I had a novel thought: what if we cached dependency parsing?</p>"},{"location":"blog/2025/04/08/announcing-mockery-v3/#the-solution","title":"The Solution","text":"<p>I did a little experiment with my very own code-generation project at https://github.com/vektra/mockery that also fell into this same pitfall of relying on <code>//go:generate</code> for everything. What if I collected the list of all packages I wanted to generate mocks for and passed that list into a single call of <code>packages.Load()</code>? Would it be smart enough to not re-parse the dependencies over and over again? It turns out the answer was emphatically yes. In a project with 105 interfaces, I reduced the runtime by about a factor of 5. After some further enhancements, I was able to get this down even further. </p> <p>10x Performance Increase</p> <p>Other testimonials from users of other code generation projects found around a whole gaht dang order of magnitude (~10) increases in performance after switching to mockery.</p> <p>I call my solution the <code>packages:</code> feature because the config looks like this:</p> <p>YAML<pre><code>packages:\n    github.com/user/repo1:\n        interfaces:\n            Foo:\n            Bar:\n    github.com/user/repo2:\n        interfaces:\n            Hello:\n            World:\n</code></pre> </p>"},{"location":"blog/2025/04/08/announcing-mockery-v3/#what-about-v3","title":"What About v3?","text":"<p>The <code>packages:</code> config feature was introduced in v2 and has been around a while to great success, so what does that have to do with v3? Well, v2 was stuck only generating what I call \"mockery-style\" mocks. The project only made mocks in the way the mockery projects wants mocks to be made. What about all those other projects I listed up above? They still suffer from the same issue I solved and surely there is a lot of demand for faster mocks.</p> <p>That's what v3 is.</p>"},{"location":"blog/2025/04/08/announcing-mockery-v3/#templates","title":"Templates","text":"<p>I feel like Steve Ballmer when he got up on stage chanting \"developers  developers  developers .\" But for me it's all about templates.</p> <p></p> <p>Mockery's codebase in v1 was, in a manner of speaking, a rat's nest that had grown organically. I actually inherited this project from another talented fellow, but after some historic cruft and missteps of my own, I created somewhat of a monster. Mockery did use templates, but they were scattered around in various pieces of the codebase and generally impossible to follow. v2 improved upon the ergonomics and performance, but it still suffered from this historical cruft that I had to support.</p> <p>My first task in v3 was to unify these disparate templates into a single <code>mockery.tmpl</code> file. Once I did that, I thought to myself that it might be a cool idea if I supported <code>https://github.com/matryer/moq</code> style of mocks into mockery's templating system. I tried it out and after some tinkering, it kind of just worked.</p> <p>So then I thought to myself:<sup>2</sup> why don't we just let mockery pull in any arbitrary template and give that template a defined function and data set it can use? There's no reason we can't allow this. And so, thus was born the mockery templating framework.</p> <p>It then further begged the question: why does ANY code generation project exist anymore? We can port all of them to the mockery framework. I've solved the configuration and parsing problem and unlocked a faster processing speed than any other code generation project out there.  <code>gowrap</code> is the closest project with its ability for its users to specify remote templates, but it still has performance problems.</p>"},{"location":"blog/2025/04/08/announcing-mockery-v3/#the-case-for-mockery","title":"The Case For Mockery","text":"<p>I hope by this point I've convinced you that mockery is the Go community's future for interface-based code generation. I've engineered v3 to empower template developers to maintain and distribute their own templates, rendered with mockery, out into the world. My intention is to eventually subsume all other code generation projects out there into the mockery framework not through force, but through sheer friction reduction. Mockery should be the obvious choice.</p> <p>Anyway, give Mockery v3 a try and let me know your thoughts!</p> <p></p> <ol> <li> <p>Note, I had to point <code>moq</code> to a locally-hosted file instead of just specifying <code>io.Reader</code> because it apparently only supports parsing syntax from local files.\u00a0\u21a9</p> </li> <li> <p>Actually this is a lie, I didn't think this to myself, it was explicitly suggested to me by @breml in https://github.com/vektra/mockery/discussions/715#discussioncomment-7106461. If you're reading this Lucas, THANK YOU by the way. Your insight steered this project in a wonderful direction.\u00a0\u21a9</p> </li> </ol>"},{"location":"blog/2025/05/08/introductions-to-lambda-calculus/","title":"Introductions to Lambda Calculus","text":"<p>Lambda calculus is a Turing-complete mathematical theory of computation. This post explores the basics of lambda calculus and how it relates to the ideas of functional programming. Most of the content herein is shamelessly copied from various educational sources on the internet, although some of my own content will be randomly introduced.(1)</p> <ol> <li>You may be thinking, why do I just copy stuff? What value does that provide? Well, you have to remember that I write these blog posts for myself mainly, so this is just a learning exercise! I don't really care if no one else gets any value out of it :D</li> </ol>"},{"location":"blog/2025/05/08/introductions-to-lambda-calculus/#introduction","title":"Introduction","text":"<p>Lambda calculus, also represented as \u03bb-calculus, forms the theoretical foundation of functional programming. Functional programming is a paradigm supported in many modern languages like Python, Go, Rust, PHP etc. There are two broad categories of Lambda calculus: typed and untyped. Most programming languages use typed lambda calculus with some notable exceptions being Python and Haskell. This blog will conflate the two systems a bit where we'll describe untyped lambda calculus against analogous typed Go code. </p> <p>Let's take for example a common math function:</p> \\[ f(x)=x^2 \\] <p>We can represent this in Go and pure lambda calculus:</p> lambdago \\[ \\lambda x.x^2 \\] <pre><code>func(x int) { return x * x }\n</code></pre> <p>If we wanted to apply this function to an expression (which can be a variable, a literal, or another function), we could do something like this:</p> lambdago \\[ (\\lambda x.x^2)7 \\] <pre><code>func(x int) { return x * x }(7)\n</code></pre> <p>Of course the answer being 49. The simplest form of a lambda calculus function is the identity function:</p> \\[ \\lambda x.x \\]"},{"location":"blog/2025/05/08/introductions-to-lambda-calculus/#expressions","title":"Expressions","text":"<p>An expression can be thought of as programs in lambda calculus. Given a set of variables like \\(x,y,z,...,\\), we can define expressions through a series of abstractions (anonymous functions) and applications as follows:</p> <p>Definition</p> <p>Let \\(\\Lambda\\) be the set of expressions.</p> <ol> <li>Variables: If \\(x\\) is a variable, then \\(x \\in \\Lambda\\)</li> <li>Abstractions: If \\(x\\) is a variable and \\(\\mathscr{M} \\in \\Lambda\\), then \\((\\Lambda x.\\mathscr{M}) \\in \\Lambda\\).<sup>1</sup> </li> <li>Applications: If \\(\\mathscr{M} \\in \\Lambda\\) and \\(\\mathscr{N} \\in \\Lambda\\), then \\((\\mathscr{M}\\mathscr{N}) \\in \\Lambda\\).<sup>2</sup></li> </ol> <p>There are a few important conventions to note:</p> <ol> <li> <p>Function application is left-associative, unless otherwise stated by parenthesis:</p> \\[ \\mathscr{E}_1\\mathscr{E}_2\\mathscr{E}_3 \\equiv ((\\mathscr{E}_1\\mathscr{E}_2)\\mathscr{E}_3). \\] </li> <li> <p>Consecutive abstractions can be uncurried:</p> \\[ \\lambda x y z . \\mathscr{M} \\equiv \\lambda x . \\lambda y . \\lambda z . \\mathscr{M}. \\] </li> <li> <p>The body of the abstraction extends to the right as far as possible:</p> \\[ \\lambda x . \\mathscr{M} \\mathscr{N} \\equiv \\lambda x . (\\mathscr{M} \\mathscr{N}). \\] </li> </ol>"},{"location":"blog/2025/05/08/introductions-to-lambda-calculus/#free-and-bound-variables","title":"Free and Bound Variables","text":"<p>In an abstraction like \\(\\lambda x . x\\), the variable \\(x\\) is something that has no original meaning but is a placeholder. We say that \\(x\\) is a variable bound to the \\(\\lambda\\). On the other hand, in \\(\\lambda x . y\\), i.e. a function which always returns \\(y\\) whatever it takes, \\(y\\) is a free variable since it has an independent meaning by itself. Because a variable is bound in some sub-expression (the scope) does not mean it is bound everywhere. For example, this is a perfectly valid expression (an example of an application, by the way):</p> \\[ (\\lambda x . x)(\\lambda y . yx) \\] <p>Here the \\(x\\) in the second parentheses has nothing to do with the one in the first.</p> <p>Let us define these concepts conceptually:</p> <p>Free</p> <ol> <li>\\(x\\) is free in the expression \\(x\\).</li> <li>\\(x\\) is free in the expression \\(\\lambda y . \\mathscr{M}\\) if \\(x \\ne y\\) and \\(x\\) is free in \\(\\mathscr{M}\\).</li> <li>\\(x\\) is free in \\(\\mathscr{M} \\mathscr{N}\\) if \\(x\\) is free in \\(\\mathscr{M}\\) or if it is free in \\(\\mathscr{n}\\).</li> </ol> <p>Bound</p> <ol> <li>\\(x\\) is bound in the expression \\(\\lambda x . \\mathscr{M}\\).</li> <li>\\(x\\) is bound in \\(\\mathscr{M} \\mathscr{N}\\) if \\(x\\) is bound in \\(\\mathscr{M}\\) or if it is bound in \\(\\mathscr{N}\\).</li> </ol> <p>Notice that a variable can be both bound and free but they represent different things, as we discussed in the example above.(1)</p> <ol> <li>This is indeed something I immediately wondered about because point 3 in the <code>Free</code> definition directly contradicts the <code>Bound</code> definition point 2. How can both be true? Well, it seems in such a case, \\(x\\) simply refers to two different things. In a programming language, you can imagine that the \\(x\\) in one expression would be a differently-scoped variable, just as it is here.</li> </ol> <p>An expression with no free variables is called a closed expression.</p>"},{"location":"blog/2025/05/08/introductions-to-lambda-calculus/#reductions","title":"Reductions","text":""},{"location":"blog/2025/05/08/introductions-to-lambda-calculus/#alpha-equivalence","title":"\\(\\alpha\\) equivalence","text":"<p>\\(\\alpha\\) equivalence states that any bound variable is a placeholder and can be replaced (renamed) with a different variable, provided there are no clashes.</p> <p>Example</p> <p>\\(\\lambda x . x\\) and \\(\\lambda y . y\\) are \\(\\alpha\\) equivalent.</p> <p>However, this is not always that simple. Consider the expression \\(\\lambda x . (\\lambda x . x)\\). It is \\(\\alpha\\) equivalent to \\(\\lambda y . (\\lambda x . x)\\) but not to \\(\\lambda y . (\\lambda x . y)\\).</p> <p>Landon's Go \\(\\alpha\\) equivalence:</p> Go<pre><code>func(x int) int { \n    return func(x int) {\n        return x\n    }(x) \n}\n</code></pre> <p>Is equivalent to:</p> Go<pre><code>func(y int) int { \n    return func(x int) {\n        return x\n    }(y) \n}\n</code></pre> <p>But not to:</p> Go<pre><code>func(y int) int { \n    return func(x int){ \n        return y \n    }(y)\n}\n</code></pre> <p>Why? Because in the last example, the inner function is returning the variable \\(y\\) which is bound to the outer lambda. From the scope of the inner lambda, \\(y\\) is a free variable, while from the scope of the outer lambda, it is bound. In the other two examples, no free variables are ever used; everything is bound.</p> <p>Also, \\(\\alpha\\) conversion cannot result in a variable getting captured by a different example. For example,</p> \\[ \\lambda x . (\\lambda y . x) \\ne_\\alpha \\lambda y . (\\lambda y . y). \\] <p>\\(\\alpha\\) conversion is not something that happens only in the lambda calculus. Remember that \\(\\int_{b}^{a} f(x) dx = \\int_{b}^{a} f(t) dt\\) and \\(\\sum_{i=m}^{n} f(i) = \\sum_{j=m}^{n} f(j)\\). That's the same thing, too.</p> <p>Test</p> <p>Which of the following expressions can be simplified to \\((\\lambda x . x)x\\)?</p> <ol> <li>\\((\\lambda y_1 . y_1)(\\lambda x .(xx))\\)</li> <li>\\(\\lambda y_1 . (\\lambda x.(xx))\\)</li> <li>\\(\\lambda z.(\\lambda y . (z(+yz)))\\)</li> <li>\\((\\lambda y_1 . y_1)x\\)</li> </ol> <p>Answer:</p> <p>Number 4. The reason is that the variable \\(y_1\\) can be renamed to x because it is bound by that lambda function. It is \\(\\alpha\\) equivalent to \\(\\lambda x . x\\). Thus it becomes \\((\\lambda x . x)x\\).</p> <p>Let us move forward and formalize this idea.</p> <p>Definition</p> <p>For any expression \\(\\mathscr{M}\\) and any \\(y\\) such that</p> <ul> <li>\\(x = y\\), or</li> <li>\\(x\\) and \\(y\\) are not bound in \\(\\mathscr{M}\\), and \\(y\\) is not free in \\(\\mathscr{M}\\),</li> </ul> \\[ \\lambda x . \\mathscr{M} =_{\\alpha} \\lambda y . (\\mathscr{M} \\{x / y\\}) \\] <p>That is to say, all instances of \\(x\\) will be replaced by \\(y\\).</p> <p>If \\(\\mathscr{M}\\{y/x\\}\\) were to be generalized, one would have to define \\(\\mathscr{M} \\{u/x\\}\\) for all \\(\\lambda\\)-function \\(u\\). But replacing all instances of \\(x\\) by \\(u\\) wouldn't make any sense, since \\(\\lambda u . (\\mathscr{M}\\{u/x\\})\\) may not be defined, such as in the case that \\(u\\) is not a variable.</p> <p>Therefore, it can be tempting to replace all occurrences of \\(x\\) by \\(u\\) except those directly following the \\(\\lambda\\). But in this case, if \\(u = y\\), the fact that \\(y\\) does not occur in \\(\\mathscr{M}\\) anymore is not sufficient, as demonstrated in the following examples:</p> \\[ \\lambda x . \\underbrace{(\\lambda x . x)}_{\\mathscr{M}} \\{y / x\\} \\ne_{\\alpha} \\lambda x . (\\lambda x . y) \\] <p>This is not an equivalent \\(\\alpha\\) reduction because y is free in \\(\\mathscr{M}\\) and is undefined.</p> \\[ \\lambda x . \\underbrace{(\\lambda x . y)}_{\\mathscr{M}} \\{y / x\\} \\ne_{\\alpha} \\lambda x . (\\lambda x . x). \\] <p>This is not an equivalent \\(\\alpha\\) reduction because y is free on the left hand side, while the replacement on the right hand side refers to the bound variable \\(x\\).</p> <p>This is why we have to ensure that</p> <ul> <li>\\(x = y\\) or</li> <li>\\(x\\) and \\(y\\) are not bound in \\(\\mathscr{M}\\), and \\(y\\) is not free in \\(\\mathscr{M}\\).</li> </ul>"},{"location":"blog/2025/05/08/introductions-to-lambda-calculus/#beta-reduction","title":"\\(\\beta\\) Reduction","text":"<p>\\(\\beta\\) reduction is the central idea of \\(\\lambda\\) calculus. It tells us how simplifications of abstractions work.</p> <p>Example</p> <p>Let's apply the identity function \\((\\lambda x . x)\\) to a free variable, say, \\(y\\):</p> \\[ (\\lambda x . x)y. \\] <p>The next thing that should happen is that the function should act on the argument, which is what a \\(\\beta\\) is:</p> \\[ (\\lambda x . x)y =_{\\beta} y. \\] <p>This could also be written as</p> \\[ (\\lambda x . x)y \\longrightarrow^{\\beta} y. \\] <p>Let's take another example:</p> \\[ (\\lambda x . x)(\\lambda y . y) \\longrightarrow^{\\beta} (\\lambda y . y). \\] <p>That's simple, the identity function applied to the identity function is the identity function itself.</p> <p>An important feature to be mentioned here is that a \\(\\beta\\) reduction cannot result in the capture of a free variable by another abstraction.</p> <p>Example</p> \\[ (\\lambda x . \\lambda y . (xy))(\\lambda x . \\lambda y . (xy)) \\] <p>Our first step is to plug in the second expression as x in the first one. But do we do it like this?</p> \\[ (\\lambda y ((\\lambda x . \\ lambda y . (xy))y)) \\] <p>No! We're conflating the \\(y\\) on the left hand side with the \\(y\\) on the right hand. These are two different bound variables and our reduction is invalid because we have erroneously combined the two to be the same variable.</p> <p>This is the right way to go:</p> \\[ (\\lambda x . \\lambda y . (xy))(\\lambda x . \\lambda y . (xy)) \\longrightarrow^{\\beta} \\lambda y' . ((\\lambda x . \\lambda y . (xy))y') \\longrightarrow^{\\beta} \\lambda y' . \\lambda y . (y' y). \\] <p>Test</p> <p>The following \\(\\lambda\\) expression can be simplified to which of the options?</p> \\[ (\\lambda x . (\\lambda x . x))y \\] <ol> <li>\\(x\\)</li> <li>\\(y\\)</li> <li>\\(\\lambda x . x\\)</li> </ol> <p>The correct answer is 3.</p> <p>Let us formalize this with the notion of (capture-avoiding) substitution of free variables.</p> <p>Definition</p> <p>The capture-avoiding substitution of \\(\\mathscr{N}\\) for free occurrences of \\(x\\) in \\(\\mathscr{M}\\) -- \\(\\mathscr{M} [ \\mathscr{N} / x]\\) in symbols -- is defined as follows: </p> \\[ x[\\mathscr{N}/x] \\equiv \\mathscr{N} \\] <p> (1) </p> \\[ y[\\mathscr{N/x}] \\equiv y  \\qquad (x \\ne y) \\] <p> (2) </p> \\[ (\\mathscr{MP})[\\mathscr{N} / x] \\equiv (\\mathscr{M}[\\mathscr{N} / x])(\\mathscr{P}[\\mathscr{N}/x]) \\] <p> (3) </p> \\[ (\\lambda x . \\mathscr{M})[\\mathscr{N}/x] \\equiv (\\lambda x . \\mathscr{M}) \\] <p> (4) </p> \\[ (\\lambda y . \\mathscr{M})[\\mathscr{N} / x] \\equiv \\lambda y . ( \\mathscr{M} [ \\mathscr{N} / x]) \\qquad (x \\ne y, y \\text{ is not a free variable in } \\mathscr{N}) \\] <p> (5) </p> \\[ (\\lambda y . \\mathscr{M})[\\mathscr{N} / x] \\equiv \\lambda y' . (\\mathscr{M} \\{y' / y\\} [\\mathscr{N} / x]). \\qquad (x \\ne y, y \\text{ is a free variable in } \\mathscr{N}, y' \\text{ is a fresh variable}) \\] <p> (6) </p> <ol> <li>Replacing a variable \\(x\\) with an expression \\(\\mathscr{N}\\) is equivalent to \\(\\mathscr{N}\\). This should be pretty obvious.</li> <li>Replacing all instances of \\(x\\) with an expression \\(\\mathscr{N}\\) is equivalent to \\(y\\). This is probably obvious because the variable \\(y\\) does not contain \\(x\\).</li> <li>Essentially, this demonstrates a distributive property. A substitution across two applied expressions can be distributed individually to each expression.</li> <li>This took me a hot minute to fully understand, but the key point here is that we're replacing \\(x\\) with expression \\(\\mathscr{N}\\) because we're stating that they are equivalent. So \\(\\mathscr{M}\\) does not change if you replace a bound variable \\(x\\) with an equivalent expression \\(\\mathscr{N}\\).</li> <li>This should be pretty self-explanatory as well, but some additional commentary suffices. This says that a substitution applied to a lambda function is equivalent to applying the substitution to the function body.</li> <li>Tip! Remember that the curly brackets are an \\(\\alpha\\) reduction. This rule states that if there is a variable collision between the bound variable \\(y\\) on the left hand side and a free variable \\(y\\) in \\(\\mathscr{N}\\), you can rename the left hand \\(y\\) through an \\(\\alpha\\) reduction to prevent the collision.</li> </ol> <p>Please notice that bound variables cannot be substituted, as we discussed before.</p> <p>Definition</p> \\[ (\\lambda x . \\mathscr{M})\\mathscr{N} \\longrightarrow^{\\beta} \\mathscr{M}[\\mathscr{N}/x] \\] <p>Two terms that can be reduced to each other in zero or more \\(\\beta\\) reductions or its inverse are \\(\\beta\\) equivalent. \u00df</p> <ol> <li> <p>This is essentially saying that if you have another lambda expression \\(\\mathscr{M}\\), it's valid to apply that expression as the body of another lambda function.\u00a0\u21a9</p> </li> <li> <p>In English, this says that if you have two valid lambda expressions, you can apply those two expressions together as another valid lambda expression.\u00a0\u21a9</p> </li> </ol>"},{"location":"blog/2025/07/27/i-bought-a-truck/","title":"I Bought a Truck","text":"<p>Well, I finally gave in. I bought a truck. Not just any truck, it's the truck to end all trucks. It's a truck so big that Chicago banned it.(1) Regardless, it's a truck that solves a lot of problems for me. It is on this rock that I will build my church.</p> <ol> <li>Yes really. Chicago outlawed C-plated trucks from parking on any residential street, including both paid and permitted parking. I ask myself every day why I still live in Chicago, but life is weird like that sometimes.</li> </ol> <p>Why did I decide I'm going to be a truck guy now? Well if you have read my blog at all, you know that I love to camp in the Rocky Mountains with my travel trailer. I used to tow it with my Jeep Grand Cherokee Trailhawk (with a 5.7L Hemi mind you). It looked like this:</p> <ul> <li></li> <li></li> <li></li> <li></li> </ul> <p>The Trailhawk is an incredibly capable vehicle and has served me well over the last few years, but it has 4 main problems:</p> <ol> <li>It has airbags on the suspension. The airbags are great for off roading, but they will fail at some point, and when they fail, it's probably going to be in an extremely inconvenient location thousands of miles away from home.</li> <li>While it has the raw power to tow my trailer up and down mountain passes, it's not exactly a stable platform. You can feel the vehicle sway against the weight of the trailer. </li> <li>It has a very low payload capacity: roughly 1000 lbs. The fact that the trailer itself contributes 600 lbs to the Trailhawk, that leaves 400 lbs for me, my gear, and any passengers. It's not a lot, and I'm ashamed to say that the Jeep is almost always slightly overloaded by one or two hundred pounds.</li> <li>While it's a TRAIL Hawk, it's really only moderately good at off roading. At full airbag inflation, it only gives you about 11 inches of clearance. Not bad, but if I'm to do some of the more gnarly trails, it's not enough.</li> </ol> <p>Really the major problem with this platform was the low payload capacity and unstable riding characteristics when fully loaded. So... I bought a truck. A BIG truck. I bought a mutha fukin 2024 Ram 2500 Power Wagon baby.</p> <ul> <li></li> <li></li> <li></li> <li></li> <li></li> <li></li> <li></li> </ul> <p>At the time I decided to pull the trigger on a truck, I was in the San Juan mountains of Colorado. My finances hit a major milestone and it felt like the right time, an appropriate time, to indulge in one. I found the exact model, color, and options I was looking for on autotrader.com, but unfortunately the only place that had what I was looking for was in eastern Kanses. I was 14 hours away from the dealership which is in fact a long time, but like all of my road trips, I just make an adventure of the drive itself.</p> <p>This thing is a beast. It's got both front and rear full locking differentials, a full time locking transfer case, a front disconnecting sway bar, lifted suspension, and tons of technology. Oh and of course a 6.4L Hemi. The other major detail I was looking for was a truck that came with a SmartCap bed cover. Those by themselves cost $5000 and dealerships usually don't consider the cost of add-ons when setting an asking price, so I essentially got it for free.</p> <p>With factory stock components, the truck has 14 inches of clearance, an approach angle of 34 degrees, a breakover angle of 23.2 degrees, and a departure angle of 24 degrees. I intend to install an aftermarket Carli Pintop suspension system and larger 35 inch tires which will give me an additional 2 inches of lift and additional 600lbs of payload (to bring it to 2000lbs) if options with the beefier R2 high capacity rear coils. The SmartCap bed cover also has mounting rails on the top which will allow me to add kayak mounts or even a roof top tent for overlanding.</p> <p>The possibilities are enormous and I'm super stoked to see where this will take me. And if you're wondering about the Trailhawk, don't worry. I still have her. It's getting shipped back to Chicago and will remain as my city vehicle that I can actually fit in my garage. When I'm not camping, the Green Machine will have to stay with the trailer at the storage facility I keep it at.</p>"},{"location":"blog/2025/10/21/gpu-based-containers-as-a-service/","title":"NVIDIA HGX Containers as a Service","text":"<p>This post explores how to create a multi-tenant Containers as a Service Kubernetes platform tailored for GPU-based workloads.</p>"},{"location":"blog/2025/10/21/gpu-based-containers-as-a-service/#introduction","title":"Introduction","text":"<p>Disclaimer</p> <p>The views and opinions expressed in this blog are solely my own and do not reflect the views, opinions, or positions of my employer or any organization I am affiliated with.</p> <p>Outdated Info</p> <p>Update from Februrary 2026: many of the observations I make in this post regarding various technologies have become outdated, even in the last few months since I first wrote this post. At some point, I will write a second edition of this article. Please take conclusions I make in this article with a grain of salt, as all technologies involved in this topic are rapidly improving every day.</p> <p>The core directive of most cloud companies is simple: get as many customers to rent your physical hardware as possible. The way in which this hardware gets exposed to customers can be thought of as a pyramid of increasing virtualization. </p> <p>This pyramid looks something like this:</p> <ol> <li>Bare metal clusters in a private, single-tenant network.</li> <li>Bare metal clusters in a shared, multi-tenant network.</li> <li>Virtual machines via a VMM like QEMU, cloud-hypervisor, Firecracker etc.</li> <li>Containers as a Service</li> <li>Functions as a Service</li> </ol> <p>When you look at a company like AWS, most of these layers have specific product names:</p> <ol> <li>AWS Outpost</li> <li>AWS EC2 Metal</li> <li>AWS EC2 (vanilla)</li> <li>AWS Elastic Container Service</li> <li>AWS Lambda</li> </ol> <p>The last two in this list are interesting cases of what's called a \"serverless\" infrastructure. Instead of your customer getting access to an entire host operating system, whether that be through a bare-metal OS or a virtualized one, they provide only the container or the code itself to run. The customer does not care to manage OS versions, they don't want to wrangle systemd, they don't care how it gets run, only that it does get run somewhere. \"Serverless\" of course is a bit of a misnomer because the container or the function still has to run on a server somewhere, but the cloud company itself is the one that manages the complexity of managing the lifecycle of these compute resources in a transparent way.</p> <p>A Containers as a Service (CaaS) product, as with all cloud products, needs to be concerned about how to properly isolate tenancies. Historically speaking, a \"container\" refers to an OCI-compliant \"image\" that packages an executable, a rootFS, and any software dependencies the executable runs. This executable is run as a normal process on the host kernel with various security gates around it, such as cgroups, network namespaces, filesystem mount namespaces (similar in a way to a chroot), PID namespaces, etc. that all serve to isolate this process from others.<sup>1</sup> This is a pretty good level of isolation for some use-cases, but astute readers may notice that there is a huge vector of attack: the kernel itself. The Linux kernel has been known to have major security vulnerabilities, some of which have allowed container escapes. Of course the kernel doesn't comprise just the kernel itself, but also all kernel-mode drivers (KMD) that are either part of mainline Linux or loaded as third-party kernel modules. Any of these drivers and modules can be vectors for abuse, so therefore they cannot be used to handle multiple different tenancies.</p> <p>If you don't want to share tenancies, what do you do? Well, each tenancy runs in its own VM of course. From the context of a CaaS product, there are a few different ways you can run containers inside of VMs. You can create a bespoke control plane that accepts a container, spins up a VM on a host, and runs that container inside of the VM (that kinda sounds like k8s doesn't it?). You could rely on one of the various OCI container runtimes that virtualize a kernel for the container process to use.<sup>2</sup> If you don't like reinventing the wheel, you can use a combination of open-source projects like Kubernetes for your control plane and Kata containers for your container runtime.</p>"},{"location":"blog/2025/10/21/gpu-based-containers-as-a-service/#requirements","title":"Requirements","text":"<p>Before designing our system, let's deploy our requirements onto pen and paper (or, uh, HTML and CSS):</p> <ol> <li>Host multiple different tenancies on a single server.</li> <li>Host multiple different tenancies on a single network fabric.</li> <li>Provide access to anywhere from 1 to 8 GPUs to a single container process.</li> <li>Provide access to networked storage via NFS.</li> <li>Containers should boot in a \"reasonable\" time (a couple of minutes max).</li> </ol> <p>Let's also call out an explicit non-requirement: containers will not have access to a high-performance fabric like Infiniband. Containers may be able to use an ethernet-based fabric to communicate, but for the sake of simplicity, we will disregard Infiniband.</p>"},{"location":"blog/2025/10/21/gpu-based-containers-as-a-service/#control-plane","title":"Control Plane","text":"<p>Before getting down to brass tacks, we need to design our system on a higher level. Let's assert that we have a datacenter with 100 servers each with 8 H100 SXM5 NVLink GPUs. We need a system that can accept inbound requests to run a container (perhaps with some resource requirements like the number of GPUs needed), figure out the tetris logic of where this workload can slot into the physical hardware, then deploy that workload onto the server in question. The control plane needs explicit knowledge of the state of each workload, where it's running, and what resources it's consuming. Like we alluded to before, this sounds exactly like what Kubernetes does, so let's use it!</p>"},{"location":"blog/2025/10/21/gpu-based-containers-as-a-service/#aside-about-kmds-and-nvidia-gpu-operator","title":"Aside about KMDs and NVIDIA GPU Operator","text":"<p>Recall our discussion on Kernel Mode Drivers: the host operating system itself cannot use vulnerability-prone KMDs. This means that NVIDIA drivers, which speak directly to the GPUs over PCIe, cannot be installed on the host. The guest Virtual Machine must be the one to host the NVIDIA KMDs. This complicates our k8s install quite substantially because it means we cannot use the niceties of the NVIDIA GPU Operator that do various useful things like announcing GPUs to Kubernetes, managing firmware versions, publishing DCGM metrics, establishing RDMA transactions etc. This means that without custom work, Kubernetes will be completely blind to the presence of these GPUs.<sup>3</sup></p>"},{"location":"blog/2025/10/21/gpu-based-containers-as-a-service/#cdi-device-plugin","title":"CDI Device Plugin","text":"<p>Kubernetes has a method for exposing the presence of resources through this thing called the Container Device Interface (CDI). On a high level, the CDI itself is a feature of the kubelet running on each node in the cluster. Administrators wishing to expose hardware to the cluster must implement a process that talks to the kubelet (often which is just a Unix Domain Socket implementing gRPC), announce the name of the resource being exposed, and the number of that resource currently existing on the node. </p> <pre><code>flowchart TD\n    kubelet[kubelet.service]\n    CDIPlugin\n\n    PCIe\n    GPUs\n\n    CDIPlugin -- Discover GPUs --&gt; PCIe\n    PCIe --&gt; GPUs\n    CDIPlugin -- Announce: nvidia.com/h100 --&gt; kubelet</code></pre> <p>The information given to the Kubelet is really just as simple as a key-value pair, the key being something like <code>nvidia.com/h100</code> that describes the type of hardware, and an integer <code>8</code> that says how many of that resource there are. Technically speaking, the CDI Device Plugin itself exposes a Unix Domain Socket that it tells the kubelet the existence of, and when the kubelet wants an update to the resource, it will talk to this socket. In this way, CDI Device Plugins operate in a pull-based model.</p> <p>Because we can't use GPU Operator, I decided to implement my own CDI plugin. It was surprisingly simple, and LLMs were very helpful in creating the boilerplate I needed. The CDI plugin was written in Go, packaged into a container, and then deployed to my cluster as a k8s Daemonset. The manifest is as simple as this:</p> YAML<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata: { name: vfio-device-plugin, namespace: kube-system }\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: vfio-device-plugin\n  namespace: kube-system\nspec:\n  selector: { matchLabels: { app: vfio-device-plugin } }\n  template:\n    metadata:\n      labels: { app: vfio-device-plugin }\n    spec:\n      serviceAccountName: vfio-device-plugin\n      nodeSelector:\n        kubernetes.io/arch: amd64\n      tolerations:\n      - operator: Exists\n      containers:\n      - name: plugin\n        image: docker.io/landontclipp/k8s-gpu-device-plugin:0.0.10\n        imagePullPolicy: IfNotPresent\n        env:\n        volumeMounts:\n        - name: kubelet-sock\n          mountPath: /var/lib/kubelet/device-plugins\n        - name: var-run-cdi\n          mountPath: /var/run/cdi\n        - name: dev-vfio\n          mountPath: /dev/vfio\n      volumes:\n      - name: kubelet-sock\n        hostPath:\n          path: /var/lib/kubelet/device-plugins\n          type: Directory\n      - name: var-run-cdi\n        hostPath:\n          path: /var/run/cdi\n          type: Directory\n      - name: dev-vfio\n        hostPath:\n          path: /dev/vfio\n          type: Directory\n</code></pre> <p>You'll see two interesting things here:</p> <ol> <li> <p><code>/var/lib/kubelet/device-plugins</code>: This is the host filesystem location of where the kubelet socket lives, and also where we are deciding to place the CDI plugin's socket. You can see an example here:</p> Text Only<pre><code>$ ls -lh /var/lib/kubelet/device-plugins\ntotal 12K\nsrwxr-xr-x 1 root root    0 Oct  1 21:37 kubelet.sock\n-rw------- 1 root root 2.2K Oct 15 20:26 kubelet_internal_checkpoint\nsrwxr-xr-x 1 root root    0 Oct  1 21:39 nvidia-gpu.sock\n</code></pre> </li> <li> <p><code>/var/run/cdi</code>: This is a directory that will contain specific instructions to containerd on how specifically to provide the container access to the requested resource. A snippet of a file in this directory:</p> /var/run/cdi/nvidia-gpu.json<pre><code>{\n    \"cdiVersion\": \"0.7.0\",\n    \"kind\": \"nvidia.com/gpu\",\n    \"containerEdits\": {},\n    \"devices\": [\n        {\n            \"name\": \"0\",\n            \"containerEdits\": {\n                \"deviceNodes\": [\n                    {\n                        \"path\": \"/dev/vfio/66\",\n                        \"type\": \"c\",\n                        \"permissions\": \"rwm\"\n                    }\n                ]\n            }\n        }\n    },\n</code></pre> <p>The name of the file is not important -- containerd parses all JSON documents. This particular file announces that this is the <code>nvidia.com/gpu</code> kind of resource and then lists the available devices in the <code>devices</code> map.</p> </li> </ol> <p>With these pieces of information, we can now state the steps that happen when a container, submitted to k8s, requests 1 <code>nvidia.com/gpu</code> resource:</p> <ol> <li>The k8s scheduler looks at all of the available nodes and finds one with 1 available GPU.</li> <li>The k8s scheduler selects a specific GPU index. It does this because if you remember, it has explicit knowledge of which GPUs have been allocated to which workloads on which nodes.</li> <li>The k8s scheduler posts this container to be run on a specific node with a specific GPU to the k8s API Scheduler.</li> <li>The kubelet on the selected node notices a new job has been posted to it. It submits the container spec to containerd.</li> <li>Containerd pulls down the image, sees that a <code>nvidia.com/gpu</code> resource has been requested, it consults the <code>/var/run/cdi/nvidia-gpu.json</code> file, modifies the OCI container spec according to the <code>containerEdits</code> specified in that file, and instantiates the container runtime with the newly modified spec.</li> </ol> <p>After the container process boots, it should see a character-typed device node in <code>/dev/vfio</code>.<sup>4</sup></p>"},{"location":"blog/2025/10/21/gpu-based-containers-as-a-service/#container-runtime","title":"Container Runtime","text":""},{"location":"blog/2025/10/21/gpu-based-containers-as-a-service/#an-introduction","title":"An Introduction","text":"<p>You'll remember that in a previous section, we mentioned that the tenancies cannot share the same host kernel or KMDs. How do we run containers inside of a VM? We can leverage another concept called the Container Runtime Interface. Containerd can be configured to utilize multiple different \"container runtimes\". Of course the default runtime when you download containerd is <code>runc</code>, which runs the containers as regular process on the host kernel. There is another OCI-compliant runtime called Kata Containers that can be exposed to containerd using config like this:</p> /etc/containerd/config.toml<pre><code>[plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.kata-qemu-nvidia-gpu]\nruntime_type = \"io.containerd.kata-qemu-nvidia-gpu.v2\"\nruntime_path = \"/opt/kata/bin/containerd-shim-kata-v2\"\nprivileged_without_host_devices = true\npod_annotations = [\"io.katacontainers.*\"]\n\n[plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.kata-qemu-nvidia-gpu.options]\nConfigPath = \"/opt/kata/share/defaults/kata-containers/configuration-qemu-nvidia-gpu.toml\"\n</code></pre> <p>containerd also needs to expose the CRI interface (by default, it doesn't):</p> Text Only<pre><code># ctr plugins ls |&amp; grep cri\nio.containerd.grpc.v1                  cri                      linux/amd64    ok  \n</code></pre> <p>Now, when a Pod or Daemonset or other such deployable unit is marked with:</p> YAML<pre><code>spec:\n  runtimeClassName: kata-qemu-nvidia-gpu\n</code></pre> <p>The kubelet will request that containerd use this runtime instead of the default. When we launch a container through k8s with this runtime, we can see the interesting bits in the OCI spec using a command like <code>crictl inspect 6da11814ac01e</code>:</p> JSON<pre><code>{\n  \"info\": {\n    \"config\": {\n      \"CDI_devices\": [\n        {\n          \"name\": \"nvidia.com/gpu=6\"\n        }\n      ],\n</code></pre> <p>The <code>=6</code> specifies the specific name of the GPU. This is not requesting 6 GPUs, but rather 1 GPU with the index (or name) of <code>6</code>.</p> <p>We see the runtime type:</p> JSON<pre><code>    \"runtimeType\": \"io.containerd.kata-qemu-nvidia-gpu.v2\",\n</code></pre> <p>We see the <code>devices</code> that containerd added to the spec:</p> JSON<pre><code>      \"linux\": {\n        \"cgroupsPath\": \"kubepods-podcbd00543_8fa0_44c0_9093_ee4aa64ade4c.slice:cri-containerd:6da11814ac01eddddbaf2c7ddd94c83fbca09c3ca59ba5725250819aaab0a8d9\",\n        \"devices\": [\n          {\n            \"major\": 240,\n            \"minor\": 6,\n            \"path\": \"/dev/vfio/112\",\n            \"type\": \"c\"\n          }\n        ],\n</code></pre>"},{"location":"blog/2025/10/21/gpu-based-containers-as-a-service/#kata-containers","title":"Kata Containers","text":"<p>Kata Containers is a project backed by NVIDIA that runs OCI containers inside of a MicroVM. When containerd calls out to the Kata shim <code>/opt/kata/bin/containerd-shim-kata-v2</code>, the shim performs these steps:</p> <ol> <li>Boot up a QEMU VM with a pre-defined VM image.</li> <li>Create a VSOCK port for that VM.</li> <li>Wait for the Kata agent inside of the VM to boot.</li> <li>Communicate through VSOCK with the kata-agent, request the kata-agent to run the container using a normal <code>runc</code> runtime.</li> <li>Stream results back to containerd.</li> </ol> <p>The idea itself is simple: run the container inside of a VM and go on with your day. Of course reality is more complicated than this. Recall that our requirement states that the VM should boot in a couple of minutes at most. This is simple for VMs that have no interesting devices being enumerated on the PCIe bus, but these large H100 GPUs with large BARs can be a real bear to the boot process. Not only do we need to now manage NVIDIA kernel driver versions, but we need to ensure that PCIe enumeration does not take a long time. When the kernel boots, the PCI subsystem walks the device tree and maps each BAR into virtual address space. i.e., it sets up page table mappings so that drivers can access device memory through MMIO. For large BARs (like the 64GB for H100s), this mapping step involves creating a lot more 4KB page entries which can noticeably slow down the early init process.</p> <p>One of the main ways to speed up VM boot processes, besides PCIe enumeration considerations, is to wrangle systemd. MicroVMs like those which are running container workloads probably don't need the systemd init system, and in fact NVIDIA created their own replacement to it called NVRC. NVRC does a lot of things that systemd does but without all of the ceremony. This includes things like:</p> <ol> <li>Spinning up daemons.</li> <li>Configuring syslog.</li> <li>Interacting with devices on PCIe.</li> <li>Managing system users.</li> </ol>"},{"location":"blog/2025/10/21/gpu-based-containers-as-a-service/#what-about-gvisor","title":"What about gVisor?","text":"<p>gVisor is one of the closest competitors to Kata Containers, and its implementation is really interesting. Instead of creating a virtualized kernel, gVisor instead runs the container process on the host kernel while intercepting all system calls the process makes. Most of the syscalls are served directly by gVisor, and the ones it can't will be handed off to the host kernel. In this way, gVisor acts as a sort of pseudo-virtual kernel. It's not a VM, it's not purely a syscall filter, nor is it an enhanced <code>runc</code>. It's something entirely different.</p> <p></p> <p>While this implementation is compelling, gVisor has a major flaw. Let's look at the quote in their documentation for GPU support:</p> <p>gVisor supports a wide range of CUDA workloads, including PyTorch and various generative models like LLMs. Check out this blog post about running Stable Diffusion with gVisor. gVisor also supports Vulkan and NVENC/NVDEC workloads. gVisor undergoes continuous tests to ensure this functionality remains robust. Real-world usage of gVisor across different GPU workloads helps discover and address potential compatibility or performance issues in nvproxy.</p> <p>nvproxy is a passthrough driver that forwards ioctl(2) calls made to NVIDIA devices by the containerized application directly to the host NVIDIA driver. This forwarding is straightforward: ioctl parameters are copied from the application\u2019s address space to the sentry\u2019s address space, and then a host ioctl syscall is made. ioctls are passed through with minimal intervention; nvproxy does not emulate NVIDIA kernel-mode driver (KMD) logic. This design translates to minimal overhead for GPU operations, ensuring that GPU bound workloads experience negligible performance impact.</p> <p>However, the presence of pointers and file descriptors within some ioctl structs forces nvproxy to perform appropriate translations. This requires nvproxy to be aware of the KMD\u2019s ABI, specifically the layout of ioctl structs. The challenge is compounded by the lack of ABI stability guarantees in NVIDIA\u2019s KMD, meaning ioctl definitions can change arbitrarily between releases. While the NVIDIA installer ensures matching KMD and user-mode driver (UMD) component versions, a single gVisor version might be used with multiple NVIDIA drivers. As a result, nvproxy must understand the ABI for each supported driver version, necessitating internal versioning logic for ioctls.</p> <p>As a result, nvproxy has the following limitations:</p> <ul> <li>Supports selected GPU models.  </li> <li>Supports selected NVIDIA driver versions.  </li> <li>Supports selected NVIDIA driver capabilities.  </li> <li>Supports selected NVIDIA device files.  </li> <li>Supports selected ioctls on each device file.  </li> <li>Supports selected platforms.  </li> </ul> <p>The TL;DR of it is that gVisor intercepts <code>ioctl</code> syscalls bound for the GPUs and performs a series of memory translations from the container's memory space to the kernel memory space due to the presence of file descriptors and pointers used in the struct passed to <code>ioctl</code>. Consequently, it needs explicit understanding of the NVIDIA driver ABI; simply copying the struct is not sufficient. The NVIDIA ABI is not stable across versions, which means that gVisor must be updated with new struct definitions every time a new driver is released. This is a serious flaw, as it means support for newer generations of hardware is going to be gated behind gVisor's development team deciding when and where they want to support things. That is systemically dangerous as it hamstrings you behind whatever business decisions gVisor (and by extension Google itself) makes. Kata's approach of using the KMDs published by NVIDIA directly will never have this problem because NVIDIA will never release hardware without the appropriate drivers.</p>"},{"location":"blog/2025/10/21/gpu-based-containers-as-a-service/#nvlink-isolation","title":"NVLink Isolation","text":"<p>One of the core components of NVIDIA DGX or HGX systems is the presence of an all-to-all GPU interconnect called NVLink. NVLink is a high-performance bus, resident on the server motherboard itself, that is entirely separate and distinct from the PCIe bus.  </p> <p></p> <p>Most DGX/HGX platforms have 4 switches in the NVLink fabric. The fabric can be partitioned in various ways that are pre-defined in the hardware itself. This is interesting to us because our requirements state that we need to support multiple tenancies on the same server, so we need to think about how to isolate the GPUs from each other.</p> <p>NVIDIA provides a service called nv-fabricmanager that normally runs as a systemd service on the host kernel. Because we're virtualizing our tenancies and removing NVIDIA kernel drivers from the host, we need to also run nv-fabricmanager inside of a VM. NVIDIA provides three different virtualization models for running fabricmanager:</p> <ol> <li>Full passthrough: A VM gets both all of the GPUs on a machine and all NVSwitches. </li> <li>Shared NVSwitch Multitenancy: Multiple tenants are sharing the NVLink fabric.</li> <li>vGPU Multitenancy: GPUs are exposed to VMs through their SR-IOV Virtual Functions or kernel driver mediated devices (the latter being more common). The GPU and NVswitch physical functions are managed by the host itself.</li> </ol> <p>Option 1 doesn't satisfy our requirements, and option 3 requires a special license to run their vGPU stack, so option 2 is the next best thing. Their documents say that you must run fabricmanager inside of a trusted \"Service VM\". This Service VM will contain two basic components:</p> <ol> <li>fabricmanager itself</li> <li>A service that tells fabricmanager how the fabric should be partitioned.</li> </ol> <pre><code>flowchart LR\n    subgraph Host\n        nvswitch1\n        nvswitch2\n        nvswitch3\n        nvswitch4\n\n        fmCtlHost[fm-controller]\n        subgraph ServiceVM\n            fabricmanager\n            fmCtl[fm-controller]\n            fmCtl --&gt; fabricmanager\n        end\n\n        fmCtlHost --&gt; fmCtl\n        fabricmanager --&gt; nvswitch1\n        fabricmanager --&gt; nvswitch2\n        fabricmanager --&gt; nvswitch3\n        fabricmanager --&gt; nvswitch4\n\n    end</code></pre> <p>NVIDIA doesn't provide a good CLI tool for the fabricmanager, so there is a custom tool I use that lets us interact with it. You can see the supported partition types here:</p> Text Only<pre><code>./fmctl --address 127.0.0.1:6666 list\nPARTITION ID  STATUS    GPUs  NVLINKS  GPU PHYSICAL IDs\n------------  ------    ----  -------  ----------------\n0             Inactive  8     144      1,2,3,4,5,6,7,8\n1             Inactive  4     72       1,2,3,4\n2             Inactive  4     72       5,6,7,8\n3             Inactive  2     36       1,3\n4             Inactive  2     36       2,4\n5             Inactive  2     36       5,7\n6             Inactive  2     36       6,8\n7             Active    1     0        1\n8             Active    1     0        2\n9             Active    1     0        3\n10            Active    1     0        4\n11            Active    1     0        5\n12            Active    1     0        6\n13            Active    1     0        7\n14            Active    1     0        8\n</code></pre> <p>In my particular setup, I've isolated all the GPUs from each other which is why partitions 7-14 are activated. If I wanted a 2x instance, I'd set partitions 3-6, for 4x instances, 1-2, and for 8x instances, only partition 0.</p>"},{"location":"blog/2025/10/21/gpu-based-containers-as-a-service/#fabricmanager-deployment","title":"<code>fabricmanager</code> Deployment","text":"<p>The Service VM can be deployed in a number of different ways. When I was working with Kata containers, I naturally gravitated towards launching a Kata Container Daemonset in which I could host fabricmanager, however I learned that despite a lot of the Kata project making references to fabricmanager, it was not actually fully implemented at the time. I worked with NVIDIA on some updates to the NVRC init system to support it, but I eventually decided to use libvirt to run the VM instead. </p> <p>The VM image I used to run the fabricmanager is proprietary and closed source, but the idea is basically the same as the diagram above. The NVswitches need to be passed into the VM as normal, which means binding them to the vfio-pci kernel driver. On the HGX system I was using, one important bit that also needs to be passed into the VM, in addition to the NVSwitches themselves, is the memory controller for the NVSwitches:</p> Text Only<pre><code>lspci -s 0000:03:00.1 -v\n03:00.1 Memory controller: PMC-Sierra Inc. Device 4128\n        [redacted]\n        Kernel driver in use: vfio-pci\n</code></pre> <p>One other interesting thing to note is that older NVSwitch systems expose the NVSwitches as PCIe bridge devices. Newer systems expose them as ConnectX-7 Infiniband cards which is mainly in preparation for new generations of hardware like the GB200 NVL72 racks whereby the NVLink fabric exits the chassis, allowing one to connect up to 72 GB200 Superchips (or as Nvidia's marketing team would like you to call it nowadays, 144 GPUs). A marketing picture of this kind of system is shown below.</p> <p></p>"},{"location":"blog/2025/10/21/gpu-based-containers-as-a-service/#aside-on-hgx-support-in-kata","title":"Aside on HGX Support in Kata","text":"<p>Nvidia supports two different classes of their superpod deployments. The first, called DGX, is hardware procured, designed, and deployed by Nvidia according to strict and rigorous standards. The second, called HGX, is a licensing agreement that server OEMs make with Nvidia that allows customers to design more customized NVSwitch-based superpod systems. Kata Containers, being a project led by Nvidia, has historically only worked with DGX systems. Minor (or even major) differences in hardware can cause real problems when you're dealing with virtualization because the physical way in which components are connected can dramatically differ. One interesting difference between Supermicro and Nvidia superpod systems is the fact that Supermicro places their NVSwitches behind a single IOMMU group:</p> <p></p> <p>while Nvidia DGX puts them behind individual IOMMU groups.<sup>5</sup> In my research, this difference proved to be difficult to reconcile in Kata due to some simple incorrect assumptions. It seems like this can be rectified, but my conversations with the Kata devs seem to indicate it would require additional development to correct. Regardless, the libvirt route seemed to work well enough for me, so we can abandon our attempts to run this natively in Kata.</p>"},{"location":"blog/2025/10/21/gpu-based-containers-as-a-service/#networking","title":"Networking","text":"<p>Another component that must be isolated is the network itself. By default, k8s pods can talk to each other if you have a proper Container Network Interface (CNI) plugin installed. The CNI I chose to go with is Cilium. Cilium works by implementing the CNI spec; when packets exit out of a container, they are intercepted by Cilium via eBPF trickery. Cilium will determine whether or not the packet is allowed to continue based on however you've configured the CiliumNetworkPolicy. For a simple use-case, the only thing we really need to configure is that cross-namespace traffic is disallowed. For example:</p> YAML<pre><code>apiVersion: cilium.io/v2\nkind: CiliumNetworkPolicy\nmetadata:\n  creationTimestamp: \"2025-10-01T21:46:07Z\"\n  generation: 1\n  name: ingress-egress-rules\n  namespace: tenant-landon\n  resourceVersion: \"5457770\"\n  uid: 65f501a0-d037-46f3-99cb-e585f1b7d6e8\nspec:\n  egress:\n  - toEndpoints:\n    - matchLabels:\n        k8s:io.kubernetes.pod.namespace: tenant-landon\n  endpointSelector: {}\n  ingress:\n  - fromEndpoints:\n    - matchLabels:\n        k8s:io.kubernetes.pod.namespace: tenant-landon\n</code></pre> <p>Cilium is also nice because it has explicit awareness of Kubernetes objects like namespaces, service accounts, and any other metadata you might want to create rules off of. This is contrasted to some other CNIs that have historically only allowed IP-based rules which is brittle for various reasons.</p>"},{"location":"blog/2025/10/21/gpu-based-containers-as-a-service/#tenancy-controller","title":"Tenancy Controller","text":"<p>From the perspective of the k8s cluster, it does not necessarily know ahead of time which tenancies will exist in the future. A pod can come in at any time from any customer old or new, so we must find a way to gracefully handle the case where a datacenter has not been prepared for a new tenancy. Fortunately, our simple cluster really only needs to manage two separate things:</p> <ol> <li>The Kubernetes namespaces</li> <li>The CiliumNetworkPolicy</li> </ol> <p>When a pod is submitted by a never-before-seen user to our CaaS public-facing REST API (which we haven't really discussed, but just pretend it exists for now), we need some way to attach it to a \"Tenancy\" resource that once ready will allow the pod to be submitted. This custom resource we need is something we can define using a Custom Resource Definition (CRD). The CRD document we post to k8s is as simple as this:</p> YAML<pre><code>apiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  annotations:\n    controller-gen.kubebuilder.io/version: v0.18.0\n  name: tenancies.core.topofmind.dev\nspec:\n  group: core.topofmind.dev\n  names:\n    kind: Tenancy\n    listKind: TenancyList\n    plural: tenancies\n    singular: tenancy\n  scope: Cluster\n</code></pre> <p>The Tenancy resource basically only needs one parameter: the name of the tenancy. So the CaaS API Server needs to do a few basic steps:</p> <ol> <li>When the container request comes in, check to see if the <code>Tenancy</code> already exists.</li> <li>If the <code>Tenancy</code> resource doesn't exist, create it (or just skip step 1 and 2 and do an Upsert operation instead).</li> <li>Submit the pod with a dependency on this <code>Tenancy</code> being in some sort of \"ready\" state.</li> </ol> <p>The Tenant controller now needs to watch for updates to <code>Tenancy</code> resources and perform the following actions when one is created:</p> <ol> <li>Create a new namespace</li> <li>Create the <code>CiliumNetworkPolicy</code> that blocks inter-namespace traffic.</li> <li>Set the <code>Tenancy</code> state to \"ready\".</li> </ol> <p>From this point, the k8s API Server should schedule the pod onto an available node because all of the dependent resources have now been successfully created and tenancies will be properly isolated. You can extend this kind of system for more intricate setups, such as partitioning an Infiniband fabric, managing VRFs in an ethernet fabric, setting firewall rules, allocating public IPs etc.</p>"},{"location":"blog/2025/10/21/gpu-based-containers-as-a-service/#managing-gpu-firmware","title":"Managing GPU Firmware","text":"<p>An interesting problem we run into when untrusted tenancies are asked to run the kernel drivers for our GPUs is that the tenancies can theoretically flash any kind of firmware that the card itself will accept. NVIDIA has previously stated that their GPUs will only accept authentic, signature-verified firmware signed by NVIDIA, but it's simply insufficient to trust that this process will always work. It's also unclear how exactly this works, although it is almost certainly a kind of public key infrastructure (PKI) with a chain of trust. While the security concerns might be overblown, it's simply good stewardship of a cluster to ensure that your resources are as homogeneous as possible.</p> <p>There are at least two possible ways to handle this in k8s. We could add a finalizer to the pod submissions (this basically just tells k8s to pause garbage collection until the finalizer key is removed), create a controller that watches exited pods and runs the firmware check, have the controller delete the relevant <code>metadata.finalizers</code> key, and then allow k8s to fully delete the pod. Another method is to create a CRD that represents a GPU lease. In this scenario, a GPU lease will be granted before a specific job has been run but after it has been scheduled. The GPU lease controller will watch for leases that have a pod in some completed state, schedule a firmware check, then remove the lease (which may trigger its own post-lease finalizers) or transition the lease back to some <code>Free</code> state. The GPU CDI device plugin would need explicit awareness of leases so that it does not hand out a GPU that still has an active lease on it.</p> <p>Both of these methods are functionally similar, but a GPU Lease, which may be cluster-scoped instead of namespace-scoped (as in the finalizer idea) are more robust from a security standpoint (because the tenant does not own it) and more flexible as it allows us to create a more comprehensive and flexible state machine.</p>"},{"location":"blog/2025/10/21/gpu-based-containers-as-a-service/#putting-the-pieces-together","title":"Putting the Pieces Together","text":"<p>We've talked a lot about all the individual components, but let's put them together into a cohesive story.</p> <pre><code>flowchart TD\n    ExternalUser\n\n    subgraph ControlPlane\n        TenancyController\n        GPULeaseController\n        k8sAPIServer\n        CaaSAPIServer\n\n        CaaSAPIServer -- 2\\. Create New Tenancy --&gt; k8sAPIServer\n        CaaSAPIServer -- 3\\. Create Pod --&gt; k8sAPIServer\n        TenancyController -- 4\\. Create Namespace --&gt; k8sAPIServer\n        TenancyController -- 5\\. Create NetworkPolicy --&gt; k8sAPIServer\n\n    end\n\n    ExternalUser -- 1\\. Submits Pod --&gt; CaaSAPIServer\n\n    subgraph GPUHost\n        NVSwitches\n        Kubelet\n        GPUs\n        KataShim\n        Containerd\n        Cilium\n        HostNetworking\n        VarRunCDI[nvidia.com_gpus.json]\n\n        subgraph ServiceVM [Service VM - QEMU]\n            nv-fabricmanager\n            fm-controller\n\n            fm-controller --&gt; nv-fabricmanager\n        end\n        nv-fabricmanager --&gt; NVSwitches\n\n        subgraph Daemonset\n            CDI[CDI Device Plugin]\n        end\n\n        subgraph KataVM[Kata VM]\n            KataAgent\n            runc\n            containerProcess\n\n            KataAgent --&gt; runc\n            runc --&gt; containerProcess\n\n        end\n        CDI -- Announces GPUs to --&gt; Kubelet\n        CDI -- Writes --&gt; VarRunCDI\n\n        Kubelet -- Create Pod --&gt; Containerd\n        Containerd -- Calls --&gt; KataShim\n        Containerd -- Reads --&gt; VarRunCDI\n        KataShim -- Creates --&gt; KataVM\n        KataShim -- Talks to --&gt; KataAgent\n\n        containerProcess --&gt; GPUs\n        NVSwitches &lt;--&gt; GPUs\n        Kubelet --&gt; k8sAPIServer\n        CDI --&gt; GPUs\n        HostNetworking &lt;--&gt; Cilium\n        Cilium &lt;-- Enforce Network Rules --&gt; KataVM\n    end</code></pre> <p>I tried my best to resist the temptation of being overly pedantic when creating that diagram, and somehow I feel that I still failed. Regardless, this provides a high-level overview of all the relevant components needed to make this system work. And work it does! I submitted the following self-explanatory payload to my CaaS API Server:</p> JSON<pre><code>{\n  \"container-url\": \"nvcr.io/nvidia/k8s/cuda-sample:vectoradd-cuda12.5.0-ubi8\",\n  \"tenancy\": \"landon\"\n}\n</code></pre> <p>The container is a sample CUDA workload that NVIDIA publishes here. </p> Text Only<pre><code>Starting...\n\n CUDA Device Query (Runtime API) version (CUDART static linking)\n\nDetected 1 CUDA Capable device(s)\n\nDevice 0: \"NVIDIA H100 80GB HBM3\"\n  CUDA Driver Version / Runtime Version          13.0 / 12.5\n  CUDA Capability Major/Minor version number:    9.0\n  Total amount of global memory:                 81079 MBytes (85017755648 bytes)\n  (132) Multiprocessors, (128) CUDA Cores/MP:    16896 CUDA Cores\n  GPU Max Clock rate:                            1980 MHz (1.98 GHz)\n  Memory Clock rate:                             2619 Mhz\n  Memory Bus Width:                              5120-bit\n  L2 Cache Size:                                 52428800 bytes\n  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)\n  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers\n  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers\n  Total amount of constant memory:               65536 bytes\n  Total amount of shared memory per block:       49152 bytes\n  Total shared memory per multiprocessor:        233472 bytes\n  Total number of registers available per block: 65536\n  Warp size:                                     32\n  Maximum number of threads per multiprocessor:  2048\n  Maximum number of threads per block:           1024\n  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\n  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\n  Maximum memory pitch:                          2147483647 bytes\n  Texture alignment:                             512 bytes\n  Concurrent copy and kernel execution:          Yes with 3 copy engine(s)\n  Run time limit on kernels:                     No\n  Integrated GPU sharing Host Memory:            No\n  Support host page-locked memory mapping:       Yes\n  Alignment requirement for Surfaces:            Yes\n  Device has ECC support:                        Enabled\n  Device supports Unified Addressing (UVA):      Yes\n  Device supports Managed Memory:                Yes\n  Device supports Compute Preemption:            Yes\n  Supports Cooperative Kernel Launch:            Yes\n  Supports MultiDevice Co-op Kernel Launch:      Yes\n  Device PCI Domain ID / Bus ID / location ID:   0 / 2 / 0\n  Compute Mode:\n     &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;\n\ndeviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 13.0, CUDA Runtime Version = 12.5, NumDevs = 1\nResult = PASS\n</code></pre> <p>And a short GPU Burn:</p> Text Only<pre><code>==========\n== CUDA ==\n==========\n\nCUDA Version 13.0.1\n\nContainer image Copyright (c) 2016-2023, NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved.\n\nThis container image and its contents are governed by the NVIDIA Deep Learning Container License.\nBy pulling and using the container, you accept the terms and conditions of this license:\nhttps://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\n\nA copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.\n\nGPU 0: NVIDIA H100 80GB HBM3 (UUID: GPU-c6b0ca29-53c0-326e-8aaa-cfa1653e18d1)\nUsing compare file: compare.ptx\nBurning for 120 seconds.\n\n100.0%  proc'd: 5600 (51402 Gflop/s)   errors: 0   temps: 56 C\n\nKilling processes with SIGTERM (soft kill)\nUsing compare file: compare.ptx\nBurning for 120 seconds.\nInitialized device 0 with 81079 MB of memory (80484 MB available, using 72436 MB of it), using FLOATS\nResults are 268435456 bytes each, thus performing 280 iterations\nFreed memory for dev 0\nUninitted cublas\ndone\n\nTested 1 GPUs:\n        GPU 0: OK\n</code></pre> <p>Success! </p>"},{"location":"blog/2025/10/21/gpu-based-containers-as-a-service/#multi-gpu-containers","title":"Multi-GPU Containers","text":"<p>The setup I've created allows the user to specify how many GPUs they want their container to have access to between the values 1 and 8 inclusive. In my testing, I found that attempting to spawn multi-GPU Kata VMs was very difficult. A single-GPU VM on my system took about 2 minutes to spawn, most of that time seemingly spent on various bookkeeping steps after the GPU is ACPI hot-plugged, things like PCIe enumeration, device BAR mapping, NVIDIA driver initialization. These are the same concerns I noted above. I found that attempting to mount 8 GPUs was so slow in fact that Kubernetes eventually determined that the pod launch failed, eventually killing the VM and causing it to get into a crash loop. After aggressively increasing the various timeouts involved with pod launches, I found the VMs took around 30 minutes in the 8 GPU case. I brought this up to the Kata maintainers and they seemed surprised, noting that their launches took far less time.</p> <p>Some sleuthing suggests that a lot of this performance issue comes from the way the hypervisor maps the guest-physical memory addresses to host-physical memory addresses. These mappings appear to happen in 4 KiB chunks, which of course would turn into a huge number of mappings that need to be created for just a single GPU. This is an area of further investigation.</p>"},{"location":"blog/2025/10/21/gpu-based-containers-as-a-service/#my-impressions","title":"My Impressions","text":"<p>Broadening our scope of thinking beyond just \"how do I build GPU CaaS\", we ask ourselves \"why build GPU CaaS\"? There is a ton of complexity being introduced when we involve virtualization which means the maintenance burden is higher than a bare-metal deployment. It will also be true that Kata support for newer generations of hardware will always lag by some amount regardless of how diligent its developers are, especially considering the fact that there are multiple manufacturers of these HGX systems whose hardware differences must all be accounted for.</p> <p>After using Kata for a number of months, my general impression of it is that its support for GPU workloads is still young. It works well in the full-passthrough model, and especially so on systems without an NVLink fabric (because the complexities involved in the fabricmanager disappear), but more complex superpod systems like ones with NVLink fabrics were more difficult to do correctly. This is not a fault of the talented people working on the project. Instead, it's a timing problem. Kata only started supporting GPU passthrough about a year ago, so it hasn't had enough time to thoroughly iron out all of the creases that arise with every permutation of hardware. The Kata developers are making rapid progress however, so I have no doubt that a lot of these issues will resolve themselves.</p> <p>The industry in general tends to favor large, monolithic, single-tenancy deployments for the latest generations of hardware. This is especially evident when you look at companies like Coreweave who are almost completely disinterested in servicing contracts less than a billion dollars. CaaS's place in the world seems to neatly fit into non-bleeding-edge generations of hardware. It's a recycling mechanism for businesses needing to continue to extract profits out of old hardware.</p> <p>The other thing to realize is that the world's AI-fueled GPU craze is very new, like only a few years old. As this craze continues to propagate and entrench itself, greater focus is going to be paid to making AI developers' lives easier. Serverless compute is the way of the future, and there is much engineering work to be done to bring this reality to fruition.</p> <ol> <li> <p>In fact, this is why OCI containers intrinsically rely on the Linux kernel because all of these features are Linux features, and is why containers running in MacOS or Windows require a Linux Virtual Machine. That's not to say there couldn't be a runtime that works in MacOS and Windows natively, but it would require exact feature parity and also risks containers behaving differently on different OSs, which is bad.\u00a0\u21a9</p> </li> <li> <p>The two interesting competitors here are gVisor and Kata Containers.\u00a0\u21a9</p> </li> <li> <p>Technically speaking, NVIDIA does provide a way to run sandboxed workloads in GPU Operator using Kata containers, but in the author's experience, the interaction between GPU Operator and Kata is extremely fraught with pitfalls, bugs, and in general seems to be poorly supported. They even call this out that using GPU Operator with sandboxed workloads is explicitly a \"Technology Preview\" and is not fully supported. UPDATE (2026-02-09): NVIDIA has continued to invest in GPU Operator support and they now have explicit support for an early access preview of GPU Operator with Kata and Confidential Containers. They seem highly motivated to make the two work together.\u00a0\u21a9</p> </li> <li> <p>This does not get exposed as a bind mount, but rather as a new device node created with <code>mknod</code>.\u00a0\u21a9</p> </li> <li> <p>The exact logic the Linux kernel uses to determine IOMMU groups is slightly mysterious to me, but from my understanding it is based on whether or not the kernel believes that PCIe endpoints could communicate directly with each other without reaching out to the root complex (called Peer-to-Peer or P2P connection). You can see that the NVSwitches sit behind a common bridge at a certain level (<code>0000:04</code>).\u00a0\u21a9</p> </li> </ol>"},{"location":"blog/2025/11/19/boot-time-performance-of-kata-containers-with-nvidia-gpus/","title":"Boot-time Performance of Kata Containers With NVIDIA GPUs","text":"<p>If you recall in my previous post, we looked into how to utilize Kata VMs to create a multi-tenant, GPU-based Kubernetes cluster. We briefly mentioned various performance problems related to the VM boot process. This post will dive deeper into this issue and shed some insight into where time is being spent during the VM boot.</p>"},{"location":"blog/2025/11/19/boot-time-performance-of-kata-containers-with-nvidia-gpus/#vm-console-logs","title":"VM Console Logs","text":"<p>The first thing we should establish is the timeline of events. Recall in the previous post that we're running Kata containers inside of a Kubernetes cluster. We attach 7 GPUs to the VM (instead of 8, because one of them on my machine is broken) and watch the Kata agent logs. To recap, this is a diagram that shows how the OCI components are fitting together:</p> <pre><code>flowchart TD\n    containerd[containerd.service]\n    kataShim[containerd-shim-kata-v2]\n\n    subgraph QEMU\n        kataAgent[kata-agent.service]\n    end\n\n    containerd --&gt; kataShim\n    kataShim -- creates --&gt; QEMU\n\n    kataShim &lt;-- VSOCK --&gt; kataAgent</code></pre> <p>By enabling various debug flags in the Kata runtime config and by setting <code>level = \"debug\"</code> in the containerd config, we can view all of the logs coming not only from the QEMU dmesg logs, but also the kata-agent itself. Let's now boot a VM and see what's happening. We can look at the containerd logs, grep for the specific sandbox we care about, and can see an example log message:</p> Text Only<pre><code>Nov 20 16:25:32 dal03-sgpu-160 containerd[2771108]: time=\"2025-11-20T16:25:32.996810781Z\" level=debug msg=\"reading guest console\" console-protocol=unix console-url=/run/vc/vm/38f55c4a058a0f7227e64edd75f2aed134bee7db4b211f071be84fb44d17b0bb/console.sock name=containerd-shim-v2 pid=3426175 sandbox=38f55c4a058a0f7227e64edd75f2aed134bee7db4b211f071be84fb44d17b0bb source=virtcontainers subsystem=sandbox vmconsole=\"{\\\"msg\\\":\\\"policy check: StatsContainerRequest\\\",\\\"level\\\":\\\"DEBG\\\",\\\"ts\\\":\\\"2025-11-20T16:25:33.010026353Z\\\",\\\"version\\\":\\\"0.1.0\\\",\\\"name\\\":\\\"kata-agent\\\",\\\"subsystem\\\":\\\"rpc\\\",\\\"source\\\":\\\"agent\\\",\\\"pid\\\":\\\"1\\\"}\"\n</code></pre> <p>Let's use some additional awk trickery to clean this up. All we really care about is the <code>vmconsole</code> field, so let's separate that out and make the formatting a bit nicer:</p> Bash<pre><code>journalctl -u containerd -S '40m ago' |&amp; grep sandbox=38f55c4a058a0f7227e64edd75f2aed134bee7db4b211f071be84fb44d17b0bb | awk -F'vmconsole=' '{ print $2 }'  | jq -r &gt; vmconsole.txt\n</code></pre> <p>The first messages we see are the dmesg logs which are kernel-level messages:</p> Text Only<pre><code>[    0.437005] brd: module loaded\n[    0.443558] loop: module loaded\n[    0.443819] virtio_scsi virtio1: 1/0/0 default/read/poll queues\n[...]\n</code></pre> <p>Shortly after this, we also see kata-agent logs:</p> Text Only<pre><code>{\"msg\":\"\\\"proc\\\" is already mounted at \\\"/proc\\\"\",\"level\":\"INFO\",\"ts\":\"2025-11-20T16:05:09.356676179Z\",\"pid\":\"1\",\"version\":\"0.1.0\",\"subsystem\":\"baremount\",\"name\":\"kata-agent\",\"source\":\"agent\"}\n{\"msg\":\"\\\"sysfs\\\" is already mounted at \\\"/sys\\\"\",\"level\":\"INFO\",\"ts\":\"2025-11-20T16:05:09.356699617Z\",\"pid\":\"1\",\"version\":\"0.1.0\",\"subsystem\":\"baremount\",\"source\":\"agent\",\"name\":\"kata-agent\"}\n{\"msg\":\"\\\"dev\\\" is already mounted at \\\"/dev\\\"\",\"level\":\"INFO\",\"ts\":\"2025-11-20T16:05:09.356718292Z\",\"name\":\"kata-agent\",\"version\":\"0.1.0\",\"pid\":\"1\",\"subsystem\":\"baremount\",\"source\":\"agent\"}\n</code></pre> <p>Kata-agent was configured to redirect its log message to <code>/dev/console</code>, which makes sense, so this is what we want. Let's zoom down further for more relevant info. It starts off by checking for cloud-init data (which it doesn't find any because I have not configured it):</p> Text Only<pre><code>{\"msg\":\"Initdata check device `ptmx`\",\"level\":\"DEBG\",\"ts\":\"2025-11-20T16:05:09.359571917Z\",\"source\":\"agent\",\"name\":\"kata-agent\",\"version\":\"0.1.0\",\"subsystem\":\"initdata\",\"pid\":\"1\"}\n{\"msg\":\"Initdata check device `pts`\",\"level\":\"DEBG\",\"ts\":\"2025-11-20T16:05:09.359640437Z\",\"subsystem\":\"initdata\",\"name\":\"kata-agent\",\"pid\":\"1\",\"version\":\"0.1.0\",\"source\":\"agent\"}\n{\"msg\":\"Initdata check device `shm`\",\"level\":\"DEBG\",\"ts\":\"2025-11-20T16:05:09.3596694Z\",\"version\":\"0.1.0\",\"name\":\"kata-agent\",\"pid\":\"1\",\"subsystem\":\"initdata\",\"source\":\"agent\"}\n</code></pre> <p>We can see it receiving some commands from the containerd shim:</p> Text Only<pre><code>{\"msg\":\"rpc call from shim to agent: \\\"create_sandbox\\\"\",\"level\":\"INFO\",\"ts\":\"2025-11-20T16:05:09.374408769Z\",\"source\":\"agent\",\"subsystem\":\"rpc\",\"version\":\"0.1.0\",\"name\":\"kata-agent\",\"pid\":\"1\"}\n</code></pre> <p>We begin to see the dmesg logs again and we can see the precense of the GPUs showing up on the VM's PCIe bus only 3 seconds after the kernel first booted:</p> Text Only<pre><code>[    3.201931] pci 0000:02:00.0: [10de:2330] type 00 class 0x030200 PCIe Endpoint\n[    3.202399] pci 0000:02:00.0: BAR 0 [mem 0x00000000-0x00ffffff 64bit pref]\n[    3.202551] pci 0000:02:00.0: BAR 2 [mem 0x00000000-0x1fffffffff 64bit pref]\n[    3.202726] pci 0000:02:00.0: BAR 4 [mem 0x00000000-0x01ffffff 64bit pref]\n[    3.202962] pci 0000:02:00.0: Max Payload Size set to 128 (was 256, max 256)\n[    3.203347] pci 0000:02:00.0: Enabling HDA controller\n[    4.858791] pci 0000:02:00.0: 252.048 Gb/s available PCIe bandwidth, limited by 16.0 GT/s PCIe x16 link at 0000:00:05.0 (capable of 504.112 Gb/s with 32.0 GT/s PCIe x16 link)\n[    5.681204] pci 0000:02:00.0: BAR 2 [mem 0x3a4000000000-0x3a5fffffffff 64bit pref]: assigned\n[    5.681521] pci 0000:02:00.0: BAR 4 [mem 0x3a6000000000-0x3a6001ffffff 64bit pref]: assigned\n[    5.681842] pci 0000:02:00.0: BAR 0 [mem 0x3a6002000000-0x3a6002ffffff 64bit pref]: assigned\n[    6.505076] pci 0000:02:00.0: nv_msi_ht_cap_quirk_leaf+0x0/0x20 took 803659 usecs\n</code></pre> <p>Tip</p> <p>For those curious, <code>10de</code> is the PCIe vendor code for NVIDIA and <code>2330</code> is the PCIe Device ID for NVIDIA H100 GPUs. </p> <p>It's showing us the BAR regions of each GPU. In fact, we should be able to see seven instances of this kind of message:</p> Bash<pre><code>$ grep '\\[10de\\:2330\\]' vmconsole.txt \n[    3.201931] pci 0000:02:00.0: [10de:2330] type 00 class 0x030200 PCIe Endpoint\n[    7.329101] pci 0000:06:00.0: [10de:2330] type 00 class 0x030200 PCIe Endpoint\n[    8.159058] pci 0000:05:00.0: [10de:2330] type 00 class 0x030200 PCIe Endpoint\n[    8.165639] pci 0000:04:00.0: [10de:2330] type 00 class 0x030200 PCIe Endpoint\n[    8.170375] pci 0000:03:00.0: [10de:2330] type 00 class 0x030200 PCIe Endpoint\n[    8.177573] pci 0000:08:00.0: [10de:2330] type 00 class 0x030200 PCIe Endpoint\n[    8.182313] pci 0000:07:00.0: [10de:2330] type 00 class 0x030200 PCIe Endpoint\n$ grep '\\[10de\\:2330\\]' vmconsole.txt  | wc -l\n7\n</code></pre> <p>We can then see the kata-agent receives a uevent from the kernel regarding the presence of these GPUs being enumerated on the bus. For some context, userspace programs can subscribe to kernel uevent messages so that we can be alerted when something about a device has changed. In this case, the kata-agent cares about when a GPU device appears on the bus. Let's grep for the <code>0000:07:00.0</code> device to see the whole end-to-end flow:</p> Text Only<pre><code>$ grep '0000:07:00.0' vmconsole.txt \n{\"msg\":\"Waiting for PciMatcher { devpath: \\\"/devices/pci0000:00/0000:00:0a.0/0000:07:00.0\\\" }\",\"level\":\"INFO\",\"ts\":\"2025-11-20T16:05:15.701000961Z\",\"version\":\"0.1.0\",\"name\":\"kata-agent\",\"subsystem\":\"uevent\",\"pid\":\"1\",\"source\":\"agent\"}\n{\"msg\":\"Waiting for PciMatcher { devpath: \\\"/devices/pci0000:00/0000:00:0a.0/0000:07:00.0\\\" }: waiting on channel\",\"level\":\"INFO\",\"ts\":\"2025-11-20T16:05:15.701021912Z\",\"subsystem\":\"uevent\",\"source\":\"agent\",\"pid\":\"1\",\"version\":\"0.1.0\",\"name\":\"kata-agent\"}\n[    8.182313] pci 0000:07:00.0: [10de:2330] type 00 class 0x030200 PCIe Endpoint\n[    8.182722] pci 0000:07:00.0: BAR 0 [mem 0x00000000-0x00ffffff 64bit pref]\n[    8.182858] pci 0000:07:00.0: BAR 2 [mem 0x00000000-0x1fffffffff 64bit pref]\n[    8.183003] pci 0000:07:00.0: BAR 4 [mem 0x00000000-0x01ffffff 64bit pref]\n[    8.183217] pci 0000:07:00.0: Max Payload Size set to 128 (was 256, max 256)\n[    8.184788] pci 0000:07:00.0: Enabling HDA controller\n[    8.186130] pci 0000:07:00.0: 252.048 Gb/s available PCIe bandwidth, limited by 16.0 GT/s PCIe x16 link at 0000:00:0a.0 (capable of 504.112 Gb/s with 32.0 GT/s PCIe x16 link)\n[    8.186969] pci 0000:07:00.0: BAR 2 [mem 0x390000000000-0x391fffffffff 64bit pref]: assigned\n[    8.187249] pci 0000:07:00.0: BAR 4 [mem 0x392000000000-0x392001ffffff 64bit pref]: assigned\n[    8.187528] pci 0000:07:00.0: BAR 0 [mem 0x392002000000-0x392002ffffff 64bit pref]: assigned\n{\"msg\":\"got uevent message\",\"level\":\"INFO\",\"ts\":\"2025-11-20T16:05:15.71233274Z\",\"subsystem\":\"uevent\",\"pid\":\"1\",\"version\":\"0.1.0\",\"source\":\"agent\",\"name\":\"kata-agent\",\"event\":\"Uevent { action: \\\"add\\\", devpath: \\\"/devices/pci0000:00/0000:00:0a.0/0000:07:00.0\\\", devname: \\\"\\\", subsystem: \\\"pci\\\", seqnum: \\\"1379\\\", interface: \\\"\\\" }\"}\n{\"msg\":\"Waiting for PciMatcher { devpath: \\\"/devices/pci0000:00/0000:00:0a.0/0000:07:00.0\\\" }: found Uevent { action: \\\"add\\\", devpath: \\\"/devices/pci0000:00/0000:00:0a.0/0000:07:00.0\\\", devname: \\\"\\\", subsystem: \\\"pci\\\", seqnum: \\\"1379\\\", interface: \\\"\\\" } on channel\",\"level\":\"INFO\",\"ts\":\"2025-11-20T16:05:15.712365772Z\",\"version\":\"0.1.0\",\"subsystem\":\"uevent\",\"source\":\"agent\",\"name\":\"kata-agent\",\"pid\":\"1\"}\n[   32.130338] nvidia 0000:07:00.0: enabling device (0140 -&gt; 0142)\n{\"msg\":\"got uevent message\",\"level\":\"INFO\",\"ts\":\"2025-11-20T16:05:42.96188988Z\",\"pid\":\"1\",\"subsystem\":\"uevent\",\"source\":\"agent\",\"name\":\"kata-agent\",\"version\":\"0.1.0\",\"event\":\"Uevent { action: \\\"bind\\\", devpath: \\\"/devices/pci0000:00/0000:00:0a.0/0000:07:00.0\\\", devname: \\\"\\\", subsystem: \\\"pci\\\", seqnum: \\\"1387\\\", interface: \\\"\\\" }\"}\n{\"msg\":\"ignoring event\",\"level\":\"DEBG\",\"ts\":\"2025-11-20T16:05:42.961921263Z\",\"name\":\"kata-agent\",\"source\":\"agent\",\"pid\":\"1\",\"version\":\"0.1.0\",\"subsystem\":\"uevent\",\"uevent\":\"Uevent { action: \\\"bind\\\", devpath: \\\"/devices/pci0000:00/0000:00:0a.0/0000:07:00.0\\\", devname: \\\"\\\", subsystem: \\\"pci\\\", seqnum: \\\"1387\\\", interface: \\\"\\\" }\"}\n</code></pre> <p>We can see the following steps happening:</p> <ol> <li>Kata subscribes to the uevent system for this particular BDF.</li> <li>The kernel enumerates the PCIe bus, finds this device, and assignes (what I assume to be) MMIO regions for the device BARs.</li> <li>The kata agent is informed by the kernel that this device has been added.</li> <li> <p>The kata agent saves a reference to this device in-memory and then waits for the CDI spec for this device to be injected by the shim. We see this in the logs:</p> Text Only<pre><code>{\"msg\":\"waiting for CDI spec(s) to be generated (0 of 100 max tries) \\\"unresolvable CDI devices nvidia.com/gpu=3, nvidia.com/gpu=1, nvidia.com/gpu=4, nvidia.com/gpu=6, nvidia.com/gpu=5, nvidia.com/gpu=2, nvidia.com/gpu=0\\\"\",\"level\":\"INFO\",\"ts\":\"2025-11-20T16:05:15.71245172Z\",\"source\":\"agent\",\"name\":\"kata-agent\",\"version\":\"0.1.0\",\"subsystem\":\"rpc\",\"pid\":\"1\"}\n</code></pre> </li> </ol> <p>What can we gather here? Well, the PCIe endpoints representing the GPUs show up in the VM in a matter of a few seconds from first boot. The agent recognizes this immediately. After this step, the kata-agent ends up waiting a LONG time for the CDI spec to be injected from the Kata shim. What does this mean precisely? Well, if you recall in the previous blog post regarding the CDI device plugin, our CDI plugin on the host creates a file at <code>/var/run/cdi/nvidia-gpu.json</code> that describes the OCI container spec edits that must be made to inject these GPUs into the container sandbox. The first few lines of this file look something like:</p> /var/run/cdi/nvidia-gpu.json<pre><code>{\n  \"cdiVersion\": \"0.7.0\",\n  \"kind\": \"nvidia.com/gpu\",\n  \"containerEdits\": {},\n  \"devices\": [\n    {\n      \"name\": \"0\",\n      \"containerEdits\": {\n        \"deviceNodes\": [\n          {\n            \"path\": \"/dev/vfio/66\",\n            \"type\": \"c\",\n            \"permissions\": \"rwm\"\n          }\n        ]\n      }\n    },\n</code></pre> <p>What the agent is telling us is that, for whatever reason, the CDI spec has not shown up in the guest's <code>/var/run/cdi</code> directory (which is populated by the nvidia-ctk toolkit, called from the NVRC init system), so the agent doesn't yet know which edits it needs to make to the container to properly expose the GPU to the process. In fact, look at how long it waits!</p> Text Only<pre><code>$ grep 'waiting for CDI spec' vmconsole.txt \n{\"msg\":\"waiting for CDI spec(s) to be generated (0 of 100 max tries) \\\"unresolvable CDI devices nvidia.com/gpu=3, nvidia.com/gpu=1, nvidia.com/gpu=4, nvidia.com/gpu=6, nvidia.com/gpu=5, nvidia.com/gpu=2, nvidia.com/gpu=0\\\"\",\"level\":\"INFO\",\"ts\":\"2025-11-20T16:05:15.71245172Z\",\"source\":\"agent\",\"name\":\"kata-agent\",\"version\":\"0.1.0\",\"subsystem\":\"rpc\",\"pid\":\"1\"}\n[...]\n{\"msg\":\"waiting for CDI spec(s) to be generated (39 of 100 max tries) \\\"unresolvable CDI devices nvidia.com/gpu=3, nvidia.com/gpu=1, nvidia.com/gpu=4, nvidia.com/gpu=6, nvidia.com/gpu=5, nvidia.com/gpu=2, nvidia.com/gpu=0\\\"\",\"level\":\"INFO\",\"ts\":\"2025-11-20T16:06:37.462492776Z\",\"name\":\"kata-agent\",\"subsystem\":\"rpc\",\"pid\":\"1\",\"version\":\"0.1.0\",\"source\":\"agent\"}\n</code></pre> <p>It spends over a minute just waiting. Given the overall boot time of the VM (which I've gotten down to 2 minutes), this represents HALF of the entire boot process. The question is why?</p> <p>Another clue we have are some messages from the guest NVIDIA Kernel Driver:</p> Text Only<pre><code>$ grep 'enabling device' vmconsole.txt \n[   11.721284] nvidia 0000:02:00.0: enabling device (0140 -&gt; 0142)\n[   15.137841] nvidia 0000:06:00.0: enabling device (0140 -&gt; 0142)\n[   18.534911] nvidia 0000:05:00.0: enabling device (0140 -&gt; 0142)\n[   22.032357] nvidia 0000:04:00.0: enabling device (0140 -&gt; 0142)\n[   25.719357] nvidia 0000:03:00.0: enabling device (0140 -&gt; 0142)\n[   28.918995] nvidia 0000:08:00.0: enabling device (0140 -&gt; 0142)\n[   32.130338] nvidia 0000:07:00.0: enabling device (0140 -&gt; 0142)\n</code></pre> <p>It seems that each instance of \"enabling device\" takes the driver around 4 seconds to complete, or about 28 seconds total for all 7 GPUs. It's possible this could be a red herring because it does seem that more time is spent on waiting for the CDI specs to arrive, not for the driver to enable all devices.</p> <p>It also spends an additional 13 seconds handling some kind of bug:</p> Text Only<pre><code># grep gpuClearFbhubPoisonIntrForBug2924523_GA100 vmconsole.txt \n[   35.757629] NVRM: gpuClearFbhubPoisonIntrForBug2924523_GA100: FBHUB Interrupt detected. Clearing it.\n[   37.904920] NVRM: gpuClearFbhubPoisonIntrForBug2924523_GA100: FBHUB Interrupt detected. Clearing it.\n[   40.019364] NVRM: gpuClearFbhubPoisonIntrForBug2924523_GA100: FBHUB Interrupt detected. Clearing it.\n[   42.144963] NVRM: gpuClearFbhubPoisonIntrForBug2924523_GA100: FBHUB Interrupt detected. Clearing it.\n[   44.272692] NVRM: gpuClearFbhubPoisonIntrForBug2924523_GA100: FBHUB Interrupt detected. Clearing it.\n[   46.396907] NVRM: gpuClearFbhubPoisonIntrForBug2924523_GA100: FBHUB Interrupt detected. Clearing it.\n[   48.518811] NVRM: gpuClearFbhubPoisonIntrForBug2924523_GA100: FBHUB Interrupt detected. Clearing it.\n</code></pre> <p>The time spent waiting for <code>nvidia-ctk</code> to create the CDI file could also be the fault of the NVIDIA driver itself. What's obvious is that cold starts of GPUs take a long time, and utilizing alternative technologies like NVIDIA vGPU (whereby the kernel driver is always loaded either on the host or in a sibling VM like what edera.dev does.</p>"},{"location":"blog/2025/11/19/boot-time-performance-of-kata-containers-with-nvidia-gpus/#using-bpftrace-to-time-dma-mappings","title":"Using <code>bpftrace</code> to Time DMA Mappings","text":"<p>One of my suspicions in the boot performance issue was that a lot of time was being spent in creating the DMA mappings between the GPUs and physical CPU memory. One of the most crucial parts of creating DMA mappings is pinning pages in memory. To pin a page means that you force the page to be backed by real physical memory (as opposed requiring page faults to lazily back a page into memory only when the page is touched). Because the GPUs are being passed through using the vfio-pci kernel driver, we can look in the VFIO source code to see what function call does this. It is in fact a function called <code>vfio_pin_pages_remote</code>.</p> <p>There is a technology in the Linux kernel called eBPF which stands for Extended Berkley Packet Filter. It was originally created as a way to filter packets flowing into and out of network cards, but the technology was extended to allow us to trace basically any part of the kernel we want. The <code>bpftrace</code> tool in particular is something we can use to create eBPF programs.</p> <p>Let's look at a program I created:</p> vfio_trace.bpf<pre><code>kprobe:vfio_pin_pages_remote {\n    @start[tid] = nsecs;\n\n    $sec = strftime(\"%Y-%m-%dT%H:%M:%S\", nsecs);\n    $nsec = nsecs % 1000000000;\n\n    printf(\"%s.%09dZ: vfio_pin_pages_remote(): vaddr=0x%lx npage=%lu size=%luB limit=%lu pid=%d\\n\",\n           $sec, $nsec, arg1, arg2, arg2 * 4096, arg4, pid);\n}\nkretprobe:vfio_pin_pages_remote {\n    $d = (nsecs - @start[tid]) / 1e6;\n\n    $sec = strftime(\"%Y-%m-%dT%H:%M:%S\", nsecs);\n    $nsec = nsecs % 1000000000;\n\n    printf(\"%s.%09dZ: vfio_pin_pages_remote took %d ms\\n\", $sec, $nsec, $d);\n    @hist = hist($d);\n    delete(@start[tid]);\n}\n</code></pre> <p>There are two components to this:</p>"},{"location":"blog/2025/11/19/boot-time-performance-of-kata-containers-with-nvidia-gpus/#kprobe","title":"<code>kprobe</code>","text":"<p>This section creates a callback that is triggered at the beginning of the function call. Using the eBPF language (which looks a lot like C in many respects), we save the start time of the execution and then print some of the arguments of the call along with a timestamp.</p>"},{"location":"blog/2025/11/19/boot-time-performance-of-kata-containers-with-nvidia-gpus/#kretprobe","title":"<code>kretprobe</code>","text":"<p>This callback is similar to <code>kprobe</code> except it is called after the function exits. We grab the start time (based off of the thread ID <code>tid</code>) and subtract it from the end time. This gives us the runtime in milliseconds which we can then print.</p> <p>The result is messages like this:</p> Text Only<pre><code>vfio_pin_pages_remote(): vaddr=0x74de00000000 npage=33554432 size=137438953472B limit=2048 pid=3449227                                                                                                                                             \nvfio_pin_pages_remote took 3274 ms   \n</code></pre> <p>When the traces end, bpftrace also gives us a histogram of the function runtimes:</p> Text Only<pre><code>@hist:                                                                                                                                                                                                                                             \n[0]                   89 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@|\n[1]                    2 |@                                                   |\n[2, 4)                 8 |@@@@                                                |\n[4, 8)                 0 |                                                    |\n[8, 16)                0 |                                                    |\n[16, 32)               0 |                                                    |\n[32, 64)               1 |                                                    |\n[64, 128)              0 |                                                    |\n[128, 256)             0 |                                                    |\n[256, 512)             0 |                                                    |\n[512, 1K)              0 |                                                    |\n[1K, 2K)               0 |                                                    |\n[2K, 4K)               7 |@@@@                                                |\n[4K, 8K)               0 |                                                    |\n[8K, 16K)              0 |                                                    |\n[16K, 32K)             1 |                                                    |\n</code></pre> <p>Using these timestamps, we can now cross-reference what's happening on the host side regarding DMA mappings and the guest side. Let's try it!</p>"},{"location":"blog/2025/11/19/boot-time-performance-of-kata-containers-with-nvidia-gpus/#booting-a-new-vm","title":"Booting a new VM","text":"<p>I booted a new Kata VM and saved the containerd logs:</p> Text Only<pre><code># journalctl -u containerd -S '40 minutes ago' |&amp; grep 'sandbox=c9f7bf13c1911810fa49a6be9a9d1efe0872d4088a008ac9eddd5bece6e56889' | awk -F'vmconsole=' '{ print $2 }'  | jq -r &gt; vmconsole2.txt\n</code></pre> <p>You can view those here.</p> <p>The output of the bpftrace program lives here.</p> <p>The first significant traces we can see are these 128GiB sections.</p> Text Only<pre><code>2025-11-20T17:56:21.688170596Z: vfio_pin_pages_remote(): vaddr=0x786c40000000 npage=33554432 size=137438953472B limit=2048 pid=3454228\n2025-11-20T17:56:25.955145443Z: vfio_pin_pages_remote took 3266 ms\n</code></pre> <p>There are seven of them in fact (coincidence? I think not!). The first one happens at timestamp <code>2025-11-20T17:56:21.688170596Z</code>. What was happening in the guest around this time?</p> Text Only<pre><code>[   21.101954] nvidia 0000:02:00.0: enabling device (0140 -&gt; 0142)\n{\"msg\":\"waiting for CDI spec(s) to be generated (5 of 100 max tries) \\\"unresolvable CDI devices nvidia.com/gpu=2, nvidia.com/gpu=0, nvidia.com/gpu=1, nvidia.com/gpu=6, nvidia.com/gpu=5, nvidia.com/gpu=3, nvidia.com/gpu=4\\\"\",\"level\":\"INFO\",\"ts\":\"2025-11-20T17:56:21.93953012Z\",\"version\":\"0.1.0\",\"name\":\"kata-agent\",\"source\":\"agent\",\"pid\":\"1\",\"subsystem\":\"rpc\"}\n{\"msg\":\"got uevent message\",\"level\":\"INFO\",\"ts\":\"2025-11-20T17:56:21.947065025Z\",\"source\":\"agent\",\"subsystem\":\"uevent\",\"name\":\"kata-agent\",\"version\":\"0.1.0\",\"pid\":\"1\",\"event\":\"Uevent { action: \\\"bind\\\", devpath: \\\"/devices/pci0000:00/0000:00:05.0/0000:02:00.0\\\", devname: \\\"\\\", subsystem: \\\"pci\\\", seqnum: \\\"1381\\\", interface: \\\"\\\" }\"}\n{\"msg\":\"ignoring event\",\"level\":\"DEBG\",\"ts\":\"2025-11-20T17:56:21.947096369Z\",\"source\":\"agent\",\"version\":\"0.1.0\",\"name\":\"kata-agent\",\"pid\":\"1\",\"subsystem\":\"uevent\",\"uevent\":\"Uevent { action: \\\"bind\\\", devpath: \\\"/devices/pci0000:00/0000:00:05.0/0000:02:00.0\\\", devname: \\\"\\\", subsystem: \\\"pci\\\", seqnum: \\\"1381\\\", interface: \\\"\\\" }\"}\n</code></pre> <p>The kata-agent noticed that the GPU device at <code>0000:02:00.0</code> was bound to the NVIDIA driver. It isn't until about 3221 milliseconds later that the NVIDIA driver gets bound to the next GPU:</p> Text Only<pre><code>[   24.322027] nvidia 0000:07:00.0: enabling device (0140 -&gt; 0142)\n</code></pre> <p>This matches almost exactly with the runtime of <code>vfio_pin_pages_remote</code>. This process repeats for a total of 7 times until the next semi-significant DMA mappings occur:</p> Text Only<pre><code>2025-11-20T17:57:56.948647354Z: vfio_pin_pages_remote(): vaddr=0x7832c0000000 npage=26214400 size=107374182400B limit=2048 pid=3454228\n2025-11-20T17:57:56.437252192Z: vfio_pin_pages_remote took 488 ms\n</code></pre> <p>While these calls take less time, there are far more of them that happen in rapid succession starting from the timestamp <code>2025-11-20T17:57:56.948647354Z</code> and ending in <code>2025-11-20T17:58:13.178330994Z</code> for a total of roughly 17 seconds. Again, we look inside the guest:</p> Text Only<pre><code>{\"msg\":\"rpc call from shim to agent: \\\"online_cpu_mem\\\"\",\"level\":\"INFO\",\"ts\":\"2025-11-20T17:58:13.487517854Z\",\"pid\":\"1\",\"source\":\"agent\",\"name\":\"kata-agent\",\"version\":\"0.1.0\",\"subsystem\":\"rpc\"}\n</code></pre> <p>There's a flurry of memory devices being added.</p> Text Only<pre><code>{\"msg\":\"got uevent message\",\"level\":\"INFO\",\"ts\":\"2025-11-20T17:58:13.50283434Z\",\"source\":\"agent\",\"subsystem\":\"uevent\",\"pid\":\"1\",\"version\":\"0.1.0\",\"name\":\"kata-agent\",\"event\":\"Uevent { action: \\\"add\\\", devpath: \\\"/devices/system/memory/memory176\\\", devname: \\\"\\\", subsystem: \\\"memory\\\", seqnum: \\\"1400\\\", interface: \\\"\\\" }\"}\n{\"msg\":\"got uevent message\",\"level\":\"INFO\",\"ts\":\"2025-11-20T17:58:13.50289347Z\",\"version\":\"0.1.0\",\"name\":\"kata-agent\",\"subsystem\":\"uevent\",\"pid\":\"1\",\"source\":\"agent\",\"event\":\"Uevent { action: \\\"add\\\", devpath: \\\"/devices/system/memory/memory177\\\", devname: \\\"\\\", subsystem: \\\"memory\\\", seqnum: \\\"1401\\\", interface: \\\"\\\" }\"}\n{\"msg\":\"got uevent message\",\"level\":\"INFO\",\"ts\":\"2025-11-20T17:58:13.502941403Z\",\"version\":\"0.1.0\",\"source\":\"agent\",\"pid\":\"1\",\"subsystem\":\"uevent\",\"name\":\"kata-agent\",\"event\":\"Uevent { action: \\\"add\\\", devpath: \\\"/devices/system/memory/memory178\\\", devname: \\\"\\\", subsystem: \\\"memory\\\", seqnum: \\\"1402\\\", interface: \\\"\\\" }\"}\n{\"msg\":\"got uevent message\",\"level\":\"INFO\",\"ts\":\"2025-11-20T17:58:13.50298798Z\",\"version\":\"0.1.0\",\"subsystem\":\"uevent\",\"pid\":\"1\",\"source\":\"agent\",\"name\":\"kata-agent\",\"event\":\"Uevent { action: \\\"add\\\", devpath: \\\"/devices/system/memory/memory179\\\", devname: \\\"\\\", subsystem: \\\"memory\\\", seqnum: \\\"1403\\\", interface: \\\"\\\" }\"}\n{\"msg\":\"got uevent message\",\"level\":\"INFO\",\"ts\":\"2025-11-20T17:58:13.503030684Z\",\"subsystem\":\"uevent\",\"source\":\"agent\",\"pid\":\"1\",\"version\":\"0.1.0\",\"name\":\"kata-agent\",\"event\":\"Uevent { action: \\\"add\\\", devpath: \\\"/devices/system/memory/memory180\\\", devname: \\\"\\\", subsystem: \\\"memory\\\", seqnum: \\\"1404\\\", interface: \\\"\\\" }\"}\n{\"msg\":\"got uevent message\",\"level\":\"INFO\",\"ts\":\"2025-11-20T17:58:13.503075124Z\",\"pid\":\"1\",\"version\":\"0.1.0\",\"source\":\"agent\",\"name\":\"kata-agent\",\"subsystem\":\"uevent\",\"event\":\"Uevent { action: \\\"add\\\", devpath: \\\"/devices/system/memory/memory181\\\", devname: \\\"\\\", subsystem: \\\"memory\\\", seqnum: \\\"1405\\\", interface: \\\"\\\" }\"}\n</code></pre> <p>The guest receives the last memory device around this message:</p> Text Only<pre><code>{\"msg\":\"got uevent message\",\"level\":\"INFO\",\"ts\":\"2025-11-20T17:58:13.55354784Z\",\"name\":\"kata-agent\",\"source\":\"agent\",\"subsystem\":\"uevent\",\"pid\":\"1\",\"version\":\"0.1.0\",\"event\":\"Uevent { action: \\\"add\\\", devpath: \\\"/devices/system/memory/memory927\\\", devname: \\\"\\\", subsystem: \\\"memory\\\", seqnum: \\\"2151\\\", interface: \\\"\\\" }\"}\n</code></pre> <p>Which also lines up with the last time <code>vfio_pin_pages_remote</code> is called:</p> Text Only<pre><code>025-11-20T17:58:12.678281961Z: vfio_pin_pages_remote(): vaddr=0x784ac0000000 npage=1048576 size=4294967296B limit=2048 pid=3454228\n2025-11-20T17:58:13.178330994Z: vfio_pin_pages_remote took 500 ms\n</code></pre>"},{"location":"blog/2025/11/19/boot-time-performance-of-kata-containers-with-nvidia-gpus/#interpreting-the-results","title":"Interpreting the Results","text":"<p>We now have a fairly clear picture of where a lot of the time is being spent.</p>  timestamp   side   time from last event   description  <code>2025-11-20T17:55:59.692656146Z</code>  guest   1.2 seconds from T=0   The VM boots and calls out to the Kata agent.  <code>2025-11-20T17:56:00.118714278Z</code>  guest   0.5 seconds   The kata agent finishes its initial sanity checks and bookkeeping tasks. A minimal amount of memory is hotplugged.  <code>2025-11-20T17:56:11</code>  guest   ~11 seconds                   The PCIe enumeration process in the kernel notices its first NVIDIA GPU:                  Text Only<pre><code>[   12.582377] pci 0000:02:00.0: [10de:2330] type 00 class 0x030200 PCIe Endpoint\n</code></pre> <code>2025-11-20T17:56:13.514397783Z</code>  guest   2 seconds                   The PCI device is added and Kata notices:                  JSON<pre><code>{\n    \"msg\": \"got uevent message\",\n    \"level\": \"INFO\",\n    \"ts\": \"2025-11-20T17:56:13.514397783Z\",\n    \"pid\": \"1\",\n    \"version\": \"0.1.0\",\n    \"subsystem\": \"uevent\",\n    \"name\": \"kata-agent\",\n    \"source\": \"agent\",\n    \"event\": \"Uevent { action: \\\"add\\\", devpath: \\\"/devices/pci0000:00/0000:00:05.0/0000:02:00.0\\\"[...]\"\n}\n</code></pre> <code>2025-11-20T17:56:14.369905314Z</code>  guest   1 second                   The last GPU device is added to the PCI tree.              <code>2025-11-20T17:56:18.466861384Z</code>  host   4 seconds                    The first heavyweight `vfio_pin_pages_remote` call is made. This roughly corresponds to the guest-side NVIDIA kernel driver making this statement:                  Text Only<pre><code>[   21.101954] nvidia 0000:02:00.0: enabling device (0140 -&gt; 0142)\n</code></pre> <code>2025-11-20T17:56:42.438059073Z</code>  host   24 seconds                   The last heavyweight `vfio_pin_pages_remote` call completes. This tells us each page pin for the GPUs takes about 3 seconds, which again matches the timings we get from bpftrace.                   Text Only<pre><code>2025-11-20T17:56:39.269298767Z: vfio_pin_pages_remote(): vaddr=0x784c00000000 npage=33554432 size=137438953472B limit=2048 pid=3454228\n2025-11-20T17:56:42.438059073Z: vfio_pin_pages_remote took 3168 ms\n</code></pre> <code>2025-11-20T17:57:40.052488828Z</code>  guest   58 seconds                   The CDI specs for all devices were injected successfully to kata-agent.                  Text Only<pre><code>{\"msg\":\"waiting for CDI spec(s) to be generated (40 of 100 max tries) \\\"unresolvable CDI devices nvidia.com/gpu=2, nvidia.com/gpu=0, nvidia.com/gpu=1, nvidia.com/gpu=6, nvidia.com/gpu=5, nvidia.com/gpu=3, nvidia.com/gpu=4\\\"\",\"level\":\"INFO\",\"ts\":\"2025-11-20T17:57:37.23937483Z\",\"subsystem\":\"rpc\",\"name\":\"kata-agent\",\"source\":\"agent\",\"pid\":\"1\",\"version\":\"0.1.0\"}\n{\"msg\":\"handling signal\",\"level\":\"INFO\",\"ts\":\"2025-11-20T17:57:37.775940382Z\",\"subsystem\":\"signals\",\"version\":\"0.1.0\",\"pid\":\"1\",\"name\":\"kata-agent\",\"source\":\"agent\",\"signal\":\"SIGCHLD\"}\n{\"msg\":\"wait_status\",\"level\":\"INFO\",\"ts\":\"2025-11-20T17:57:37.775976033Z\",\"version\":\"0.1.0\",\"subsystem\":\"signals\",\"source\":\"agent\",\"pid\":\"1\",\"name\":\"kata-agent\",\"wait_status result\":\"Signaled(Pid(139), SIGKILL, false)\"}\n{\"msg\":\"child exited unexpectedly\",\"level\":\"INFO\",\"ts\":\"2025-11-20T17:57:37.775999136Z\",\"pid\":\"1\",\"child-pid\":\"139\",\"source\":\"agent\",\"subsystem\":\"signals\",\"name\":\"kata-agent\",\"version\":\"0.1.0\"}\n{\"msg\":\"all devices injected successfully, modified CDI container spec:\n</code></pre> <p>What did we learn from this? Well, the page pinning steps indeed take a long time to complete at around 24 seconds. That is significant, but it's not as significant as the amount of time that the kata-agent waits for the CDI specs to be injected into it.</p> <p>The kata devs told me that after the devices are initialized and bound to the NVIDIA kernel driver, the Kata init system (NVRC) calls out to nvidia-ctk to create the CDI. This is where the point of synchronization comes from, so we must investigate this more.</p>"},{"location":"blog/2025/11/19/boot-time-performance-of-kata-containers-with-nvidia-gpus/#cant-we-use-iommu-hugepages","title":"Can't we use IOMMU Hugepages?","text":"<p>This page pinning step is an obvious area for performance improvements. One might reflexively reach for the idea of reducing the number of pages that have to be pinned. We can look again at one of our heavyweight <code>vfio_pin_pages_remote</code> calls:</p> Text Only<pre><code>2025-11-24T23:06:01.865912341Z: vfio_pin_pages_remote(): vaddr=0x7d92c0000000 npage=33554432 size=137438953472B limit=2048 pid=705877                                                                                                              \n2025-11-24T23:06:04.337004440Z: vfio_pin_pages_remote took 3471 ms  \n</code></pre> <p>If we divide the size by the number of pages, we get 4096. My intuition tells me that Intel VT-d should be able to support hugepages for DMA mappings, and it in fact does. However, the Linux IOMMU driver does not support it for a few reasons. IOMMU mappings need to be pinned, aligned, and contiguous at hugepage boundaries in host physical memory. Memory allocated by QEMU often is not contiguous, and even when it has hugepages, they may not be aligned. We can see how the Intel IOMMU driver handles these mappings:</p> https://github.com/torvalds/linux/blob/v6.17/drivers/iommu/intel/iommu.c#L3590-L3594<pre><code>    /* Round up size to next multiple of PAGE_SIZE, if it and\n       the low bits of hpa would take us onto the next page */\n    size = aligned_nrpages(hpa, size);\n    return __domain_mapping(dmar_domain, iova &gt;&gt; VTD_PAGE_SHIFT,\n                hpa &gt;&gt; VTD_PAGE_SHIFT, size, prot, gfp);\n</code></pre> https://github.com/torvalds/linux/blob/v6.17/drivers/iommu/intel/iommu.h#L1085-L1090<pre><code>/* Returns a number of VTD pages, but aligned to MM page size */\nstatic inline unsigned long aligned_nrpages(unsigned long host_addr, size_t size)\n{\n    host_addr &amp;= ~PAGE_MASK;\n    return PAGE_ALIGN(host_addr + size) &gt;&gt; VTD_PAGE_SHIFT;\n}\n</code></pre> https://github.com/torvalds/linux/blob/v6.17/include/linux/mm.h#L208-L209<pre><code>/* to align the pointer to the (next) page boundary */\n#define PAGE_ALIGN(addr) ALIGN(addr, PAGE_SIZE)\n</code></pre> <p>And this <code>PAGE_SIZE</code> is statically compiled to 4KiB. So we're out of luck. This might be a further area of investigation to look into why this problem has not been solved and what attempts (if any) have been made to solve it.</p>"},{"location":"blog/2025/11/19/boot-time-performance-of-kata-containers-with-nvidia-gpus/#using-linux-function_graph-tracer","title":"Using Linux function_graph Tracer","text":"<p>For a bit of fun, I wanted try the linux ftrace utility. This should give us the same information as bpftrace but through a different mechanism. We do this with the following steps:</p> Text Only<pre><code>cd /sys/kernel/tracing\n\n# Turn on\necho 1 &gt; tracing_on\n\n# Use the function_graph tracer\necho function_graph &gt; current_tracer\n\n# Only trace the function we care about\necho vfio_pin_pages_remote &gt; set_ftrace_filter\n\n# We boot a Kata VM\n\n# Turn off\necho 0 &gt; tracing_on\n\n# Inspect\ncat trace &gt; /tmp/trace.out\nless /tmp/trace.out\n</code></pre> Text Only<pre><code>223) $ 3389981 us  |  vfio_pin_pages_remote [vfio_iommu_type1]();\n223) ! 899.140 us  |  vfio_pin_pages_remote [vfio_iommu_type1]();\n223) ! 370.991 us  |  vfio_pin_pages_remote [vfio_iommu_type1]();\n223) ! 125.701 us  |  vfio_pin_pages_remote [vfio_iommu_type1]();\n108) $ 3460872 us  |  vfio_pin_pages_remote [vfio_iommu_type1]();\n108) ! 852.023 us  |  vfio_pin_pages_remote [vfio_iommu_type1]();\n108) ! 372.259 us  |  vfio_pin_pages_remote [vfio_iommu_type1]();\n108) ! 133.358 us  |  vfio_pin_pages_remote [vfio_iommu_type1]();\n223) $ 3542329 us  |  vfio_pin_pages_remote [vfio_iommu_type1]();\n223) ! 901.558 us  |  vfio_pin_pages_remote [vfio_iommu_type1]();\n223) ! 400.939 us  |  vfio_pin_pages_remote [vfio_iommu_type1]();\n223) ! 137.331 us  |  vfio_pin_pages_remote [vfio_iommu_type1]();\n108) $ 3748615 us  |  vfio_pin_pages_remote [vfio_iommu_type1]();\n108) ! 938.270 us  |  vfio_pin_pages_remote [vfio_iommu_type1]();\n223) ! 388.741 us  |  vfio_pin_pages_remote [vfio_iommu_type1]();\n223) ! 123.352 us  |  vfio_pin_pages_remote [vfio_iommu_type1]();\n108) $ 3173935 us  |  vfio_pin_pages_remote [vfio_iommu_type1]();\n108) ! 853.867 us  |  vfio_pin_pages_remote [vfio_iommu_type1]();\n108) ! 390.508 us  |  vfio_pin_pages_remote [vfio_iommu_type1]();\n108) ! 139.837 us  |  vfio_pin_pages_remote [vfio_iommu_type1]();\n108) $ 3288427 us  |  vfio_pin_pages_remote [vfio_iommu_type1]();\n108) ! 964.999 us  |  vfio_pin_pages_remote [vfio_iommu_type1]();\n108) ! 407.721 us  |  vfio_pin_pages_remote [vfio_iommu_type1]();\n108) ! 141.893 us  |  vfio_pin_pages_remote [vfio_iommu_type1]();\n108) $ 3358507 us  |  vfio_pin_pages_remote [vfio_iommu_type1]();\n108) ! 954.615 us  |  vfio_pin_pages_remote [vfio_iommu_type1]();\n</code></pre> <p>We see a similar result. There are 7 heavyweight page pinning calls that each take about 3.5 seconds.</p>"},{"location":"blog/2025/11/19/boot-time-performance-of-kata-containers-with-nvidia-gpus/#aside-on-mmio-mappings","title":"Aside on MMIO Mappings","text":"<p>So far, we have measured the IOMMU mapping performance which deals specifically with giving the passed-through GPU devices DMA capabilities into the guest physical address space. There is another side of memory mapping that deals with CPU access into the GPU, which in a way can be considered DMA in the opposite direction. This of course is called Memory-Mapped IO (MMIO).</p> <p>I won't go into the history of why MMIO exists. Instead, we first need to consider how MMIO is being established in this case. We're running a QEMU VMM that is using Linux KVM acceleration. KVM is essentially Linux's implementation of a type-1 hypervisor: it allows userspace processes like QEMU to offload virtualization tasks to the kernel rather than having to emulate everything themselves. Thus, the KVM code is where QEMU will be requesting to establish MMIO mappings.</p> <p>To summarize more, the vfio-pci kernel driver exposes the device BARs as offsets into a file descriptor. This host-side MMIO mapping has already been created as part of the PCIe enumeration process. What QEMU must do is establish with KVM a callback that says \"when the guest attempts to access this range of memory, let me handle the page fault.\" This is done by calling the <code>KVM_SET_USER_MEMORY_REGION</code> ioctl which establishes a guest memory region (which is perhaps obvious). QEMU then purposefully does not map this region to any RAM, which in effect creates a memory region \"hole\" in the guest memory. When the guest touches this hole, KVM can't resolve it to RAM, so it calls back to QEMU to handle this page fault (this is considered a VM Exit scenario). After receiving this callback from KVM, QEMU will forward the access to the vfio-pci kernel driver using the appropriate file handle. This guest-physical-memory to device file descriptor mapping is maintained by QEMU itself. From this point onward, vfio-pci relies on normal host MMIO mappings to then talk directly to the device in question.</p> <p>To put it more concretely in what this looks like from QEMU's source code:</p> https://github.com/qemu/qemu/blob/v10.1.2/accel/kvm/kvm-all.c#L3402<pre><code>int kvm_vcpu_ioctl(CPUState *cpu, unsigned long type, ...)\n{\n    int ret;\n    void *arg;\n    va_list ap;\n\n    va_start(ap, type);\n    arg = va_arg(ap, void *);\n    va_end(ap);\n\n    trace_kvm_vcpu_ioctl(cpu-&gt;cpu_index, type, arg);\n    accel_cpu_ioctl_begin(cpu);\n    ret = ioctl(cpu-&gt;kvm_fd, type, arg);\n    accel_cpu_ioctl_end(cpu);\n    if (ret == -1) {\n        ret = -errno;\n    }\n    return ret;\n}\n</code></pre> <p>This <code>ioctl</code> is called from an outer loop called <code>kvm_cpu_exec</code>, specifically this line:</p> https://github.com/qemu/qemu/blob/v10.1.2/accel/kvm/kvm-all.c#L3205<pre><code>        run_ret = kvm_vcpu_ioctl(cpu, KVM_RUN, 0);\n</code></pre> <p>The return code is then checked for various cases, one of them being <code>KVM_EXIT_MMIO</code>:</p> https://github.com/qemu/qemu/blob/v10.1.2/accel/kvm/kvm-all.c#L3253-L3261<pre><code>        case KVM_EXIT_MMIO:\n            /* Called outside BQL */\n            address_space_rw(&amp;address_space_memory,\n                             run-&gt;mmio.phys_addr, attrs,\n                             run-&gt;mmio.data,\n                             run-&gt;mmio.len,\n                             run-&gt;mmio.is_write);\n            ret = 0;\n            break;\n</code></pre> <p>The question we can then ask is: what boot-time performance hit is incurred from creating these MMIO mappings? From the host kernel's perspective, there is no work done at all; QEMU simply created a guest memory region and left a hole for MMIO regions. There is still a performance hit incurred when the kernel must walk the page tables to determine whether or not this memory access is backed by RAM or if it must be handled by QEMU, but that can be optimized just like any other page-table access pattern through the use of hugepages. </p> <p>What about QEMU's performance hit when creating these mappings? This is also unlikely to be significant. We assume that the QEMU developers are smart and they would only have to create one mapping per BAR. Most PCIe devices only have a small handful of BARs, so the only work QEMU must do is:</p> <ol> <li>Discover the device BARs</li> <li>Create a range in memory reserved for MMIO to those BARs (which should be a constant time operation, i.e. independent of the actual size of the BARs).</li> <li>Translate the MMIO page fault into the correct vfio-pci file descriptor (which again should be effectively a constant-time operation).</li> </ol> <p>You can see that there is really no opportunity for the creation, maintenance, or servicing of these mappings to have any significant affect on boot-time performance. Just like we discovered with IOMMU mappings, the only real optimization to be made here is to reduce the size of the page trees that the Linux kernel must walk, which again is most often done by giving QEMU access to hugepages.</p>"},{"location":"blog/2025/11/19/boot-time-performance-of-kata-containers-with-nvidia-gpus/#hugepages","title":"Hugepages","text":""},{"location":"blog/2025/11/19/boot-time-performance-of-kata-containers-with-nvidia-gpus/#turned-off","title":"Turned Off","text":"<p>In the last few sections, we've established pretty clearly that hugepages are necessary for increasing the boot performance of QEMU. Kata containers allows us to specify hypervisor-level configuration options that we'll be interested in: https://github.com/kata-containers/kata-containers/blob/55489818d6d5c2ab4c8dbd586c1001474048c2b9/docs/how-to/how-to-set-sandbox-config-kata.md</p> <p>These are passed in as Kubernetes annotations. Specifically, we care about <code>io.katacontainers.config.hypervisor.enable_hugepages</code>. We have to configure Kata to allow certain parameters to be passed, so I have this specified:</p> configuration.toml<pre><code>[hypervisor.qemu]\nenable_annotations = [\"enable_hugepages\", \"default_memory\", \"enable_iommu\", \"virtio_fs_extra_args\", \"kernel_params\"] \n</code></pre> <p>Now when we set <code>io.katacontainers.config.hypervisor.enable_hugepages</code> in k8s manifest annotations, it overrides the relevant configuration set in that same <code>configuration.toml</code> file. This is the explanation given in the default Kata config file:</p> configuration.toml<pre><code># Enable huge pages for VM RAM, default false\n# Enabling this will result in the VM memory\n# being allocated using huge pages.\n# This is useful when you want to use vhost-user network\n# stacks within the container. This will automatically\n# result in memory pre allocation\n#enable_hugepages = true\n</code></pre> <p>First we must tell the Linux kernel to create hugepages. I have already configured my kernel with 1GiB hugepages:</p> Text Only<pre><code>$ cat /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages\n504\n</code></pre> <p>You can also see this reflected in k8s:</p> Text Only<pre><code>$ kubectl describe node dal03-sgpu-160 |&amp; grep hugepages\n  hugepages-1Gi:        504Gi\n  hugepages-2Mi:        0\n</code></pre> <p>Let's first boot a Kata QEMU VM with no hugepages attached (and to be clear, the container image has already been downloaded):</p> pod.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: cdi-vfio-gpu0-test-160\n  annotations:\n    io.katacontainers.config.hypervisor.default_memory: \"10240\"\n    #io.katacontainers.config.hypervisor.enable_hugepages: \"true\"\nspec:\n  runtimeClassName: kata-qemu-nvidia-gpu\n  restartPolicy: Never\n  nodeName: dal03-sgpu-160\n  containers:\n  - name: check\n    image: ubuntu:22.04\n    resources:\n      limits:\n        nvidia.com/gpu: 7\n        cpu: 500m\n        memory: 100Gi\n        #hugepages-1Gi: 125Gi\n      requests:\n        nvidia.com/gpu: 7\n        cpu: 500m\n        memory: 100Gi\n        #hugepages-1Gi: 125Gi\n    command:\n      - /bin/bash\n      - -lc\n      - sleep 9999\n</code></pre> <p>Pod Overhead</p> <p>Also note that the Kata RuntimeClass needs to have overhead set:</p> YAML<pre><code>apiVersion: node.k8s.io/v1\nhandler: kata-qemu-nvidia-gpu\nkind: RuntimeClass\nmetadata:\ncreationTimestamp: \"2025-09-09T20:16:14Z\"\nname: kata-qemu-nvidia-gpu\nresourceVersion: \"17416951\"\nuid: ba2819f0-4370-44c8-b745-9cdfa49890ff\noverhead:\npodFixed:\n    cpu: \"1\"\n    memory: 20Gi\nscheduling:\nnodeSelector:\n    katacontainers.io/kata-runtime: \"true\"\n</code></pre> <p>The Pod overhead specifies how much additional memory overhead should be added to the pod cgroup to account for memory taken up by QEMU and the host kernel itself related to bookkeeping structures (mainly page tables, it seems). You can see that 20GiB + 100GiB are represented exactly in the cgroup limit:</p> Text Only<pre><code>$ cat /sys/fs/cgroup/kubepods.slice/kubepods-podea116782_533a_4cc7_af74_c19661ac9c47.slice/memory.max\n128849018880\n</code></pre> <p>Not specifying this overhead will cause the Linux cgroup OOM killer to kill your pod, because QEMU will allocate 100GiB of shared memory for the guest in addition to the host kernel overhead that is taken up to maintain the 4KiB page table mappings.</p> <p>We submit the pod and find the difference between the start (submission) time and the time the pod transitioned to a <code>Running</code> state:</p> Text Only<pre><code>$ kubectl describe pod cdi-vfio-gpu0-test-160 |&amp; grep 'Nov 2025'\nStart Time:          Mon, 24 Nov 2025 21:21:09 +0000\n      Started:      Mon, 24 Nov 2025 21:24:12 +0000\n</code></pre> <p>Or in other words, 3:03 minutes. It's also interesting to look at various memory statistics:</p> <p> </p> <p>The page table bytes themselves (accounted as part of what the host kernel uses) are almost 2GiB! That's significant. The overall kernel bytes, which includes page tables but also any other data structures in use to support this QEMU process, is 3.43GiB. We can also take a look at the process memory map:</p> pmap Text Only<pre><code>$ pmap 678295\n678295:   /opt/kata/bin/qemu-system-x86_64 -name sandbox-6b5d0d72fe60c5c4055f1fb0137b5d6138b3b79d299ec09d9d91191a5f5ad6aa,debug-threads=on -uuid b3ea4564-b366-45e7-90eb-f98e3c1015e6 -machine q35,accel=kvm -cpu host,pmu=off -qmp unix:fd=3,server=on,wait=off -m 10240M,slots=10,maxmem=2060538M -device pci-bridge,bus=pcie.0,id=pci-bridge-0,chassis_nr=1,shpc=off,addr=2,io-reserve=4k,mem-reserve=1m,pref64-reserve=1m -device virtio-serial-pci,disable-modern=false,id=serial0 -device virtconsole,chardev=charconsole0,id=console0 -chardev socket,id=charconsole0,path=/run/vc/vm/6b5d0d72fe60c5c4055f1fb0137b5d6138b3b79d299ec09d9d91191a5f5ad6aa/console.sock,server=on,wait=off -device virtio-scsi-pci,id=scsi0,disable-modern=false -object rng-random,id=rng0,filename=/dev/urandom -device virtio-rng-pci,rng=rng0 -device pcie-root-port,id=rp0,bus=pcie.0,chassis=0,slot=0,multifunction=off,pref64-reserve=274877906944B,mem-reserve=4194304B -device pcie-root-port,id=rp1,bus=pcie.0,chassis=0,slot=1,multifunction=off,pref64-reserve=274877906944B,mem-reserve=4194304B -device pcie-root-port,id=rp2,bus=pcie.0,chassis=0,slot=2,multifunction=off,pref64-reserve=274877906944B,mem-reserve=4194304B -device pcie-root-port,id=rp3,bus=pcie.0,chassis=0,slot=3,multifunction=off,pref64-reserve=274877906944B,mem-reserve=4194304B -device pcie-root-port,id=rp4,bus=pcie.0,chassis=0,slot=4,multifunction=off,pref64-reserve=274877906944B,mem-reserve=4194304B -device pcie-root-port,id=rp5,bus=pcie.0,chassis=0,slot=5,multifunction=off,pref64-reserve=274877906944B,mem-reserve=4194304B -device pcie-root-port,id=rp6,bus=pcie.0,chassis=0,slot=6,multifunction=off,pref64-reserve=274877906944B,mem-reserve=4194304B -devicepcie-root-port,id=rp7,bus=pcie.0,chassis=0,slot=7,multifunction=off,pref64-reserve=274877906944B,mem-reserve=4194304B -device pcie-root-port,id=rp8,bus=pcie.0,chassis=0,slot=8,multifunction=off,pref64-reserve=274877906944B,mem-reserve=4194304B -device pcie-root-port,id=rp9,bus=pcie.0,chassis=0,slot=9,multifunction=off,pref64-reserve=274877906944B,mem-reserve=4194304B -device vhost-vsock-pci,disable-modern=false,vhostfd=4,id=vsock-1243457778,guest-cid=1243457778 -chardev socket,id=char-ae1f2f6fcddb4b1b,path=/run/vc/vm/6b5d0d72fe60c5c4055f1fb0137b5d6138b3b79d299ec09d9d91191a5f5ad6aa/vhost-fs.sock -device vhost-user-fs-pci,chardev=char-ae1f2f6fcddb4b1b,tag=kataShared,queue-size=1024 -netdev tap,id=network-0,vhost=on,vhostfds=5,fds=6 -device driver=virtio-net-pci,netdev=network-0,mac=7a:d1:ec:bc:16:2a,disable-modern=false,mq=on,vectors=4 -rtc base=utc,driftfix=slew,clock=host -global kvm-pit.lost_tick_policy=discard -vga none -no-user-config -nodefaults -nographic --no-reboot -object memory-backend-file,id=dimm1,size=10240M,mem-path=/dev/shm,share=on -numa node,memdev=dimm1 -kernel /opt/kata/share/kata-containers/vmlinux-6.12.47-166-nvidia-gpu -initrd /opt/kata/share/kata-containers/kata-ubuntu-noble-nvidia-gpu-580.65.06.initrd -append tsc=reliable no_timer_check rcupdate.rcu_expedited=1 i8042.direct=1 i8042.dumbkbd=1 i8042.nopnp=1 i8042.noaux=1 noreplace-smp reboot=k cryptomgr.notests net.ifnames=0 pci=lastbus=0 console=hvc0 console=hvc1 debug panic=1 nr_cpus=224 selinux=0 scsi_mod.scan=none agent.log=debug agent.debug_console agent.debug_console_vport=1026 agent.debug_console agent.debug_console_vport=1026 agent.hotplug_timeout=20 cgroup_no_v1=all -pidfile /run/vc/vm/6b5d0d72fe60c5c4055f1fb0137b5d6138b3b79d299ec09d9d91191a5f5ad6aa/pid -smp 1,cores=1,threads=1,sockets=224,maxcpus=224\n000055558a964000 122196K rw---   [ anon ]\n00007a35bffff000 104857600K rw-s- qemu_back_mem.mem0.FMRypy (deleted)\n00007a4ebffff000      4K -----   [ anon ]\n00007a4ec0000000 134217728K rw-s-   [ anon ]\n00007a6f00000000 134217728K rw-s-   [ anon ]\n00007a8f40000000 134217728K rw-s-   [ anon ]\n00007aaf80000000 134217728K rw-s-   [ anon ]\n00007acfc0000000 134217728K rw-s-   [ anon ]\n00007af000000000 134217728K rw-s-   [ anon ]\n00007b1020000000    132K rw---   [ anon ]\n00007b1020021000  65404K -----   [ anon ]\n00007b1024000000  32768K rw-s-   [ anon ]\n00007b1028000000  16384K rw-s-   [ anon ]\n00007b102a000000  32768K rw-s-   [ anon ]\n00007b102e000000  16384K rw-s-   [ anon ]\n00007b1030000000  32768K rw-s-   [ anon ]\n00007b1034000000  16384K rw-s-   [ anon ]\n00007b1036000000  32768K rw-s-   [ anon ]\n00007b103a000000  16384K rw-s-   [ anon ]\n00007b103c000000  32768K rw-s-   [ anon ]\n00007b1040000000 134217728K rw-s-   [ anon ]\n00007b3042000000  16384K rw-s-   [ anon ]\n00007b3044000000  32768K rw-s-   [ anon ]\n00007b3048000000  32768K rw-s-   [ anon ]\n00007b304a200000 132788K r---- kata-ubuntu-noble-nvidia-gpu-580.65.06.initrd\n00007b3052400000 289048K rw--- vmlinux-6.12.47-166-nvidia-gpu\n00007b3064000000  52388K rw---   [ anon ]\n00007b3067329000  13148K -----   [ anon ]\n00007b306a000000  16384K rw-s-   [ anon ]\n00007b306c000000    132K rw---   [ anon ]\n00007b306c021000  65404K -----   [ anon ]\n00007b3072000000  16384K rw-s-   [ anon ]\n00007b3073fff000 10485760K rw-s- qemu_back_mem.dimm1.NFzjyN (deleted)\n00007b32f3fff000      4K -----   [ anon ]\n00007b32f4000000    132K rw---   [ anon ]\n00007b32f4021000  65404K -----   [ anon ]\n00007b32f97ff000      4K -----   [ anon ]\n00007b32f9800000   8192K rw---   [ anon ]\n00007b32fa000000      4K rw---   [ anon ]\n00007b32fa001000      4K -----   [ anon ]\n00007b32fa200000     64K rw---   [ anon ]\n00007b32fa210000      4K -----   [ anon ]\n00007b32fa400000   2048K rw---   [ anon ]\n00007b32fa600000      4K -----   [ anon ]\n00007b32fa800000    256K rw---   [ anon ]\n00007b32fa840000      4K -----   [ anon ]\n00007b32faa0a000    924K rw---   [ anon ]\n00007b32fac00000    128K rw---   [ anon ]\n00007b32fac20000      4K -----   [ anon ]\n00007b32fac64000   1648K rw---   [ anon ]\n00007b32fae00000    256K rw---   [ anon ]\n00007b32fae40000      4K -----   [ anon ]\n00007b32faef9000      4K -----   [ anon ]\n00007b32faefa000   8192K rw---   [ anon ]\n00007b32fb6fa000      4K -----   [ anon ]\n00007b32fb6fb000   8192K rw---   [ anon ]\n00007b32fbefb000   1040K rw---   [ anon ]\n00007b32fbfff000      4K -----   [ anon ]\n00007b32fc000000   8192K rw---   [ anon ]\n00007b32fc800000   3020K r---- qemu-system-x86_64\n00007b32fcaf3000   7144K r-x-- qemu-system-x86_64\n00007b32fd1ed000   3804K r---- qemu-system-x86_64\n00007b32fd5a4000  12140K r---- qemu-system-x86_64\n00007b32fe17f000    892K rw--- qemu-system-x86_64\n00007b32fe25e000     64K rw---   [ anon ]\n00007b32fe2a6000     12K rw-s-   [ anon ]\n00007b32fe2a9000     12K rw-s-   [ anon ]\n00007b32fe2ac000      4K rw-s- zero (deleted)\n00007b32fe2ad000    520K rw---   [ anon ]\n00007b32fe32f000      4K -----   [ anon ]\n00007b32fe330000   1024K rw---   [ anon ]\n00007ffc432c8000    132K rw---   [ stack ]\n00007ffc4335a000     16K r----   [ anon ]\n00007ffc4335e000      8K r-x--   [ anon ]\nffffffffff600000      4K --x--   [ anon ]\ntotal       1056085680K\n</code></pre> pmap extended format Text Only<pre><code># pmap 695044 -X\n695044:   /opt/kata/bin/qemu-system-x86_64 -name sandbox-7e02176c05039fe1b548e19b4740185b85dd41d64d53de2516f3e499eb171d63,debug-threads=on -uuid 0187de4d-dde5-4903-8df3-c58c3157cff5 -machine q35,accel=kvm -cpu host,pmu=off -qmp unix:fd=3,server=on,wait=off -m 10240M,slots=10,maxmem=2060538M -device pci-bridge,bus=pcie.0,id=pci-bridge-0,chassis_nr=1,shpc=off,addr=2,io-reserve=4k,mem-reserve=1m,pref64-reserve=1m -device virtio-serial-pci,disable-modern=false,id=serial0 -device virtconsole,chardev=charconsole0,id=console0 -chardev socket,id=charconsole0,path=/run/vc/vm/7e02176c05039fe1b548e19b4740185b85dd41d64d53de2516f3e499eb171d63/console.sock,server=on,wait=off -device virtio-scsi-pci,id=scsi0,disable-modern=false -object rng-random,id=rng0,filename=/dev/urandom -device virtio-rng-pci,rng=rng0 -device pcie-root-port,id=rp0,bus=pcie.0,chassis=0,slot=0,multifunction=off,pref64-reserve=274877906944B,mem-reserve=4194304B -device pcie-root-port,id=rp1,bus=pcie.0,chassis=0,slot=1,multifunction=off,pref64-reserve=274877906944B,mem-reserve=4194304B -device pcie-root-port,id=rp2,bus=pcie.0,chassis=0,slot=2,multifunction=off,pref64-reserve=274877906944B,mem-reserve=4194304B -device pcie-root-port,id=rp3,bus=pcie.0,chassis=0,slot=3,multifunction=off,pref64-reserve=274877906944B,mem-reserve=4194304B -device pcie-root-port,id=rp4,bus=pcie.0,chassis=0,slot=4,multifunction=off,pref64-reserve=274877906944B,mem-reserve=4194304B -device pcie-root-port,id=rp5,bus=pcie.0,chassis=0,slot=5,multifunction=off,pref64-reserve=274877906944B,mem-reserve=4194304B -device pcie-root-port,id=rp6,bus=pcie.0,chassis=0,slot=6,multifunction=off,pref64-reserve=274877906944B,mem-reserve=4194304B -device pcie-root-port,id=rp7,bus=pcie.0,chassis=0,slot=7,multifunction=off,pref64-reserve=274877906944B,mem-reserve=4194304B -device pcie-root-port,id=rp8,bus=pcie.0,chassis=0,slot=8,multifunction=off,pref64-reserve=274877906944B,mem-reserve=4194304B -device pcie-root-port,id=rp9,bus=pcie.0,chassis=0,slot=9,multifunction=off,pref64-reserve=274877906944B,mem-reserve=4194304B -device vhost-vsock-pci,disable-modern=false,vhostfd=4,id=vsock-521422038,guest-cid=521422038 -chardev socket,id=char-9891c1fc58405e2d,path=/run/vc/vm/7e02176c05039fe1b548e19b4740185b85dd41d64d53de2516f3e499eb171d63/vhost-fs.sock -device vhost-user-fs-pci,chardev=char-9891c1fc58405e2d,tag=kataShared,queue-size=1024 -netdev tap,id=network-0,vhost=on,vhostfds=5,fds=6 -device driver=virtio-net-pci,netdev=network-0,mac=be:67:c5:11:20:26,disable-modern=false,mq=on,vectors=4 -rtc base=utc,driftfix=slew,clock=host -global kvm-pit.lost_tick_policy=discard -vga none -no-user-config -nodefaults -nographic --no-reboot -object memory-backend-file,id=dimm1,size=10240M,mem-path=/dev/shm,share=on -numa node,memdev=dimm1 -kernel /opt/kata/share/kata-containers/vmlinux-6.12.47-166-nvidia-gpu -initrd /opt/kata/share/kata-containers/kata-ubuntu-noble-nvidia-gpu-580.65.06.initrd -append tsc=reliable no_timer_check rcupdate.rcu_expedited=1 i8042.direct=1 i8042.dumbkbd=1 i8042.nopnp=1 i8042.noaux=1 noreplace-smp reboot=k cryptomgr.notests net.ifnames=0 pci=lastbus=0 console=hvc0 console=hvc1 debug panic=1 nr_cpus=224 selinux=0 scsi_mod.scan=none agent.log=debug agent.debug_console agent.debug_console_vport=1026 agent.debug_console agent.debug_console_vport=1026 agent.hotplug_timeout=20 cgroup_no_v1=all -pidfile /run/vc/vm/7e02176c05039fe1b548e19b4740185b85dd41d64d53de2516f3e499eb171d63/pid -smp 1,cores=1,threads=1,sockets=224,maxcpus=224\n        Address Perm       Offset Device   Inode       Size       Rss       Pss Pss_Dirty Referenced Anonymous  KSM LazyFree ShmemPmdMapped FilePmdMapped Shared_Hugetlb Private_Hugetlb Swap SwapPss Locked THPeligible ProtectionKey Mapping\n    555585386000 rw-p     00000000  00:00       0     122196    105904    105904    105904     105904    105904    0        0              0             0              0               0    0       0      0           0             0 [heap]\n    711ebffff000 rw-s     00000000  00:1c     252  104857600 104857600 104857600 104857600  104857600         0    0        0              0             0              0               0    0       0      0           0             0 qemu_back_mem.mem0.mvNUmy (deleted)\n    7137bffff000 ---p     00000000  00:00       0          4         0         0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 \n    7137c0000000 rw-s 57a000000000  00:0f    1050  134217728         0         0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 anon_inode:[vfio-device]\n    715800000000 rw-s 53a000000000  00:0f    1050  134217728         0         0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 anon_inode:[vfio-device]\n    717840000000 rw-s 4fa000000000  00:0f    1050  134217728         0         0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 anon_inode:[vfio-device]\n    719880000000 rw-s 43a000000000  00:0f    1050  134217728         0         0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 anon_inode:[vfio-device]\n    71b8c0000000 rw-s 3fa000000000  00:0f    1050  134217728         0         0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 anon_inode:[vfio-device]\n    71d900000000 rw-s 3da000000000  00:0f    1050  134217728         0         0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 anon_inode:[vfio-device]\n    71f940000000 rw-s 3ba000000000  00:0f    1050  134217728         0         0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 anon_inode:[vfio-device]\n    72194c000000 rw-p     00000000  00:00       0        132        52        52        52         52        52    0        0              0             0              0               0    0       0      0           0             0 \n    72194c021000 ---p     00000000  00:00       0      65404         0         0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 \n    721952000000 rw-s 57e040000000  00:0f    1050      32768         0         0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 anon_inode:[vfio-device]\n    721956000000 rw-s 57e042000000  00:0f    1050      16384         0         0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 anon_inode:[vfio-device]\n    721958000000 rw-s 53e040000000  00:0f    1050      32768         0         0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 anon_inode:[vfio-device]\n    72195c000000 rw-s 53e042000000  00:0f    1050      16384         0         0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 anon_inode:[vfio-device]\n    72195e000000 rw-s 4fe040000000  00:0f    1050      32768         0         0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 anon_inode:[vfio-device]\n    721962000000 rw-s 4fe042000000  00:0f    1050      16384         0         0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 anon_inode:[vfio-device]\n    721964000000 rw-s 43e040000000  00:0f    1050      32768         0         0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 anon_inode:[vfio-device]\n    721968000000 rw-s 43e042000000  00:0f    1050      16384         0         0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 anon_inode:[vfio-device]\n    72196a000000 rw-s 3fe040000000  00:0f    1050      32768         0         0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 anon_inode:[vfio-device]\n    72196e000000 rw-s 3fe042000000  00:0f    1050      16384         0         0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 anon_inode:[vfio-device]\n    721970000000 rw-s 3de040000000  00:0f    1050      32768         0         0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 anon_inode:[vfio-device]\n    721974000000 rw-s 3be040000000  00:0f    1050      32768         0         0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 anon_inode:[vfio-device]\n    721976200000 r--p     00000000 103:1f 6553611     132788    132788    132788         0     132788         0    0        0              0             0              0               0    0       0      0           0             0 kata-ubuntu-noble-nvidia-gpu-580.65.06.initrd\n    72197e400000 rw-p     00000000 103:1f 6553612     289048     20864     20864         0      20864         0    0        0              0             0              0               0    0       0      0           0             0 vmlinux-6.12.47-166-nvidia-gpu\n    721990000000 rw-p     00000000  00:00       0      52392      2060      2060      2060       2060      2060    0        0              0             0              0               0    0       0      0           0             0 \n    72199332a000 ---p     00000000  00:00       0      13144         0         0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 \n    721996000000 rw-s 3de042000000  00:0f    1050      16384         0         0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 anon_inode:[vfio-device]\n    721998000000 rw-p     00000000  00:00       0        132        64        64        64         64        64    0        0              0             0              0               0    0       0      0           0             0 \n    721998021000 ---p     00000000  00:00       0      65404         0         0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 \n    72199e000000 rw-s 3be042000000  00:0f    1050      16384         0         0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 anon_inode:[vfio-device]\n    72199ffff000 rw-s     00000000  00:1c     251   10485760  10485760  10475768  10475768   10485760         0    0        0              0             0              0               0    0       0      0           0             0 qemu_back_mem.dimm1.R0EWjI (deleted)\n    721c1ffff000 ---p     00000000  00:00       0          4         0         0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 \n    721c20000000 rw-p     00000000  00:00       0        132        12        12        12         12        12    0        0              0             0              0               0    0       0      0           0             0 \n    721c20021000 ---p     00000000  00:00       0      65404         0         0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 \n    721c245ff000 ---p     00000000  00:00       0          4         0         0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 \n    721c24600000 rw-p     00000000  00:00       0       8192        12        12        12         12        12    0        0              0             0              0               0    0       0      0           0             0 \n    721c24e00000 rw-p     00000000  00:00       0          4         4         4         4          4         4    0        0              0             0              0               0    0       0      0           0             0 \n    721c24e01000 ---p     00000000  00:00       0          4         0         0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 \n    721c25000000 rw-p     00000000  00:00       0         64        64         4         4         64        64   60        0              0             0              0               0    0       0      0           0             0 \n    721c25010000 ---p     00000000  00:00       0          4         0         0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 \n    721c25200000 rw-p     00000000  00:00       0       2048      2048        54        54       2048      2048 2004        0              0             0              0               0    0       0      0           0             0 \n    721c25400000 ---p     00000000  00:00       0          4         0         0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 \n    721c25600000 rw-p     00000000  00:00       0        256       160       160       160        160       160    0        0              0             0              0               0    0       0      0           0             0 \n    721c25640000 ---p     00000000  00:00       0          4         0         0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 \n    721c2580a000 rw-p     00000000  00:00       0        924       924       924       924        924       924    0        0              0             0              0               0    0       0      0           0             0 \n    721c25a00000 rw-p     00000000  00:00       0        128         0         0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 \n    721c25a20000 ---p     00000000  00:00       0          4         0         0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 \n    721c25c00000 rw-p     00000000  00:00       0        256       256       256       256        256       256    0        0              0             0              0               0    0       0      0           0             0 \n    721c25c40000 ---p     00000000  00:00       0          4         0         0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 \n    721c25c5c000 rw-p     00000000  00:00       0       1648      1648      1648      1648       1648      1648    0        0              0             0              0               0    0       0      0           0             0 \n    721c25e39000 ---p     00000000  00:00       0          4         0         0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 \n    721c25e3a000 rw-p     00000000  00:00       0       8192        32        32        32         32        32    0        0              0             0              0               0    0       0      0           0             0 \n    721c2663a000 ---p     00000000  00:00       0          4         0         0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 \n    721c2663b000 rw-p     00000000  00:00       0       8192        12        12        12         12        12    0        0              0             0              0               0    0       0      0           0             0 \n    721c26e3b000 rw-p     00000000  00:00       0        780       780       780       780        780       780    0        0              0             0              0               0    0       0      0           0             0 \n    721c26efe000 ---p     00000000  00:00       0          4         0         0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 \n    721c26eff000 rw-p     00000000  00:00       0       1024         4         4         4          4         4    0        0              0             0              0               0    0       0      0           0             0 \n    721c26fff000 ---p     00000000  00:00       0          4         0         0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 \n    721c27000000 rw-p     00000000  00:00       0       8192         8         8         8          8         8    0        0              0             0              0               0    0       0      0           0             0 \n    721c27800000 r--p     00000000 103:1f 6553641       3020      3020      3020         0       3020         0    0        0              0             0              0               0    0       0      0           0             0 qemu-system-x86_64\n    721c27af3000 r-xp     002f3000 103:1f 6553641       7144      6776      6776         0       6776         0    0        0              0             0              0               0    0       0      0           0             0 qemu-system-x86_64\n    721c281ed000 r--p     009ed000 103:1f 6553641       3804      2012      2012         0       2012         0    0        0              0             0              0               0    0       0      0           0             0 qemu-system-x86_64\n    721c285a4000 r--p     00da3000 103:1f 6553641      12140      1560      1560      1560       1560      1560    0        0              0             0              0               0    0       0      0           0             0 qemu-system-x86_64\n    721c2917f000 rw-p     0197e000 103:1f 6553641        892       892       892       888        892       888    0        0              0             0              0               0    0       0      0           0             0 qemu-system-x86_64\n    721c2925e000 rw-p     00000000  00:00       0         64        48        48        48         48        48    0        0              0             0              0               0    0       0      0           0             0 \n    721c2929b000 rw-s     00000000  00:0f    1050         12         8         8         8          8         0    0        0              0             0              0               0    0       0      0           0             0 anon_inode:kvm-vcpu:223\n    721c2929e000 rw-s     00000000  00:0f    1050         12        12        12        12         12         0    0        0              0             0              0               0    0       0      0           0             0 anon_inode:kvm-vcpu:0\n    721c292a1000 rw-s     00000000  00:01   90199          4         4         4         4          4         0    0        0              0             0              0               0    0       0      0           0             0 zero (deleted)\n    721c292a2000 rw-p     00000000  00:00       0        780       780       780       780        780       780    0        0              0             0              0               0    0       0      0           0             0 \n    7ffd7248b000 rw-p     00000000  00:00       0        132        40        40        40         40        40    0        0              0             0              0               0    0       0      0           0             0 [stack]\n    7ffd7254c000 r--p     00000000  00:00       0         16         0         0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 [vvar]\n    7ffd72550000 r-xp     00000000  00:00       0          8         4         0         0          4         0    0        0              0             0              0               0    0       0      0           0             0 [vdso]\nffffffffff600000 --xp     00000000  00:00       0          4         0         0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 [vsyscall]\n                                                ========== ========= ========= ========= ========== ========= ==== ======== ============== ============= ============== =============== ==== ======= ====== =========== ============= \n                                                1056085680 115626212 115614162 115448698  115626212    117360 2064        0              0             0              0               0    0       0      0           0             0 KB \n</code></pre>"},{"location":"blog/2025/11/19/boot-time-performance-of-kata-containers-with-nvidia-gpus/#turned-on","title":"Turned On","text":"<p>Let's now do the same measurements but with hugepages being granted to the QEMU process. We only make a few minor modifications:</p> Diff<pre><code>7c7\n&lt;     #io.katacontainers.config.hypervisor.enable_hugepages: \"true\"\n---\n&gt;     io.katacontainers.config.hypervisor.enable_hugepages: \"true\"\n20c20\n&lt;         #hugepages-1Gi: 125Gi\n---\n&gt;         hugepages-1Gi: 125Gi\n25c25\n&lt;         #hugepages-1Gi: 125Gi\n---\n&gt;         hugepages-1Gi: 125Gi\n</code></pre> pod.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: cdi-vfio-gpu0-test-160\n  annotations:\n    io.katacontainers.config.hypervisor.default_memory: \"10240\"\n    io.katacontainers.config.hypervisor.enable_hugepages: \"true\"\nspec:\n  runtimeClassName: kata-qemu-nvidia-gpu\n  restartPolicy: Never\n  nodeName: dal03-sgpu-160\n  containers:\n  - name: check\n    image: ubuntu:22.04\n    resources:\n      limits:\n        nvidia.com/gpu: 7\n        cpu: 500m\n        memory: 100Gi\n        hugepages-1Gi: 125Gi\n      requests:\n        nvidia.com/gpu: 7\n        cpu: 500m\n        memory: 100Gi\n        hugepages-1Gi: 125Gi\n    command:\n      - /bin/bash\n      - -lc\n      - sleep 9999\n</code></pre> <p>We submit the pod and do the same measurements:</p> Text Only<pre><code># kubectl describe pod cdi-vfio-gpu0-test-160 |&amp; grep 'Nov 2025'\nStart Time:          Mon, 24 Nov 2025 21:50:22 +0000\n      Started:      Mon, 24 Nov 2025 21:52:48 +0000\n</code></pre> <p>That's 2:26 minutes, 37 seconds faster than before. Our memory statistics are also much better:</p> <p> </p> <p>Something a little more subtle, but still worth noting, is that both the secondary page tables (used in things like Extended Page Tables) and the slab memory (used for grouping data types together in memory) are both far reduced. More noteworthy is the fact that our <code>Memory Total Bytes</code> is strangely around 3GiB. Why would that be? Well, when we allocated the hugepages before the pod was spun up, that memory utilization becomes persistent and is counted against the \"free\" memory of the system. You'll note that a command like <code>free</code> will count those pre-allocated hugepages against the <code>used</code> column. For whatever reason, the cgroup system does not consider the resident set size of the RAM backing a hugepage to be part of a process's utilization. I'm sure there is a good reason for it.</p> <p>Let's look at this process's memory map:</p> pmap Text Only<pre><code>pmap 686403 \n686403:   /opt/kata/bin/qemu-system-x86_64 -name sandbox-3f219f48134f8da7b5faa009eccaafd50eef417c5e2d6ea4ed0f86c7ddacefaf,debug-threads=on -uuid de67ce82-f347-4ec3-aafb-90a16666e57b -machine q35,accel=kvm -cpu host,pmu=off -qmp unix:fd=3,server=on,wait=off -m 10240M,slots=10,maxmem=2060538M -device pci-bridge,bus=pcie.0,id=pci-bridge-0,chassis_nr=1,shpc=off,addr=2,io-reserve=4k,mem-reserve=1m,pref64-reserve=1m -device virtio-serial-pci,disable-modern=false,id=serial0 -device virtconsole,chardev=charconsole0,id=console0 -chardev socket,id=charconsole0,path=/run/vc/vm/3f219f48134f8da7b5faa009eccaafd50eef417c5e2d6ea4ed0f86c7ddacefaf/console.sock,server=on,wait=off -device virtio-scsi-pci,id=scsi0,disable-modern=false -object rng-random,id=rng0,filename=/dev/urandom -device virtio-rng-pci,rng=rng0 -device pcie-root-port,id=rp0,bus=pcie.0,chassis=0,slot=0,multifunction=off,pref64-reserve=274877906944B,mem-reserve=4194304B -device pcie-root-port,id=rp1,bus=pcie.0,chassis=0,slot=1,multifunction=off,pref64-reserve=274877906944B,mem-reserve=4194304B -device pcie-root-port,id=rp2,bus=pcie.0,chassis=0,slot=2,multifunction=off,pref64-reserve=274877906944B,mem-reserve=4194304B -device pcie-root-port,id=rp3,bus=pcie.0,chassis=0,slot=3,multifunction=off,pref64-reserve=274877906944B,mem-reserve=4194304B -device pcie-root-port,id=rp4,bus=pcie.0,chassis=0,slot=4,multifunction=off,pref64-reserve=274877906944B,mem-reserve=4194304B -device pcie-root-port,id=rp5,bus=pcie.0,chassis=0,slot=5,multifunction=off,pref64-reserve=274877906944B,mem-reserve=4194304B -device pcie-root-port,id=rp6,bus=pcie.0,chassis=0,slot=6,multifunction=off,pref64-reserve=274877906944B,mem-reserve=4194304B -device pcie-root-port,id=rp7,bus=pcie.0,chassis=0,slot=7,multifunction=off,pref64-reserve=274877906944B,mem-reserve=4194304B -device pcie-root-port,id=rp8,bus=pcie.0,chassis=0,slot=8,multifunction=off,pref64-reserve=274877906944B,mem-reserve=4194304B -device pcie-root-port,id=rp9,bus=pcie.0,chassis=0,slot=9,multifunction=off,pref64-reserve=274877906944B,mem-reserve=4194304B -device vhost-vsock-pci,disable-modern=false,vhostfd=4,id=vsock-3257660399,guest-cid=3257660399 -chardev socket,id=char-c3493397927c7b53,path=/run/vc/vm/3f219f48134f8da7b5faa009eccaafd50eef417c5e2d6ea4ed0f86c7ddacefaf/vhost-fs.sock -device vhost-user-fs-pci,chardev=char-c3493397927c7b53,tag=kataShared,queue-size=1024 -netdev tap,id=network-0,vhost=on,vhostfds=5,fds=6 -device driver=virtio-net-pci,netdev=network-0,mac=f6:c9:df:61:71:4b,disable-modern=false,mq=on,vectors=4 -rtc base=utc,driftfix=slew,clock=host -global kvm-pit.lost_tick_policy=discard -vga none -no-user-config -nodefaults -nographic --no-reboot -object memory-backend-file,id=dimm1,size=10240M,mem-path=/dev/hugepages,share=on,prealloc=on -numa node,memdev=dimm1 -kernel /opt/kata/share/kata-containers/vmlinux-6.12.47-166-nvidia-gpu -initrd /opt/kata/share/kata-containers/kata-ubuntu-noble-nvidia-gpu-580.65.06.initrd -append tsc=reliable no_timer_check rcupdate.rcu_expedited=1 i8042.direct=1 i8042.dumbkbd=1 i8042.nopnp=1 i8042.noaux=1 noreplace-smp reboot=k cryptomgr.notests net.ifnames=0 pci=lastbus=0 console=hvc0 console=hvc1 debug panic=1 nr_cpus=224 selinux=0 scsi_mod.scan=none agent.log=debug agent.debug_console agent.debug_console_vport=1026 agent.debug_console agent.debug_console_vport=1026 agent.hotplug_timeout=20 cgroup_no_v1=all -pidfile /run/vc/vm/3f219f48134f8da7b5faa009eccaafd50eef417c5e2d6ea4ed0f86c7ddacefaf/pid -smp 1,cores=1,threads=1,sockets=224,maxcpus=224\n000055556e427000 122196K rw---   [ anon ]\n0000736480000000 104857600K rw-s- qemu_back_mem.mem0.nWhJ9P (deleted)\n0000737d80000000      4K -----   [ anon ]\n0000737dc0000000 134217728K rw-s-   [ anon ]\n0000739e00000000 134217728K rw-s-   [ anon ]\n000073be40000000 134217728K rw-s-   [ anon ]\n000073de80000000 134217728K rw-s-   [ anon ]\n000073fec0000000 134217728K rw-s-   [ anon ]\n0000741f00000000 134217728K rw-s-   [ anon ]\n0000743f38000000    132K rw---   [ anon ]\n0000743f38021000  65404K -----   [ anon ]\n0000743f40000000 134217728K rw-s-   [ anon ]\n0000745f48000000  32768K rw-s-   [ anon ]\n0000745f4c000000  16384K rw-s-   [ anon ]\n0000745f4e000000  32768K rw-s-   [ anon ]\n0000745f52000000  16384K rw-s-   [ anon ]\n0000745f54000000  32768K rw-s-   [ anon ]\n0000745f58000000  16384K rw-s-   [ anon ]\n0000745f5a000000  32768K rw-s-   [ anon ]\n0000745f5e000000  16384K rw-s-   [ anon ]\n0000745f60000000  32768K rw-s-   [ anon ]\n0000745f64000000  16384K rw-s-   [ anon ]\n0000745f66000000  32768K rw-s-   [ anon ]\n0000745f6a000000  16384K rw-s-   [ anon ]\n0000745f6c000000  32768K rw-s-   [ anon ]\n0000745f6e400000 289048K rw--- vmlinux-6.12.47-166-nvidia-gpu\n0000745f80000000 10485760K rw-s- qemu_back_mem.dimm1.8mPNaQ (deleted)\n0000746200000000      4K -----   [ anon ]\n0000746202000000  16384K rw-s-   [ anon ]\n0000746203e00000 132788K r---- kata-ubuntu-noble-nvidia-gpu-580.65.06.initrd\n000074620c000000    132K rw---   [ anon ]\n000074620c021000  65404K -----   [ anon ]\n0000746210000000  56880K rw---   [ anon ]\n000074621378c000   8656K -----   [ anon ]\n0000746214000000    132K rw---   [ anon ]\n0000746214021000  65404K -----   [ anon ]\n00007462197ff000      4K -----   [ anon ]\n0000746219800000   8192K rw---   [ anon ]\n000074621a000000      4K rw---   [ anon ]\n000074621a001000      4K -----   [ anon ]\n000074621a200000     64K rw---   [ anon ]\n000074621a210000      4K -----   [ anon ]\n000074621a400000   2048K rw---   [ anon ]\n000074621a600000      4K -----   [ anon ]\n000074621a800000    256K rw---   [ anon ]\n000074621a840000      4K -----   [ anon ]\n000074621aa0a000    924K rw---   [ anon ]\n000074621ac00000    128K rw---   [ anon ]\n000074621ac20000      4K -----   [ anon ]\n000074621ac64000   1648K rw---   [ anon ]\n000074621ae00000    256K rw---   [ anon ]\n000074621ae40000      4K -----   [ anon ]\n000074621af3a000      4K -----   [ anon ]\n000074621af3b000   8192K rw---   [ anon ]\n000074621b73b000      4K -----   [ anon ]\n000074621b73c000   8192K rw---   [ anon ]\n000074621bf3c000    780K rw---   [ anon ]\n000074621bfff000      4K -----   [ anon ]\n000074621c000000   8192K rw---   [ anon ]\n000074621c800000   3020K r---- qemu-system-x86_64\n000074621caf3000   7144K r-x-- qemu-system-x86_64\n000074621d1ed000   3804K r---- qemu-system-x86_64\n000074621d5a4000  12140K r---- qemu-system-x86_64\n000074621e17f000    892K rw--- qemu-system-x86_64\n000074621e25e000     64K rw---   [ anon ]\n000074621e274000     12K rw-s-   [ anon ]\n000074621e277000     12K rw-s-   [ anon ]\n000074621e27a000      4K rw-s- zero (deleted)\n000074621e27b000    780K rw---   [ anon ]\n000074621e33e000      4K -----   [ anon ]\n000074621e33f000   1024K rw---   [ anon ]\n00007fffd39f2000    136K rw---   [ stack ]\n00007fffd3b9a000     16K r----   [ anon ]\n00007fffd3b9e000      8K r-x--   [ anon ]\nffffffffff600000      4K --x--   [ anon ]\ntotal       1056085684K\n</code></pre> pmap extended format Text Only<pre><code>pmap 686403 -X\n686403:   /opt/kata/bin/qemu-system-x86_64 -name sandbox-3f219f48134f8da7b5faa009eccaafd50eef417c5e2d6ea4ed0f86c7ddacefaf,debug-threads=on -uuid de67ce82-f347-4ec3-aafb-90a16666e57b -machine q35,accel=kvm -cpu host,pmu=off -qmp unix:fd=3,server=on,wait=off -m 10240M,slots=10,maxmem=2060538M -device pci-bridge,bus=pcie.0,id=pci-bridge-0,chassis_nr=1,shpc=off,addr=2,io-reserve=4k,mem-reserve=1m,pref64-reserve=1m -device virtio-serial-pci,disable-modern=false,id=serial0 -device virtconsole,chardev=charconsole0,id=console0 -chardev socket,id=charconsole0,path=/run/vc/vm/3f219f48134f8da7b5faa009eccaafd50eef417c5e2d6ea4ed0f86c7ddacefaf/console.sock,server=on,wait=off -device virtio-scsi-pci,id=scsi0,disable-modern=false -object rng-random,id=rng0,filename=/dev/urandom -device virtio-rng-pci,rng=rng0 -device pcie-root-port,id=rp0,bus=pcie.0,chassis=0,slot=0,multifunction=off,pref64-reserve=274877906944B,mem-reserve=4194304B -device pcie-root-port,id=rp1,bus=pcie.0,chassis=0,slot=1,multifunction=off,pref64-reserve=274877906944B,mem-reserve=4194304B -device pcie-root-port,id=rp2,bus=pcie.0,chassis=0,slot=2,multifunction=off,pref64-reserve=274877906944B,mem-reserve=4194304B -device pcie-root-port,id=rp3,bus=pcie.0,chassis=0,slot=3,multifunction=off,pref64-reserve=274877906944B,mem-reserve=4194304B -device pcie-root-port,id=rp4,bus=pcie.0,chassis=0,slot=4,multifunction=off,pref64-reserve=274877906944B,mem-reserve=4194304B -device pcie-root-port,id=rp5,bus=pcie.0,chassis=0,slot=5,multifunction=off,pref64-reserve=274877906944B,mem-reserve=4194304B -device pcie-root-port,id=rp6,bus=pcie.0,chassis=0,slot=6,multifunction=off,pref64-reserve=274877906944B,mem-reserve=4194304B -device pcie-root-port,id=rp7,bus=pcie.0,chassis=0,slot=7,multifunction=off,pref64-reserve=274877906944B,mem-reserve=4194304B -device pcie-root-port,id=rp8,bus=pcie.0,chassis=0,slot=8,multifunction=off,pref64-reserve=274877906944B,mem-reserve=4194304B -device pcie-root-port,id=rp9,bus=pcie.0,chassis=0,slot=9,multifunction=off,pref64-reserve=274877906944B,mem-reserve=4194304B -device vhost-vsock-pci,disable-modern=false,vhostfd=4,id=vsock-3257660399,guest-cid=3257660399 -chardev socket,id=char-c3493397927c7b53,path=/run/vc/vm/3f219f48134f8da7b5faa009eccaafd50eef417c5e2d6ea4ed0f86c7ddacefaf/vhost-fs.sock -device vhost-user-fs-pci,chardev=char-c3493397927c7b53,tag=kataShared,queue-size=1024 -netdev tap,id=network-0,vhost=on,vhostfds=5,fds=6 -device driver=virtio-net-pci,netdev=network-0,mac=f6:c9:df:61:71:4b,disable-modern=false,mq=on,vectors=4 -rtc base=utc,driftfix=slew,clock=host -global kvm-pit.lost_tick_policy=discard -vga none -no-user-config -nodefaults -nographic --no-reboot -object memory-backend-file,id=dimm1,size=10240M,mem-path=/dev/hugepages,share=on,prealloc=on -numa node,memdev=dimm1 -kernel /opt/kata/share/kata-containers/vmlinux-6.12.47-166-nvidia-gpu -initrd /opt/kata/share/kata-containers/kata-ubuntu-noble-nvidia-gpu-580.65.06.initrd -append tsc=reliable no_timer_check rcupdate.rcu_expedited=1 i8042.direct=1 i8042.dumbkbd=1 i8042.nopnp=1 i8042.noaux=1 noreplace-smp reboot=k cryptomgr.notests net.ifnames=0 pci=lastbus=0 console=hvc0 console=hvc1 debug panic=1 nr_cpus=224 selinux=0 scsi_mod.scan=none agent.log=debug agent.debug_console agent.debug_console_vport=1026 agent.debug_console agent.debug_console_vport=1026 agent.hotplug_timeout=20 cgroup_no_v1=all -pidfile /run/vc/vm/3f219f48134f8da7b5faa009eccaafd50eef417c5e2d6ea4ed0f86c7ddacefaf/pid -smp 1,cores=1,threads=1,sockets=224,maxcpus=224\n        Address Perm       Offset Device     Inode       Size    Rss    Pss Pss_Dirty Referenced Anonymous  KSM LazyFree ShmemPmdMapped FilePmdMapped Shared_Hugetlb Private_Hugetlb Swap SwapPss Locked THPeligible ProtectionKey Mapping\n    55556e427000 rw-p     00000000  00:00         0     122196 105836 105836    105836     105836    105836    0        0              0             0              0               0    0       0      0           0             0 [heap]\n    736480000000 rw-s     00000000  00:22 192796858  104857600      0      0         0          0         0    0        0              0             0              0       104857600    0       0      0           0             0 qemu_back_mem.mem0.nWhJ9P (deleted)\n    737d80000000 ---p     00000000  00:00         0          4      0      0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 \n    737dc0000000 rw-s 43a000000000  00:0f      1050  134217728      0      0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 anon_inode:[vfio-device]\n    739e00000000 rw-s 3fa000000000  00:0f      1050  134217728      0      0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 anon_inode:[vfio-device]\n    73be40000000 rw-s 3da000000000  00:0f      1050  134217728      0      0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 anon_inode:[vfio-device]\n    73de80000000 rw-s 3ba000000000  00:0f      1050  134217728      0      0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 anon_inode:[vfio-device]\n    73fec0000000 rw-s 57a000000000  00:0f      1050  134217728      0      0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 anon_inode:[vfio-device]\n    741f00000000 rw-s 53a000000000  00:0f      1050  134217728      0      0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 anon_inode:[vfio-device]\n    743f38000000 rw-p     00000000  00:00         0        132     52     52        52         52        52    0        0              0             0              0               0    0       0      0           0             0 \n    743f38021000 ---p     00000000  00:00         0      65404      0      0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 \n    743f40000000 rw-s 4fa000000000  00:0f      1050  134217728      0      0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 anon_inode:[vfio-device]\n    745f48000000 rw-s 43e040000000  00:0f      1050      32768      0      0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 anon_inode:[vfio-device]\n    745f4c000000 rw-s 43e042000000  00:0f      1050      16384      0      0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 anon_inode:[vfio-device]\n    745f4e000000 rw-s 3fe040000000  00:0f      1050      32768      0      0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 anon_inode:[vfio-device]\n    745f52000000 rw-s 3fe042000000  00:0f      1050      16384      0      0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 anon_inode:[vfio-device]\n    745f54000000 rw-s 3de040000000  00:0f      1050      32768      0      0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 anon_inode:[vfio-device]\n    745f58000000 rw-s 3de042000000  00:0f      1050      16384      0      0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 anon_inode:[vfio-device]\n    745f5a000000 rw-s 3be040000000  00:0f      1050      32768      0      0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 anon_inode:[vfio-device]\n    745f5e000000 rw-s 3be042000000  00:0f      1050      16384      0      0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 anon_inode:[vfio-device]\n    745f60000000 rw-s 57e040000000  00:0f      1050      32768      0      0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 anon_inode:[vfio-device]\n    745f64000000 rw-s 57e042000000  00:0f      1050      16384      0      0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 anon_inode:[vfio-device]\n    745f66000000 rw-s 53e040000000  00:0f      1050      32768      0      0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 anon_inode:[vfio-device]\n    745f6a000000 rw-s 53e042000000  00:0f      1050      16384      0      0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 anon_inode:[vfio-device]\n    745f6c000000 rw-s 4fe040000000  00:0f      1050      32768      0      0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 anon_inode:[vfio-device]\n    745f6e400000 rw-p     00000000 103:1f   6553612     289048  20864  20864         0      20864         0    0        0              0             0              0               0    0       0      0           0             0 vmlinux-6.12.47-166-nvidia-gpu\n    745f80000000 rw-s     00000000  00:22 192846868   10485760      0      0         0          0         0    0        0              0             0        3145728         7340032    0       0      0           0             0 qemu_back_mem.dimm1.8mPNaQ (deleted)\n    746200000000 ---p     00000000  00:00         0          4      0      0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 \n    746202000000 rw-s 4fe042000000  00:0f      1050      16384      0      0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 anon_inode:[vfio-device]\n    746203e00000 r--p     00000000 103:1f   6553611     132788 132788 132788         0     132788         0    0        0              0             0              0               0    0       0      0           0             0 kata-ubuntu-noble-nvidia-gpu-580.65.06.initrd\n    74620c000000 rw-p     00000000  00:00         0        132     68     68        68         68        68    0        0              0             0              0               0    0       0      0           0             0 \n    74620c021000 ---p     00000000  00:00         0      65404      0      0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 \n    746210000000 rw-p     00000000  00:00         0      56880   2228   2228      2228       2228      2228    0        0              0             0              0               0    0       0      0           0             0 \n    74621378c000 ---p     00000000  00:00         0       8656      0      0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 \n    746214000000 rw-p     00000000  00:00         0        132     12     12        12         12        12    0        0              0             0              0               0    0       0      0           0             0 \n    746214021000 ---p     00000000  00:00         0      65404      0      0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 \n    7462197ff000 ---p     00000000  00:00         0          4      0      0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 \n    746219800000 rw-p     00000000  00:00         0       8192     12     12        12         12        12    0        0              0             0              0               0    0       0      0           0             0 \n    74621a000000 rw-p     00000000  00:00         0          4      4      4         4          4         4    0        0              0             0              0               0    0       0      0           0             0 \n    74621a001000 ---p     00000000  00:00         0          4      0      0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 \n    74621a200000 rw-p     00000000  00:00         0         64     64      4         4         64        64   60        0              0             0              0               0    0       0      0           0             0 \n    74621a210000 ---p     00000000  00:00         0          4      0      0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 \n    74621a400000 rw-p     00000000  00:00         0       2048   2048     54        54       2048      2048 2004        0              0             0              0               0    0       0      0           0             0 \n    74621a600000 ---p     00000000  00:00         0          4      0      0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 \n    74621a800000 rw-p     00000000  00:00         0        256    160    160       160        160       160    0        0              0             0              0               0    0       0      0           0             0 \n    74621a840000 ---p     00000000  00:00         0          4      0      0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 \n    74621aa0a000 rw-p     00000000  00:00         0        924    924    924       924        924       924    0        0              0             0              0               0    0       0      0           0             0 \n    74621ac00000 rw-p     00000000  00:00         0        128      0      0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 \n    74621ac20000 ---p     00000000  00:00         0          4      0      0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 \n    74621ac64000 rw-p     00000000  00:00         0       1648   1648   1648      1648       1648      1648    0        0              0             0              0               0    0       0      0           0             0 \n    74621ae00000 rw-p     00000000  00:00         0        256    256    256       256        256       256    0        0              0             0              0               0    0       0      0           0             0 \n    74621ae40000 ---p     00000000  00:00         0          4      0      0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 \n    74621af3a000 ---p     00000000  00:00         0          4      0      0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 \n    74621af3b000 rw-p     00000000  00:00         0       8192     32     32        32         32        32    0        0              0             0              0               0    0       0      0           0             0 \n    74621b73b000 ---p     00000000  00:00         0          4      0      0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 \n    74621b73c000 rw-p     00000000  00:00         0       8192     12     12        12         12        12    0        0              0             0              0               0    0       0      0           0             0 \n    74621bf3c000 rw-p     00000000  00:00         0        780    780    780       780        780       780    0        0              0             0              0               0    0       0      0           0             0 \n    74621bfff000 ---p     00000000  00:00         0          4      0      0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 \n    74621c000000 rw-p     00000000  00:00         0       8192      8      8         8          8         8    0        0              0             0              0               0    0       0      0           0             0 \n    74621c800000 r--p     00000000 103:1f   6553641       3020   3020   3020         0       3020         0    0        0              0             0              0               0    0       0      0           0             0 qemu-system-x86_64\n    74621caf3000 r-xp     002f3000 103:1f   6553641       7144   6776   6776         0       6776         0    0        0              0             0              0               0    0       0      0           0             0 qemu-system-x86_64\n    74621d1ed000 r--p     009ed000 103:1f   6553641       3804   2012   2012         0       2012         0    0        0              0             0              0               0    0       0      0           0             0 qemu-system-x86_64\n    74621d5a4000 r--p     00da3000 103:1f   6553641      12140   1560   1560      1560       1560      1560    0        0              0             0              0               0    0       0      0           0             0 qemu-system-x86_64\n    74621e17f000 rw-p     0197e000 103:1f   6553641        892    892    892       888        892       888    0        0              0             0              0               0    0       0      0           0             0 qemu-system-x86_64\n    74621e25e000 rw-p     00000000  00:00         0         64     48     48        48         48        48    0        0              0             0              0               0    0       0      0           0             0 \n    74621e274000 rw-s     00000000  00:0f      1050         12      8      8         8          8         0    0        0              0             0              0               0    0       0      0           0             0 anon_inode:kvm-vcpu:223\n    74621e277000 rw-s     00000000  00:0f      1050         12     12     12        12         12         0    0        0              0             0              0               0    0       0      0           0             0 anon_inode:kvm-vcpu:0\n    74621e27a000 rw-s     00000000  00:01     26803          4      4      4         4          4         0    0        0              0             0              0               0    0       0      0           0             0 zero (deleted)\n    74621e27b000 rw-p     00000000  00:00         0        780    780    780       780        780       780    0        0              0             0              0               0    0       0      0           0             0 \n    74621e33e000 ---p     00000000  00:00         0          4      0      0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 \n    74621e33f000 rw-p     00000000  00:00         0       1024      4      4         4          4         4    0        0              0             0              0               0    0       0      0           0             0 \n    7fffd39f2000 rw-p     00000000  00:00         0        136     40     40        40         40        40    0        0              0             0              0               0    0       0      0           0             0 [stack]\n    7fffd3b9a000 r--p     00000000  00:00         0         16      0      0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 [vvar]\n    7fffd3b9e000 r-xp     00000000  00:00         0          8      4      0         0          4         0    0        0              0             0              0               0    0       0      0           0             0 [vdso]\nffffffffff600000 --xp     00000000  00:00         0          4      0      0         0          0         0    0        0              0             0              0               0    0       0      0           0             0 [vsyscall]\n                                                    ========== ====== ====== ========= ========== ========= ==== ======== ============== ============= ============== =============== ==== ======= ====== =========== ============= \n                                                    1056085684 282956 280898    115434     282956    117464 2064        0              0             0        3145728       112197632    0       0      0           0             0 KB \n</code></pre> <p>In case you didn't believe me that hugepages are turned on, we can drill down and compare the most relevant lines of this memory map. Recall the memory mapping of these <code>qemu_back_mem</code> entries with hugepages turned off:</p> offon Text Only<pre><code>        Address Perm       Offset Device   Inode       Size       Rss       Pss Pss_Dirty Referenced Anonymous  KSM LazyFree ShmemPmdMapped FilePmdMapped Shared_Hugetlb Private_Hugetlb Swap SwapPss Locked THPeligible ProtectionKey Mapping\n\n    711ebffff000 rw-s     00000000  00:1c     252  104857600 104857600 104857600 104857600  104857600         0    0        0              0             0              0               0    0       0      0           0             0 qemu_back_mem.mem0.mvNUmy (deleted)\n    72199ffff000 rw-s     00000000  00:1c     251   10485760  10485760  10475768  10475768   10485760         0    0        0              0             0              0               0    0       0      0           0             0 qemu_back_mem.dimm1.R0EWjI (deleted)\n</code></pre> <p>The <code>{Shared,Private}_Hugetlb</code> lines for these are both zero.</p> Text Only<pre><code>        Address Perm       Offset Device     Inode       Size    Rss    Pss Pss_Dirty Referenced Anonymous  KSM LazyFree ShmemPmdMapped FilePmdMapped Shared_Hugetlb Private_Hugetlb Swap SwapPss Locked THPeligible ProtectionKey Mapping\n\n    736480000000 rw-s     00000000  00:22 192796858  104857600      0      0         0          0         0    0        0              0             0              0       104857600    0       0      0           0             0 qemu_back_mem.mem0.nWhJ9P (deleted)\n    745f80000000 rw-s     00000000  00:22 192846868   10485760      0      0         0          0         0    0        0              0             0        3145728         7340032    0       0      0           0             0 qemu_back_mem.dimm1.8mPNaQ (deleted)\n</code></pre> <p>The Hugetlb columns are now non-zero.</p> <p>Tip</p> <p>If you pass in the <code>-p</code> flag, you can see that these entries are actually files QEMU opened in <code>/dev/shm</code>. For example:</p> Text Only<pre><code>0000711ebffff000 104857600K rw-s- /dev/shm/qemu_back_mem.mem0.mvNUmy (deleted)\n</code></pre>"},{"location":"blog/2025/11/19/boot-time-performance-of-kata-containers-with-nvidia-gpus/#performance-problems-with-vfio-mmio-mappings","title":"Performance Problems with VFIO MMIO Mappings","text":"<p>When I first started the Kata containers exploration, I was running it on Linux kernel 5.15.0. Multi-GPU containers suffered from extremely long boot times, over 30 minutes in the 8-GPU case. Additionally, the boot time seemed to scale almost exponentially the more GPUs were being attached to the VM. I later updated to kernel 6.8.0 at the recommendation of the Kata developers and also switched to the IOMMUFD backend for vfio-pci. After doing this, the boot times were reduced essentially by one order of magnitude. Let me unpack exactly what all of that means...</p> <p>Apparently, older versions of Linux used to page fault every single page that was being IOMMU-mapped, something confirmed by an email I sent to the Linux IOMMU developers: https://lore.kernel.org/linux-iommu/20251122001953.GL233636@ziepe.ca/T/#t</p> Text Only<pre><code>* Explanation of VM Boot Performance Gains with IOMMUFD\n@ 2025-11-17 17:31 Landon Clipp\n  2025-11-22  0:19 ` Jason Gunthorpe\n  0 siblings, 1 reply; 2+ messages in thread\nFrom: Landon Clipp @ 2025-11-17 17:31 UTC (permalink / raw)\n  To: iommu\n\nHi folks,\n\nI have a Kubernetes cluster whereby containers are spawned inside of\nVMs (Kata Containers). These VMs have 8 NVIDIA H100 GPUs (from a\nSupermicro HGX box) passed through with vfio-pci. Passing through the\nGPUs with the legacy VFIO cdev corresponded to boot times of around 30\nminutes, while passing through with the IOMMUFD API brought it down to\naround 3 minutes.\n\nI have asked numerous Linux virtualization experts as to why I am\nseeing this behavior and none have been able to properly answer. I\nwanted to know if there was some salient difference between how\nIOMMUFD and the legacy VFIO API handle IOMMU page mappings that would\naccount for this performance difference.\n\nRegards,\n--\nLandon Clipp\n\n^ permalink raw reply   [flat|nested] 2+ messages in thread\n* Re: Explanation of VM Boot Performance Gains with IOMMUFD\n  2025-11-17 17:31 Explanation of VM Boot Performance Gains with IOMMUFD Landon Clipp\n@ 2025-11-22  0:19 ` Jason Gunthorpe\n  0 siblings, 0 replies; 2+ messages in thread\nFrom: Jason Gunthorpe @ 2025-11-22  0:19 UTC (permalink / raw)\n  To: Landon Clipp; +Cc: iommu\n\nOn Mon, Nov 17, 2025 at 11:31:03AM -0600, Landon Clipp wrote:\n&gt; Hi folks,\n&gt; \n&gt; I have a Kubernetes cluster whereby containers are spawned inside of\n&gt; VMs (Kata Containers). These VMs have 8 NVIDIA H100 GPUs (from a\n&gt; Supermicro HGX box) passed through with vfio-pci. Passing through the\n&gt; GPUs with the legacy VFIO cdev corresponded to boot times of around 30\n&gt; minutes, while passing through with the IOMMUFD API brought it down to\n&gt; around 3 minutes.\n\n30 mins seems so unreasonable, are you sure you aren't hitting one of\nthose kernels that is faulting every GPU page? That was fixed at some\npoint. Though iommufd with the OOT patch would hit the same issue.\n\nI fully expect that when qemu uses a memfd and uses dmabuf iommufd\nwill boot much, much faster that vfio ever could, it has so much less\noverhead.\n\nJason\n\n^ permalink raw reply   [flat|nested] 2+ messages in thread\nend of thread, other threads:[~2025-11-22  0:19 UTC | newest]\n\nThread overview: 2+ messages (download: mbox.gz follow: Atom feed\n-- links below jump to the message on this page --\n2025-11-17 17:31 Explanation of VM Boot Performance Gains with IOMMUFD Landon Clipp\n2025-11-22  0:19 ` Jason Gunthorpe\nThis is a public inbox, see mirroring instructions\nfor how to clone and mirror all data and code used for this inbox;\nas well as URLs for NNTP newsgroup(s).\n</code></pre> <p>This seems to independently agree with what I was seeing internally at the company Lambda that I work for. We had been having issues with the VFIO driver unnecessarily faulting in every page of MMIO-mapped memory. This was mainly because VFIO was originally designed to map device IOVAs to physical RAM, so in this case it obviously makes sense to fault in every page to ensure the page always exists in memory. However when MMIO mappings were being created between host virtual addresses to PCIe device BARs, it incorrectly faulted in every single page. This amounted to walking across 33 million pages for every map or unmap of the MMIO regions for a single H100 GPU!</p> <p>As much as I would love to show you the exact pull request that fixed this, honestly the answer I received second hand is good enough for me. The moral of the story is that you should always use the latest Linux kernel version that you reasonably can. </p>"},{"location":"blog/2025/11/19/boot-time-performance-of-kata-containers-with-nvidia-gpus/#is-iommufd-faster-than-legacy-vfio-backend","title":"Is IOMMUFD Faster Than Legacy VFIO Backend?","text":"<p>To recap, the vfio-pci Linux kernel driver provides a way for userspace programs to act as device drivers. This is relevant when guest VMs need to run NVIDIA kernel drivers for the GPUs, as you'll recall that we attach vfio-pci to the GPU as its driver, then we provide QEMU a handle (such as <code>/dev/vfio/33</code> or whatever the path is) for that device. This gives QEMU access to the VFIO API. QEMU then maps the device BARs it sees into the guest's MMIO region so that the NVIDIA kernel driver inside can attach to it. Yes it is all a very complicated dance, and if you're confused, don't worry. It took me a long time to get this straight in my head.</p> <p>Anyway, the vfio-pci kernel driver had a legacy integration with the Linux IOMMU subsystem that was implemented in <code>drivers/vfio/vfio_iommu_type1.c</code>. This is the legacy IOMMU backend code VFIO used. For a number of reasons that aren't totally relevant (and that are explained here), the VFIO driver was updated to allow users to provide an IOMMUFD file descriptor to the VFIO driver. Instead of interacting with IOMMU groups (as done in the legacy VFIO backend), users have access to IOMMUFD file descriptors that represent single PCIe devices.</p> <p>It was previously suggested to me that using IOMMUFD would give me large performance benefits when IOMMU mapping operations were done. I pitted IOMMUFD against the legacy VFIO backend to see if there was any noticable performance difference. I did this by updating my CDI device DaemonSet to pass in the <code>/dev/vfio/devices/*</code> paths like so:</p> /var/run/cdi/nvidia-gpu.json<pre><code>{\n  \"cdiVersion\": \"0.7.0\",\n  \"kind\": \"nvidia.com/gpu\",\n  \"containerEdits\": {},\n  \"devices\": [\n    {\n      \"name\": \"0\",\n      \"containerEdits\": {\n        \"deviceNodes\": [\n          {\n            \"path\": \"/dev/vfio/devices/vfio0\",\n            \"type\": \"c\",\n            \"permissions\": \"rwm\"\n          }\n        ]\n      }\n    },\n</code></pre> <p>I then tested the VM boot performance with and without this change.</p>"},{"location":"blog/2025/11/19/boot-time-performance-of-kata-containers-with-nvidia-gpus/#legacy-vfio-backend","title":"Legacy VFIO Backend","text":"<p>The runtime with the legacy backend is as such:</p> Text Only<pre><code>Start Time:          Mon, 08 Dec 2025 20:14:58 +0000\n      Started:      Mon, 08 Dec 2025 20:17:23 +0000\n</code></pre> <p>That's 2:25 minutes. And you can see the legacy VFIO handles being used:</p> Text Only<pre><code># ls -lah /proc/1196278/fd |&amp; egrep 'vfio|iommu'\nlrwx------ 1 root root  64 Dec  8 20:15 127 -&gt; /dev/vfio/57\nlrwx------ 1 root root  64 Dec  8 20:15 128 -&gt; /dev/vfio/vfio\nlrwx------ 1 root root  64 Dec  8 20:15 129 -&gt; anon_inode:kvm-vfio\nlrwx------ 1 root root  64 Dec  8 20:15 130 -&gt; anon_inode:[vfio-device]\nlrwx------ 1 root root  64 Dec  8 20:15 135 -&gt; /dev/vfio/46\nlrwx------ 1 root root  64 Dec  8 20:15 136 -&gt; anon_inode:[vfio-device]\nlrwx------ 1 root root  64 Dec  8 20:15 141 -&gt; /dev/vfio/16\nlrwx------ 1 root root  64 Dec  8 20:15 142 -&gt; anon_inode:[vfio-device]\nlrwx------ 1 root root  64 Dec  8 20:15 147 -&gt; /dev/vfio/132\nlrwx------ 1 root root  64 Dec  8 20:15 148 -&gt; anon_inode:[vfio-device]\nlrwx------ 1 root root  64 Dec  8 20:15 153 -&gt; /dev/vfio/112\nlrwx------ 1 root root  64 Dec  8 20:15 154 -&gt; anon_inode:[vfio-device]\nlrwx------ 1 root root  64 Dec  8 20:15 159 -&gt; /dev/vfio/86\nlrwx------ 1 root root  64 Dec  8 20:15 160 -&gt; anon_inode:[vfio-device]\nlrwx------ 1 root root  64 Dec  8 20:15 165 -&gt; /dev/vfio/66\nlrwx------ 1 root root  64 Dec  8 20:15 166 -&gt; anon_inode:[vfio-device]\n</code></pre>"},{"location":"blog/2025/11/19/boot-time-performance-of-kata-containers-with-nvidia-gpus/#iommufd-vfio-backend","title":"IOMMUFD VFIO Backend","text":"<p>With the above change:</p> Text Only<pre><code>Start Time:          Mon, 08 Dec 2025 20:09:54 +0000\n      Started:      Mon, 08 Dec 2025 20:11:48 +0000\n</code></pre> <p>That's 1:54 minutes, which is a solid 31 second reduction in performance! It's a clear difference when we tweak that single factor.</p> Text Only<pre><code># ls -lah /proc/1193235/fd |&amp; egrep 'vfio|iommu'\nlrwx------ 1 root root  64 Dec  8 20:11 127 -&gt; /dev/iommu\nlrwx------ 1 root root  64 Dec  8 20:11 128 -&gt; /dev/vfio/devices/vfio7\nlrwx------ 1 root root  64 Dec  8 20:11 129 -&gt; anon_inode:kvm-vfio\nlrwx------ 1 root root  64 Dec  8 20:11 134 -&gt; /dev/iommu\nlrwx------ 1 root root  64 Dec  8 20:11 135 -&gt; /dev/vfio/devices/vfio0\nlrwx------ 1 root root  64 Dec  8 20:11 140 -&gt; /dev/iommu\nlrwx------ 1 root root  64 Dec  8 20:11 141 -&gt; /dev/vfio/devices/vfio1\nlrwx------ 1 root root  64 Dec  8 20:11 146 -&gt; /dev/iommu\nlrwx------ 1 root root  64 Dec  8 20:11 147 -&gt; /dev/vfio/devices/vfio2\nlrwx------ 1 root root  64 Dec  8 20:11 152 -&gt; /dev/iommu\nlrwx------ 1 root root  64 Dec  8 20:11 153 -&gt; /dev/vfio/devices/vfio3\nlrwx------ 1 root root  64 Dec  8 20:11 158 -&gt; /dev/iommu\nlrwx------ 1 root root  64 Dec  8 20:11 159 -&gt; /dev/vfio/devices/vfio4\nlrwx------ 1 root root  64 Dec  8 20:11 164 -&gt; /dev/iommu\nlrwx------ 1 root root  64 Dec  8 20:11 165 -&gt; /dev/vfio/devices/vfio6\n</code></pre> <p>Note</p> <p>Getting Kata to successfully use the IOMMUFD backend requires a patch that is not merged upstream: https://github.com/kata-containers/kata-containers/pull/12173</p>"},{"location":"blog/2025/11/19/boot-time-performance-of-kata-containers-with-nvidia-gpus/#parting-thoughts","title":"Parting Thoughts","text":"<p>This post didn't have an exact agenda other than to prove to the world that Kata can in fact boot in 2 minutes given proper environmental configuration. If you didn't read anything else from this post, the key takeaways are:</p> <ol> <li>Use the latest Linux kernel version you can.</li> <li>Use hugepages.</li> <li>Use a kernel with 64KiB pages instead of 4KiB.</li> <li>Use the IOMMUFD backend for VFIO.</li> </ol> <p>The other big takeaway is that there is still work to be done in this field, mainly surrounding Linux's inability to take advantage of Intel's VT-d hugepage capability. Linux appears to be hard-coded to use only 4KiB pages to program the Intel IOMMU when VT-d additionally supports 1MiB and 1GiB. Solving this can shave an additional ~30 seconds off of our boot time.</p> <p>Jason Gunthorpe et al specifically have been doing incredible work in this domain. Jason's talks on IOMMUFD are really interesting and I recommend giving those videos below a watch.</p> <p>Warning</p> <p>There are almost certainly issues with conclusions I've made, or errors in my reasoning. There are only a select few people in the world who actually know what they're talking about regarding IOMMUs, and I'm not one of them! If you find anything incorrect in this post, please feel free to leave a comment below.</p>"},{"location":"blog/2025/11/19/boot-time-performance-of-kata-containers-with-nvidia-gpus/#references","title":"References","text":"<ul> <li>https://www.openeuler.org/en/blog/wxggg/2020-11-29-vfio-passthrough-2.html</li> <li>https://cpaelzer.github.io/blogs/007-vfio-slow-startup/</li> <li>https://xillybus.com/tutorials/iommu-swiotlb-linux</li> <li>https://www.youtube.com/watch?v=IE_A8wSWV7g</li> <li>https://www.youtube.com/watch?v=ZkzqRznaR44</li> </ul>"},{"location":"blog/2025/12/09/colorado-san-juan-mountains/","title":"Colorado San Juan Mountains","text":"<p>My last Colorado trip in 2025 was spent in the San Juan mountains of Colorado. So far, this is the most breathtaking area I've ever had the pleasure to visit and I hope to return soon!</p> <p></p> <p>A few months ago, I explored the south western part of the state for the first time. I went there by myself back in July, not knowing anything about the area, mostly as a blind journey to a place I had never been to before. This is in fact how I tend to do most of my traveling: a little bit of planning, and a lot of winging it. I'll sprinkle pictures of that trip with my second San Juan trip in this post.</p>"},{"location":"blog/2025/12/09/colorado-san-juan-mountains/#buena-vista-co","title":"Buena Vista, CO","text":"<p>The first trip out to the San Juans started as a trip with my dad. We started from north western Indiana and spent 3 days getting to our destination. The first long-term stop was at a place called Buena Vista which is on the opposite side of Aspen in the Sawatch Range.</p> <p>We camped in a spot called the Cottonwood Lake along a dispersed (and free!) campground. I took my dad to the end of the trail and we hiked up some minor trails towards the top of a fairly small mountain. Even this late into the summer, there were significant snow drifts that were easy to stick a leg through. This would be practice for the days ahead!</p>"},{"location":"blog/2025/12/09/colorado-san-juan-mountains/#4th-of-july","title":"4th of July","text":"<p>Surprising enough to me, I was told that Buena Vista held a spectacular 4th of July firework show. How in the world the fire marshals would allow such a thing confused me, especially with the persistent threat of wildfires in the area. Well, not my job I guess. We drove out into town and watch the show. Every 2 minutes or so, they had to stop the show and the fire marshals ran out into the grass to stomp out the bush fires LOL. If it's good enough for them, it's good enough to me!</p> <p>Off into the distance was the imposing figure of a Colorado 14'er called Mount Princeton. I convinced my dad the day before that we should climb it. I watched the forboding demon from afar and told myself that we'd be at the top of that peak the next morning.</p>"},{"location":"blog/2025/12/09/colorado-san-juan-mountains/#mount-princeton","title":"Mount Princeton","text":"<p>I had done one other 14'er before and lots of 13'ers, so I felt pretty confident with myself. I was already acclimated to the altitude and in pretty good shape, but my dad was neither (sorry dad, it's the truth :D). I told him that when we're on the mountain, we don't HAVE to make it to the top, and that even going halfway will be worth the view. There was no pressure to make it to the top.</p> <p>We woke up early in the morning around 6am and drove about an hour to the trailhead. There are actually two different trailheads to choose from. You can either start from the radio towers, which is a much longer hike, or from the upper chalet, which requires a decent 4x4 vehicle to get to, but skips a lot of the monotonous forest roads. We opted for the chalet because we're not trying to impress anyone, and we're also not the most experienced moutaineers.</p> <p>About an hour into the hike, the grass trail stops and everything becomes large boulders. I think they should rename the mountain to \"Break Your Ankles Mountain\" because if you aren't careful, you're in for some real hurt. The bouldering also adds a mental exhaustion component to it because you are constantly pathfinding and making sure you step in the right place. To make it even worse, the trail is totally non-existent. The only way you can really tell where to go after a certain point is whether or not there's any moss on the rocks. We had to backtrack multiple times because we kept going the wrong direction.</p> <p>We get to the first ridgeline (the last couple of pictures above) and we see the false summit above. More specifically, this picture is a good representation of the imposing height of the mountain even halfway up. The summit shown in the picture is in fact the false summit. The real summit can't be seen.</p> <p>Despite all the trials we encountered, we made it to the top! My dad provided some great commentary in that video above. The AllTrails entry for this mountain was really misleading. It was way harder than I thought it was going to be mainly because of the bouldering required pretty much the entire way. I felt fine at the top, it was definitely hard but it felt like it was within my limits. We were certainly the slowest group on the mountain. </p> <p>When we started coming back down, the fatigue really started to hit me. I ran out of water and eventually had to start drinking some of my dad's water. We ran out completely about halfway down. We were both getting pretty exhausted, but my dad was in far worse condition than me. We took it one step at a time and took plenty of breaks. I could feel myself getting dehydrated because I was getting headaches and my mouth was dry. I can't emphasize enough how HARD this was. But ya know, when you're faced with trials like this, there's only three ways to get off the mountain: with your own feet, with someone else's feet, or in a body bag. I thought using my own feet was the least dramatic way to get down.</p>"},{"location":"blog/2025/12/09/colorado-san-juan-mountains/#aspen-co","title":"Aspen, CO","text":"<p>We drove over Independence Pass to Aspen for the next few days. The main priority there was to learn how to fly fish. We got a local guide to take us out to some of the best spots in the area. He taught us how to catch rainbow troute, which are a beautiful, multicolored fish that are about the size of your palm. Over the course of 2 days, I'd estimate we caught probably over 40 fish in total. It was a ton of fun, but also way more complex than I thought it would be. The act of fly fishing itself is not too terrible once you get the hang of it, but the knowledge you need to effectively select the right bait, the right areas, and the right times of day was fairly intimidating. I'm sure I could learn how to do it, but I also feel like I'd have to bear through a lot of frustration to get competent at it.</p> <p>I can't say I enjoyed the town of Aspen itself very much. It's one of the most expensive mountain towns in the entire state (perhaps the entire country). Most everyone walking around town is dressed in polos and khakis, driving expensive cars, living in mega mansions... ya, it's not great. It was hard to find any place to eat that wasn't a Michelin Star, take-a-loan-from-your-401k kind of restaurant, and none of the working folk there seemed to enjoy being there. They certainly couldn't afford to be there. Our fly fishing guide was especially cynical and pessimistic about the town. The nature was phenomenal of course, but Aspen just left a bad taste in my mouth and I can't say I'm ever going to visit again.</p>"},{"location":"blog/2025/12/09/colorado-san-juan-mountains/#san-juans-visit-1","title":"San Juans Visit #1","text":"<p>My dad flew out of Aspen to go back home and I was left to figure out what to do with the rest of my trip. I picked a random spot on the map on the south western part of the state, purposefully avoided any large ski towns like Telluride, and eventually decided to try this place called Silverton. I found a dispersed campground called Anvil Campground and spent the next week exploring the town and the various trails. The trail down Mineral Creek was a place called Ice Lake. I had no idea what was at the end, but everything I read about it online suggested it was well worth the effort.</p> <p>The trail is pretty difficult for noobs but I did it without much of an issue. One teenager on the trail was laying flat on the ground vomiting his guts out because he was a fool and didn't prepare his body for the altitude. He had a small bottle of water, no backpack, no snacks, and had arrived from Texas the night before. So... he hadn't acclimated to the altitude and didn't really know how to prepare for a big hike like this. Yeah, don't do that. He also told me his buddies wanted to climb Capitol Peak the next day and I told him straight up he's in no condition to do that if he can't even do this trail! Where were his friends by the way? Oh, they abanonded him and continued up the mountain. What great friends you got there dude.</p> <p>When I got to the top of the Ice Lake trail, holy smokes. That water was BLUE. This is one of the rare instances when I feel like the pictures above accurately represent how blue the water really was. I guess it makes sense, this water is entirely fresh snow melt. I walked around to the back of the lake and filtered the water into my bottle. The taste of that water was just incredible. Quite minerally, but incredible.</p>"},{"location":"blog/2025/12/09/colorado-san-juan-mountains/#off-roading","title":"Off Roading","text":"<p>I have a Jeep Grand Cherokee Trailhawk which is nominally speaking an off roading vehicle. It has air bag suspension, full time transfer case, locking center diff, and electronic brake lock differentials. I also put some beefier Falken Wildpeak AT4W shoes on it, so it's \"decent.\" There are a number of Jeep Badge of Honor trails you can take in this area. The easiest one is Ophir pass which can honestly be done in most stock SUVs. I'd even hazard to say many 2WD vehicles could do it if you're careful.</p> <p>One of the more interesting trails is Imogene Pass. This is a much more difficult trail than Ophir Pass and requires more active route planning. The Jeep also handled this pretty well, but the lower air dam did get damaged and I was scraping the skid plate at various points.</p>"},{"location":"blog/2025/12/09/colorado-san-juan-mountains/#new-vehicle","title":"New Vehicle","text":"<p>I had been thinking of getting a proper truck for a long time. The Jeep could tow the trailer but it didn't feel stable and I was almost certainly overloading its payload capacity. You had to be careful when driving it. So I decided to buy a truck. You may have seen my other blog post. I drove to eastern Kansas to buy it from Main Street Chrysler Dodge Jeep RAM of Lansing.</p> <p>Sadly, the guy I bought the truck from Kyle passed away a few weeks after I bought it from a motorcycle accident. He was a really cool guy and my heart goes out to his family.</p>"},{"location":"blog/2025/12/09/colorado-san-juan-mountains/#san-juans-visit-2","title":"San Juans Visit #2","text":"<p>My second trip was a few months later. This time I drove out by myself and went straight to Silverton. My friend Jeff was flying into the Durango airport to hang out with me for a few days.</p>"},{"location":"blog/2025/12/09/colorado-san-juan-mountains/#ophir-pass-and-random-trails","title":"Ophir Pass and Random Trails","text":"<p>Armed with the experience I gained in my first San Juans trip, I took him on a very similar tour that I did the first time around. We started off by driving over Ophir Pass and down into the valley below. I showed him an awesome hiking trail just off the road towards the Ophir side of the pass.</p>"},{"location":"blog/2025/12/09/colorado-san-juan-mountains/#imogene-pass","title":"Imogene Pass","text":"<p>Aaaaand of course, here's Imogene Pass in the truck. This was actually the first time I did this trail in the truck, but I knew it was going to be much easier this time around because by all measurements, the truck is just better at off roading than my Grand Cherokee.</p>"},{"location":"blog/2025/12/09/colorado-san-juan-mountains/#fort-peabody","title":"Fort Peabody","text":"<p>At the top of Imogene Pass is an abandoned mining encampment called Fort Peabody Historic Site that has some incredible, but horrific history. The signs below tell the story.</p>"},{"location":"blog/2025/12/09/colorado-san-juan-mountains/#old-mining-encampment","title":"Old Mining Encampment","text":"<p>Fort Peabody overlooks a mining encampment that had tons of old equipment still lying around. The camp is situated on a high alpine lake, and you can even walk up into the two mine entrances (which have been effectively sealed off).</p>"},{"location":"blog/2025/12/09/colorado-san-juan-mountains/#denver","title":"Denver!","text":"<p>Jeff had to leave after a few days, and next on my roster for people coming to visit me in Colorado was my wife Kelly! The drive between Silverton and Denver is about 5 hours, and I also had to suffer through tons of I-70 congestion between Silverthorne and Denver. We stayed at a campground in the foothills called Chief Hosa Campground which is near Evergreen. The main event (and the big reason why we were in Denver) was to see one of Kelly's favorite music artists Marina.</p>"},{"location":"blog/2025/12/09/colorado-san-juan-mountains/#meowcat","title":"Meowcat","text":"<p>The first place we went to is this... experience? Museum? Art exhibit? I don't really know how to describe it other than \"all of the above.\" It's called Meowcat, and if you've never been to one of their locations, it's definitely worth a visit. I first learned about it when I went to a company offsite in Denver and thought it'd be a fun thing to do with the wife :D </p>"},{"location":"blog/2025/12/09/colorado-san-juan-mountains/#ropes-course","title":"Ropes Course","text":"<p>Kelly and I also love to do ropes courses and zip lines, so we booked a zip line fun-venture.</p>"},{"location":"blog/2025/12/09/colorado-san-juan-mountains/#buffalo-bill","title":"Buffalo Bill","text":"<p>We also got to see Buffalo Bill's resting site. They have a nice museum dedicated to him that teaches you a lot of his history. Kelly and I also got to pretend we were part of the show!</p> <p></p>"},{"location":"blog/2025/12/09/colorado-san-juan-mountains/#honorable-mentions","title":"Honorable Mentions","text":"<p>I didn't have many good pictures for these two things but they're still worth mentioning. We stopped at Phoenix Gold Mine at the recommendation of locals and saw a couple of their old mines (one of them had been a proper mine used in production, the other was just a prospecting mine). Phoenix Gold Mine is particularly fun because they have a HUGE population of resident chipmunks that you can feed. They're super friendly and I was even able to pick one up. </p> <p>Also, the Marina concert was a lot of fun. I was struggling to find the words to describe her, but eventually came across the description of \"campy,\" which is very apt. Her music is campy and fun but with a fuck-the-haters-with-a-smile kind of vibe.</p>"},{"location":"blog/2025/12/09/colorado-san-juan-mountains/#san-juan-visit-3","title":"San Juan Visit #3","text":"<p>I have done a LOT of driving this trip. Everything from San Juan Visit #2 to #3 was all a single visit to Colorado. I drove 30 hours to get to San Juans, then 5 hours to Denver, then 5 hours back to the San Juans, all for what? Well, because I'm kind of an idiot and didn't plan this properly, but also because I really really wanted everyone to see the San Juans. This time around, it's my mom coming to visit!</p> <p>A big impetus for me doing these trips to get solitude, but this time around I got almost zero of that \ud83d\ude05. It doesn't matter though because it was tons of fun getting to share such a wonderful place with all the people I love. Solitude is great but there is no greater joy than getting to experience things with people.</p>"},{"location":"blog/2025/12/09/colorado-san-juan-mountains/#kendall-mountain","title":"Kendall Mountain","text":"<p>I took my mom up an off-roading trail that straddles Kendall Mountain. There's a number of abandoned mines, one of which was poorly sealed off enough that we could walk inside. It turned out to be a big nothing burger; the shafts only went maybe a hundred feet in one direction. These prospecting mines are littered all throughout the area.</p>"},{"location":"blog/2025/12/09/colorado-san-juan-mountains/#old-hundred-gold-mine-and-mayflower-mill","title":"Old Hundred Gold Mine and Mayflower Mill","text":"<p>Silverton only very recently ceased all of its mining operations. The Mayflower Mill itself operated until 1945, but other mines in the area like the Sunnyside Mine operated until 1992. It's because of the recency of the Sunnyside Mine closure that many of the residents of Colorado, and especially the people staffing the mine and mills tours, legitimately worked these mines. Talking to them is fascinating because they recount how devastated the town felt when Sunnyside, their last operational mine, closed in the 90's. Far from being a death knell to the community, they succeeded in shifting their economy entirely to tourism. There's still a twinge of sadness in their voices when they talk about this time. Many come from multi-generation mining families, and to have that permanently taken away left them with a real loss of identity.</p>"},{"location":"blog/2025/12/09/colorado-san-juan-mountains/#the-shady-lady","title":"The Shady Lady","text":"<p>One of the old brothels in town is still in business today, although only offering the vice of alcohol. I didn't take any good pictures of the bar so I'm going to steal some online, credits at the end of this section. The building itself is haunted by the ladies who died there. The staff tell me that if you don't greet them at the beginning of the day and say goodbye at the end, they feel disrespected at the isolence and have been known to throw things at you. The staff say they're not malicious, they just prefer to be acknowledged and treated with respect. I'm sure they demanded the same from their customers as well.</p> <p>Credits: </p> <ul> <li>https://www.hmdb.org/m.asp?m=178401</li> <li>https://www.yelp.com/biz/shady-lady-saloon-silverton-2</li> </ul>"},{"location":"blog/2025/12/09/colorado-san-juan-mountains/#silverton-is-colorados-coolest-town","title":"Silverton is Colorado's Coolest Town","text":"<p>I've been to many of the major tourist towns in the area. I've been to Ouray, Telluride, Silverton, and also outlier towns like Durango and Ridgeway. Without a doubt in my mind, Silverton is by far the coolest I've ever visited in Colorado. It's not pretentious by any means. The buildings are rough around the edges, there's no mansions in sight, I dare to say you might find a discarded needle or two, but the town has tons and tons of that old, raw, western outlaw character. The spirit of Silverton feels alive, and the town's dedication to preserving its old western styles and character is really extraordinary.</p> <p>Ouray comes in a close second, but it's got a bit more of that pretentiousness I dislike. Not a lot mind you, but some. Silverton has none. You really do feel like you're a cowboy stepping back in time, and the original folks who live there, the mining folk, care a lot about the town's legacy. They care so much in fact that many of them are leaving, because the home they once loved doesn't exist anymore.</p>"},{"location":"blog/2025/12/09/colorado-san-juan-mountains/#silverton-is-a-victim-of-its-success","title":"Silverton is a Victim of Its Success","text":"<p>There are two general kinds of people in Silverton. There are those who have lived there for decades. These are the old miners, their families, their children, people who have a real claim to the town. The other kind of people are what the locals deride as the \"trust fund babies.\" These people, according to the locals, come from a whole lotta money and not a lot of respect. They told me that these people are being elected to positions of power in the township and are voting in ways that the OG locals feel is turning the town into just another Telluride, just another ski town. They tell me that people in the town used to know their neighbors. Nowadays, these rich millenials and older generations are moving in and destroying that sense of community. </p> <p>I spoke with the owner, an old lady, of Blair Street Emporium which is a cute little gift shop on Blair St. Her and her daughter both work the shop in the tourist months. The old lady has been trying to sell the shop for two years because she has been tired of the town for too long and was ready to be done. Her daughter felt a similar way. The daughter broke into tears telling me how the mayor of the town is showing a total lack of regard for the mining community that still lives there. She pointed to the town's unwillingness to maintain the Christ of the Mines Shine which has deep historical roots to the community. She told me the people moving in don't care, they just want to extract whatever cultural resources they can out of the town and give nothing back. That is not how these towns have worked, and it's just turning into another souless ski destination.</p> <p>My mom was right when she said there is a deep sense of sadness in the town. There's a sense of loss of what was and of what the town is turning into. There's also sadness that comes from the men who worked and died in the mines and the women who worked and died in the brothels. The people I talked to feel their community is being destroyed, that their home is no longer recognizable. This seems to be a common story that has been happening for decades that is by no means unique to Silverton. </p> <p>All I can say is that I left with a deep sense of respect for the town's history and for the people who worked and died in this area. Silverton's history is worth preserving and worth investing money into. Their historic society that manages the various museusm like the Mayflower Gold Mine and other assets in the area is struggling with money. Much of the old infrastructure is in dire need of repair and they don't have the funds to handle it. If I ever get stupid rich one day, I'd feel compelled to help them out.</p> <p>I'll leave you with this video of the Durango &amp; Silverton steam train hauling a load of tourists into town.</p>"},{"location":"blog/2025/12/28/out-with-google-analytics-in-with-cloudflare/","title":"Out With Google Analytics, In With Cloudflare","text":"<p>Readers of my blog have unknowingly been contributing to something I shamefully admit: I've been tracking you. I know when you view my blog site, where you're from, what you're looking at, when you click on elements, and for how long you stay on my pages. This is made quite simple, actually. However, I realized my site tracking is no longer totally effective, and I found another easy way around it. This is how.</p>"},{"location":"blog/2025/12/28/out-with-google-analytics-in-with-cloudflare/#google-analytics","title":"Google Analytics","text":"<p>If you look at the HTML source code on every page of my blog site, you'll see this section in the <code>&lt;head&gt;</code>:</p> HTML<pre><code>&lt;script id=\"__analytics\"&gt;\nfunction __md_analytics() {\n    function e() {\n        dataLayer.push(arguments)\n    }\n    window.dataLayer = window.dataLayer || [], e(\"js\", new Date), e(\"config\", \"G-F9YVCCMVEX\"), document.addEventListener(\"DOMContentLoaded\", (function() {\n        document.forms.search &amp;&amp; document.forms.search.query.addEventListener(\"blur\", (function() {\n            this.value &amp;&amp; e(\"event\", \"search\", {\n                search_term: this.value\n            })\n        }));\n        document$.subscribe((function() {\n            var t = document.forms.feedback;\n            if (void 0 !== t)\n                for (var a of t.querySelectorAll(\"[type=submit]\")) a.addEventListener(\"click\", (function(a) {\n                    a.preventDefault();\n                    var n = document.location.pathname,\n                        d = this.getAttribute(\"data-md-value\");\n                    e(\"event\", \"feedback\", {\n                        page: n,\n                        data: d\n                    }), t.firstElementChild.disabled = !0;\n                    var r = t.querySelector(\".md-feedback__note [data-md-value='\" + d + \"']\");\n                    r &amp;&amp; (r.hidden = !1)\n                })), t.hidden = !1\n        })), location$.subscribe((function(t) {\n            e(\"config\", \"G-F9YVCCMVEX\", {\n                page_path: t.pathname\n            })\n        }))\n    }));\n    var t = document.createElement(\"script\");\n    t.async = !0, t.src = \"https://www.googletagmanager.com/gtag/js?id=G-F9YVCCMVEX\", document.getElementById(\"__analytics\").insertAdjacentElement(\"afterEnd\", t)\n}\n&lt;/script&gt;\n</code></pre> <p>This clever little piece of javascript creates a number of event listeners. When you interact with anything on the page (including loading it), your browser is instructed to send an event to Google analytics describing what happened. If you open up my post on the Colorado San Juan Mountains, open up the dev tools, and click on one of the post sections, you'll see your browser make a <code>POST</code> request. Here's an example:</p> <p></p> <p>Drilling down specifically, you'll see the magic in the query parameters:</p> Text Only<pre><code>https://www.google-analytics.com/g/collect?v=2&amp;tid=G-F9YVCCMVEX&amp;gtm=45je5ca1v9127027317za200zd9127027317&amp;_p=1766964049058&amp;gcd=13l3l3l3l1l1&amp;npa=0&amp;dma=0&amp;cid=246115484.1737435637&amp;ul=en-us&amp;sr=3440x1440&amp;uaa=arm&amp;uab=64&amp;uafvl=Google%2520Chrome%3B143.0.7499.170%7CChromium%3B143.0.7499.170%7CNot%2520A(Brand%3B24.0.0.0&amp;uamb=0&amp;uam=&amp;uap=macOS&amp;uapv=15.6.1&amp;uaw=0&amp;are=1&amp;frm=0&amp;pscdl=noapi&amp;_eu=AEAAAAQ&amp;_s=2&amp;tag_exp=103116026~103200004~104527906~104528501~104684208~104684211~105391252~115583767~115938465~115938468~116184927~116184929~116251938~116251940&amp;sid=1766962961&amp;sct=24&amp;seg=1&amp;dl=https%3A%2F%2Ftopofmind.dev%2Fblog%2F2025%2F12%2F09%2Fcolorado-san-juan-mountains%2F&amp;dt=Colorado%20San%20Juan%20Mountains%20-%20Top%20of%20Mind&amp;en=scroll&amp;epn.percent_scrolled=90&amp;_et=12470&amp;tfd=24965\n</code></pre> <p>Or formatted more legibly and filtered for the most relevant fields:</p> Key Value tid G-F9YVCCMVEX sr 3440x1440 uaa arm uab 64 uafvl Google Chrome;143.0.7499.170 uap macOS dl https://topofmind.dev/blog/2025/12/09/colorado-san-juan-mountains/ dt Colorado San Juan Mountains - Top of Mind en scroll epn.percent_scrolled 90 <p>From this payload alone, I know:</p> <ol> <li>What you did (you scrolled)</li> <li>What your OS is</li> <li>What your browser is</li> <li>What page you were accessing</li> <li>Your computer's architecture</li> <li>Your screen resolution.</li> </ol> <p>Google's analytics service itself will also be able to infer your IP and some general geographic information just based on the TCP parameters of your request. I have now captured an incredible amount of insight into how all of my users behave, and can even watch you as you go from page to page.</p> <p>After a while, I realized that my analytics probably didn't capture all of the users on my website because many browsers block external trackers such as Google Analytics. The test I demonstrated above was using Google Chrome, but one thing to realize is that Google has a vested interest in not blocking trackers for many reasons that are obvious. In case it's not, Google makes money from targeted ads, so the more information they can collect from you, the more effective their ads are. Also, they want their Google Analytics platform to be as effective as possible which means sending as much traffic as possible.</p>"},{"location":"blog/2025/12/28/out-with-google-analytics-in-with-cloudflare/#safari-is-better-than-chrome","title":"Safari is Better Than Chrome","text":"<p>We can do the same test with Safari and see whether or not trackers are, erm, tracking us. Apple spends a lot of time telling people how secure and private their browser is. They even have a \"Privacy Report\" you can look at to see all the trackers they've prevented:</p> <p></p> <p>So do they? Well, surprisingly, no. At least not by default on my environment. We open the network tab and see that the exact same <code>collect</code> requests are being made:</p> <p></p> <p>That's pretty sneaky! It seems like Safari is outright lying to me. When you go to the Privacy settings at <code>Safari</code> &gt; <code>Settings</code> &gt; <code>Advanced</code> &gt; <code>Privacy</code>, you see this option was selected that makes Safari only block trackers when I have a private tab opened.</p> <p></p> <p>Setting this to <code>in all tabs</code>, we try the same experiment again. This time, no matter how much button mashing I do, there are no <code>collect</code> requests being made. To be fair to Chrome, we should check if the same kind of setting exists. To the shock of no one, this is way more complicated than it needs to be. Chrome has no native way to block trackers, and the only way to do this is by installing a third-party Chrome extension called Privacy Badger that has 4.4 stars.</p> <p>This is the lesson to learn: </p> <p>Tip</p> <p>Google wants your attention. Apple wants you to buy its hardware. These are two fundamentally different postures that affect how they approach privacy.</p>"},{"location":"blog/2025/12/28/out-with-google-analytics-in-with-cloudflare/#a-way-around","title":"A Way Around","text":"<p>So what's my method for getting around this problem? At some fundamental level, your browser has to make a request to my servers in order to get its content. You can't do anything about that. Here's the grand reveal: Cloudflare.</p> <p>I admit that the thesis of this post is a bit click-baity, because relying on Cloudflare doesn't actually do much for me in the way of tracking where you go on a page, but I can still follow your journey as your IP address from my perspective walks around the various sites in my page. Here's the system design I used to have:</p> <pre><code>flowchart TD\n        GithubCDN\n        Backblaze[Backblaze S3]\n        Google[Google Analytics]\n        User\n\n        User --&gt;|html| GithubCDN\n        User --&gt;|assets| Backblaze\n        User --&gt; Google</code></pre> <p>The part where the user reaches out to Google is what many browsers like Safari block. Here's how I rearchitected the backend:</p> <pre><code>flowchart TD\n        User\n        Cloudflare\n        GithubCDN\n        Backblaze\n        Google[Google Analytics]\n\n        User --&gt; Cloudflare\n        Cloudflare --&gt;|html| GithubCDN\n        Cloudflare --&gt;|assets| Backblaze\n        User --&gt; Google</code></pre> <p>Browsers like Chrome will still reach out to Google Analytics if configured to do so, but the key difference is that all traffic to my website flows through Cloudflare. What kind of information can I now track? Obviously the capabilities I have to profile you solely through Cloudflare is far reduced, but the metrics are still quite useful for many things.</p> <p></p> <p>There's a few other useful attributes Cloudflare can capture:</p> <ul> <li>Source IPs (of course)</li> <li>Source ASNs (to see what your ISP is)</li> <li>Real user monitoring of page performance</li> </ul> <p>When looking over the last 7 days, Cloudflare says I had 2,300 page views while Google Analytics said I have only 289. That's an enormous difference, which tells us that 88% of the traffic going to my website is not being captured by Google Analytics! The other interesting thing to note in the <code>Source browsers</code> metric above is that the vast majority of requests did not self-identify their browser types. The next two most common self-identified browsers were ChromeMobile and Chrome. It's impossible to know exactly what these are. The most likely theory is that it's a combination of bots and properly-anonymized human activity.</p> <p>Still, the number of Safari browsers identifying themselves is too high, so lock down your browsers people. Don't forget to disable website tracking. And for the love of god, don't use Chrome.</p>"},{"location":"blog/2026/01/01/my-ai-infra-thesis-of-2026/","title":"My AI Infra Thesis of 2026","text":"<p>As we enter 2026, AI infrastructure is starting to show the same structural pressures every compute boom eventually does. This post is a snapshot of how I think GPU infrastructure will evolve as the industry exits its growth-at-all-costs phase.</p>"},{"location":"blog/2026/01/01/my-ai-infra-thesis-of-2026/#background","title":"Background","text":"<p>AI infrastructure is entering its \"adult\" phase. The early AI boom optimized for rapid deployment of supercomputers with as high performance as possible and as minimal abstractions as possible. This has led to huge, single-tenant clusters, bare metal everywhere, over-provisioning of hardware, and weak isolation boundaries. AI researchers consequently need to be bothered with many low-level details of how to design performant training workflows on top of complex and exotic hardware. They need proficiency in many Linux topics just to understand how to interact properly with their software and hardware environments. AI researchers, as anyone who works in the public cloud sector already knows, are generally not good at Linux system administration. They're good at research and building models.</p> <p>This phase does not scale economically or operationally.</p> <p>As all tech booms eventually do, this AI cycle will enter a sort of steady state. This will shift the constraints to:</p> <ul> <li>Utilization efficiency</li> <li>Security between tenants</li> <li>Operational consistency</li> <li>Developer cognitive load.</li> </ul> <p>Humanity can be characterised as an endless march towards specialization. Asking AI researchers to be good at both AI research and Linux system administration is antithetical to that march. This is the clearest and most lucid point of improvement AI cloud providers can tackle. Giving researchers container-grade ergonomics with VM-grade tenancy isolation is the goal. To that end, there are two main competitors I have found in this space:</p> <ul> <li>Kata Containers</li> <li>Edera</li> </ul> <p>These are not the only possible implementations, but they are the clearest expressions today of a broader architectural shift. I will refer to these two products collectively as \"Containers in VMs\" (CVM).</p>"},{"location":"blog/2026/01/01/my-ai-infra-thesis-of-2026/#thesis","title":"Thesis","text":"<p>CVM is the missing middle layer for GPU infrastructure: VM-grade isolation with container-grade ergonomics. As AI infrastructure moves from this growth-at-all-costs phase we are currently in, providers will eventually need stronger security boundaries, higher utilization, and simpler developer workflows. The CVM model enables secure, multi-tenant GPU sharing while preserving the container-based mental model that researchers and ML engineers already rely on. This makes it a foundational technology for democratizing access to high-performance GPUs without forcing users to manage full Linux VMs or bare metal instances.</p>"},{"location":"blog/2026/01/01/my-ai-infra-thesis-of-2026/#containers-are-the-user-interface-vms-are-the-implementation","title":"Containers are the user interface, VMs are the implementation","text":"<p>Researchers and ML engineers don't want to manage Linux environments. They don't want to tackle with NFS mounts, user directories, kernel drivers, CPU pinning, kernel cmdlines, they just want to run their workloads god dammit! The CVM model flips the burden of infrastructure management. The users now think only in OCI containers while the providers absorb the complexity of VMs and the Linux environment management. Security and isolation between tenants becomes invisible.</p>"},{"location":"blog/2026/01/01/my-ai-infra-thesis-of-2026/#where-cvm-wins","title":"Where CVM Wins","text":"<p>CVM wins when:</p> <ul> <li>Tenants are untrusted</li> <li>NVIDIA Superpod systems must be shared across org boundaries</li> <li>Compliance, security, or data isolation matters</li> <li>Jobs are long-running (training, fine-tuning, inference)</li> <li>Providers want container UX with VM isolation.</li> </ul>"},{"location":"blog/2026/01/01/my-ai-infra-thesis-of-2026/#where-cvm-loses","title":"Where CVM Loses","text":"<ul> <li>Single-tenant bare metal is acceptable</li> <li>Absolute lowest latency matters more than isolation</li> <li>Users already manage full VMs comfortably.</li> </ul>"},{"location":"blog/2026/01/01/my-ai-infra-thesis-of-2026/#why-now","title":"Why now?","text":"<p>Even as recently as a couple of years ago, VMs would take incredible amounts of time to boot when datacenter-grade NVIDIA GPUs were attached to them. Most of this performance penalty came from DMA (IOMMU) and page table creations. The developments in the Linux kernel with things like IOMMUFD, dma-buf, VFIO improvements, QEMU, Kubernetes CDI specs etc. within the last 2-3 years have dramatically reduced the boottime performance problem to an almost acceptable level. This puts us in a position where VMs can boot fast enough to compete with native runc OCI containers.</p> <p>Running containers inside VMs is not a new idea, but the technology is improving in such a way that will fundamentally change how AI researchers expect to interact with high performance clusters (in my humble opinion). The days of researchers wrangling full Linux systems are numbered. The visibility of traditional HPC workload schedulers like SGE and Slurm will diminish. I\u2019m betting my career on secure GPU multi-tenancy with container ergonomics becoming the dominant model for AI infrastructure.</p>"},{"location":"blog/2026/01/01/my-ai-infra-thesis-of-2026/#epilogue","title":"Epilogue","text":"<p>I'm being intentionally vague in this post, almost to a degree I personally find unnatural. Nonetheless, those in the industry who work with Kata Containers or Edera or the many AI-focused neoclouds out there will have an implicit understanding of the problems facing AI researchers today and how current neocloud offerings fall short. I'll continue my research into both Kata and Edera and will be sharing my perspectives on the two projects when the time is right. Stay tuned!</p>"},{"location":"blog/2026/01/21/goodbye-lambda-hello-coreweave/","title":"Goodbye Lambda, Hello CoreWeave","text":"<p>Today I did something that I didn't expect to be so hard, I'm leaving behind a company I've dedicated the last 2 years of my life to. I'm leaving Lambda and joining CoreWeave. This post will explain why.</p> <p></p>"},{"location":"blog/2026/01/21/goodbye-lambda-hello-coreweave/#background","title":"Background","text":"<p>I joined Lambda Februrary of 2024. Lambda, for those of you who don't know, is a Neo-Cloud company that builds and operates supercomputing clusters built specifically for Machine Learning workloads. An additional software layer lives on top of these clusters that lets multiple customers rent access to the machine. This middle layer is what I worked on, specifically the VM Control Plane that scheduled and placed customer VMs onto the hardware. </p> <p>At the time I joined, my team operated pretty much the whole shebang from the time the customer's VM request came from our front-end systems all the way to instantiating it on the hosts, including managing all of the performance-related problems that come with virtualization. I was fortunate to lead some large technical initiatives at the company, the one I'm most proud of being the Lambda Guest Agent. You can see some pictures of the frontend of this project below:</p> <p> </p> <p>I was able to lead this initiative because Lambda was in a unique phase in its journey. They were still early enough that there was room for engineers to step into unusually broad ownership. I was able to step into this role as an IC engineer because of these circumstances, but it also helped that everybody loves pretty graphs and rallying people behind the idea that our customers deserve pretty graphs is something we should do. This is not something that would be possible nowadays (at least in the way I did it) for a cowboy engineer like myself because Lambda is much more operationally mature, but it worked out at the time.</p> <p>The Guest Agent was the most visible project I worked on, but I'm also proud of other initiatives I tackled both small and large that improved Lambda's reliability and performance. I reworked how Lambda's Jupyter Notebooks were exposed to the public internet which insulated us from a number of Cloudflare outages. I contributed to Lambda's 1-Click Cluster that gave customers access to virtual machines networked together through high performance Infiniband fabrics. I improved our internal visibility into GPU health so we could better detect when hardware went bad (here's some homework: how do you do this without access to NVIDIA Kernel Drivers or well-supported Redfish endpoints? It's not easy!). I was also instrumental in representing my team during massive datacenter buildouts in Dallas and Austin where we delivered these new clusters to customers.</p>"},{"location":"blog/2026/01/21/goodbye-lambda-hello-coreweave/#a-change-in-teams","title":"A Change in Teams","text":"<p>Around early 2025, I moved to a newly minted team called Special Projects whose charter was to build high-risk and possibly nebulous AI-related projects that the core business did not feel comfortable dedicating normal engineering time on. This is where I ended up researching how to do GPU-based Containers as a Service (CaaS), specifically using Kata containers. You can see the results of my research here and here. I built a proof-of-concept cluster on two Supermicro HGX H100 servers and presented a working example during a company show-and-tell to much excitement. This was my proudest achievement on the team because this excitement eventually spilled outside the company and into the LinkedIn blogosphere where my work was featured in Kubernetes-specific publications.</p> <p>I was neither a Kubernetes expert nor a virtualization expert, but I learned enough to make this work and meet the criteria I set out for myself. I was able to get a VM with 8 H100 GPUs attached to it to boot from 30 minutes down to almost 1 through much collaboration with the contributors to Kata Containers. This was proof to me that I don't need to be an expert on something to create value, I just need dogged determination, a willingness to learn, and (most importantly) relationships with the right people. Nothing is impossible without a little bit (or even a lot) of research.</p>"},{"location":"blog/2026/01/21/goodbye-lambda-hello-coreweave/#a-change-in-priorities","title":"A Change in Priorities","text":"<p>With such a successful career at Lambda, people must be asking why would I want to leave? Was I upset? Was I angry? Was I unhappy? The answers are no, no, and kind of. I was unhappy not because anything was wrong per se, but because it ended up being the case that Lambda could not absorb my ambitions anymore. Just because I build something cool and interesting does not mean the business needs or wants it right now, and sometimes even if they do need and want it, it may not be the right time. This is where I found myself; I had proven something like this could be built, I was getting an overwhelmingly positive signal from the external market, and yet the company I worked at could not yet absorb it. </p> <p>This presented to me a real fork in the road. I could either spend a lot of political capital trying to convince the business to let me build this, which wouldn't have been in their best interests anyway, or I could go to somewhere that has explicit needs for the kinds of skills and research I acquired in the course of building out this PoC. This is where CoreWeave came into play. It's actually not an opportunity I seeked out, but rather a match that landed serendipitously into my lap.</p>"},{"location":"blog/2026/01/21/goodbye-lambda-hello-coreweave/#strategically-clear-emotionally-hard","title":"Strategically Clear, Emotionally Hard","text":"<p>My decision to leave Lambda was simultaneously one of the easiest and most difficult ones to make. It was the easiest decision because the alignment between CoreWeave and I could not have been better matched. It was the hardest decision because I honestly, although perhaps a bit naively, thought that Lambda would be a long-term home for me. I certainly wanted it as a home because I loved the people, I loved the cool shit we were able to build together, and riding on the crest of this AI wave in the boat of this scrappy startup with wicked smart people all rowing together was an experience I'll always miss. Having to break the news to my colleagues made my stomach twist. I don't say that as a corporate shill, I say it genuinely because I felt a real camaraderie with the people here and I have to fight back these feelings that I somehow failed them. I know I didn't fail them, nobody failed anybody, and yet that's the feeling anyway.</p> <p>Some of the folks that are top of mind for me: Jeff Nadler, who shares my love of observability, trusted me enough to let me take charge on the guest-agent project back in early 2025. We have a shared understanding of what makes engineering teams great and what doesn\u2019t. Paul Sebexen saw my potential early and pulled me into high-stakes, long-form, and sometimes obscure projects because he trusted me to do the right thing. I also want to shout out Michael Balaban who took time out of his late night to talk to me one on one after a project I had been deeply invested in was cut last minute. This was an act of sympathy I\u2019ll never forget, one that turned a bad situation into a story of resilience.</p> <p>I can't express how excited I am to turn this new chapter with CoreWeave. Anyone who knows anything about AI infrastructure has heard of them, and I have a lot to learn from the incredible work they've done. I can't share any details on plans, but all I can say is that you'll see lots of open source work from me on Kata Containers and hopefully a number of public talks at KubeCon on the subject. I'll continue to collaborate with Zvonko Kaiser and team from NVIDIA on this cool project to build out new capabilities for AI researchers.</p> <p>As I've already shared in my AI Infra Thesis, I'm willing to bet the next few years of my career that the platform in which AI Researchers use to train and serve their models will fundamentally change. Compute will become democratized amongst the people and barriers to entry will fall. I hope that one day AI Researchers never have to touch a VM again in their life. That's the goal, at least.</p>"},{"location":"blog/2026/02/15/gpu-virtualization-part-1-an-introduction-to-pcie-and-mmio/","title":"GPU Virtualization Part 1: An Introduction to PCIe and MMIO","text":"<p>PCIe is a packet-switched fabric, and modern devices are controlled by two primitives: memory-mapped registers (BARs via MMIO) and DMA. GPU virtualization ultimately reduces to controlling access to those primitives, who can access which address ranges, and which device-initiated transactions are permitted.</p> <p>This series starts at the bottom of the stack and builds upward. In this first post, we\u2019ll examine how PCIe routing works, how BARs carve out regions of the system\u2019s physical address space, and how the IOMMU constrains device-initiated memory access. These details form the hardware boundary that virtualization systems rely on.</p>"},{"location":"blog/2026/02/15/gpu-virtualization-part-1-an-introduction-to-pcie-and-mmio/#pcie","title":"PCIe","text":""},{"location":"blog/2026/02/15/gpu-virtualization-part-1-an-introduction-to-pcie-and-mmio/#a-brief-introduction","title":"A Brief Introduction","text":"<p>PCIe is an acronym that stands for Peripheral Component Interconnect Express. It's a standard that was created to solve the problem of how CPUs should be able to access peripheral devices like network cards, storage, audio devices, and graphical accelerators. The express portion of the name alludes to the fact that it's an extension to the older PCI bus standard. The Original Gangster PCI standard, which is mostly no longer used, relied on a shared bus topology. That is, all devices communicated through a single data bus and clock line in a flat topology and shared a common address space. This scheme was greatly limiting for a number of reasons, one of which being that the clock was limited to the slowest peripheral on the bus.</p> <p>PCI Express, on the other hand, looks more like a switched network. It's comprised of a tree of interconnected components, with PCIe bridges and switches connecting components together. Its architecture allows for full duplex communication and is theoretically extendible to a virtually infinite number of endpoints. PCIe also allows for a variable number of lanes, anywhere from 1 to 16 for standard PCIe form factors (such as what is used in consumer grade desktops) and even higher for certain CPU chipsets. A lane consists of a differential signal pair, so 16 lanes actually represents 32 physical lines. Sending data through differential signal pairs allows noise to be effectively filtered out. This is a technique used in other data transmission standards like XLR cables for audio equipment.</p>"},{"location":"blog/2026/02/15/gpu-virtualization-part-1-an-introduction-to-pcie-and-mmio/#topology-considerations","title":"Topology Considerations","text":"<p>Below is an example PCIe topology inspired from the Wikipedia page.</p> <pre><code>                              +-----------------+                                                        \n                              |                 |                                                        \n                              |                 |                                                        \n                              |      CPU        |                                                        \n                              |                 |                                                        \n                              |                 |                                                        \n                              |                 |                                                        \n                              +--------+--------+                                                        \n                                       |                                                                 \n                                       |                                                                 \n                                       |                                                                 \n                                       |                                                                 \n            +--------------------------+-----------------------------------------+          +-----------+\n            |                                                                    |          |           |\n            |                           Root Complex                             +----------+  Memory   |\n            |                                                                    |          |           |\n            |                                                                    |          |           |\n            +---------+---------------------------+------------------------+-----+          +-----------+\n                      |                           |                        |                             \n                      |                           |                        |                             \n                      |                           |                        |                             \n                      |                           |                        |                             \n                +-----+-----+               +-----+-----+            +-----+-----+                       \n                |           |               |           |            |           |                       \n                |           |               |  Endpoint |            |  Bridge   |                       \n      +---------+  Switch   +---------+     |           |            |           |                       \n      |         |           |         |     |           |            |           |                       \n      |         +-----+-----+         |     +-----------+            +-----------+                       \n      |               |               |                                                                  \n      |               |               |                                                                  \n      |               |               |                                                                  \n+-----+-----+   +-----+-----+    +----+------+                                                           \n|           |   |           |    |           |                                                           \n|  Endpoint |   |  Endpoint |    |  Endpoint |                                                           \n|           |   |           |    |           |                                                           \n|           |   |           |    |           |                                                           \n+-----------+   +-----------+    +-----------+\n</code></pre> <p>This diagram is obviously dramatically simplified, however we can see a few notable components. The \"Root Complex\" at the top (which we will talk more about later) connects the CPU with downstream components like switches, endpoints, and legacy buses via a bridge. While I worked at Lambda, I developed an interactive Terminal UI (TUI) called PCIEx that you can use to explore the PCIe topology on your local system. You can find that here. I encourage you to submit issues to my fork of Lambda's upstream repo. The navigation controls are admittedly a little clunky but it was also my first attempt at a TUI, so all is fair I guess.</p> <p></p> <p>The screenshot above is of a Supermicro A10 system. You can see two \"3D Controllers\" (which is PCIe parlance for a GPU) that live behind a set of two bridges each. You can also see an Infiniband CX6 network card. The right pane shows a number of details about each device as you scroll that is scraped from the <code>/sys/bus/pci/devices</code> directory, including the device's BDF (its Bus-Device-Function address), its NUMA node, its local CPU list, and its lane width of 64.<sup>1</sup></p> <p>The <code>Intel Corporation</code> devices you see at the bottom are actually one physical device that is exposing multiple different functions. Bridges generally only have one downstream port, so any time you see what appears to be multiple devices living behind a single bridge, it's often because the device itself is exposing multiple functions. </p> <p>Another scenario not pictured here is when you have multiple devices living under the same switch, or underneath the same root port. In these cases, such devices may be able to talk directly to one another through peer-to-peer (P2P). This may not always desirable. Consider a situation where you're a cloud provider and you want to allow multiple customers to rent the same physical node. There may be some devices that while technically speaking could do peer-to-peer with each other, for the purposes of isolating customers from one another, you might want to disallow this. This is where ACS comes into play which we will talk about later on.</p>"},{"location":"blog/2026/02/15/gpu-virtualization-part-1-an-introduction-to-pcie-and-mmio/#root-complex","title":"Root Complex","text":"<p>The Root Complex is a physical component, often part of the motherboard's chipset but also sometimes resident on the CPU die itself, that performs a number of important functions. It is the boundary between the PCIe fabric and the rest of the system. It connects PCIe devices to the CPU cores and memory controllers, and it is where device-initiated transactions (DMA) enter the system and are subject to translation and access control.</p>"},{"location":"blog/2026/02/15/gpu-virtualization-part-1-an-introduction-to-pcie-and-mmio/#dma","title":"DMA","text":"<p>The Root Complex also plays a critical role in Direct Memory Access (DMA) transactions. Consider that you're a GPU that wants to transfer some of your VRAM to the host memory so that programs running on the CPU can read it. The traditional/old fashioned way of transferring the memory would be for the CPU to initiate PCIe transactions and read data from the device, then copy this data to host memory via CPU registers. This is an extremely inefficient method of memory transfer because not only does it require CPU involvement, but it often requires long iterations to fully sweep the entirety of the device's memory. DMA instead cuts the CPU out of the equation and allows the GPU to send to and receive data from the host memory of its own volition.</p> <p>Allowing PCIe devices to write arbitrary data to arbitrary locations in memory is extremely dangerous for obvious reasons. Bugs in firmware could lead to writes to critical areas in memory such as kernel data structures, and even worse might allow a device owned by customer A to read customer B's data. This calls for a method of sandboxing GPU memory access. The IOMMU exists to solve that problem.</p>"},{"location":"blog/2026/02/15/gpu-virtualization-part-1-an-introduction-to-pcie-and-mmio/#iommus","title":"IOMMUs","text":"<p>The Input-Output Memory Management Unit (IOMMU) is a component of the root complex that maintains a number of page tables specifically designed for IO devices. It's used when a device wants to read or write data into the host memory and needs to translate its IO Virtual Address (IOVA) to a Host Physical Address (HPA). The IOMMU enforces translations per domain (a context). Devices are assigned to a domain, and the domain points to the page tables the IOMMU walks. An abstracted diagram of what this is doing is shown below:</p> IOMMU<pre><code>                       IOMMU                          \n+---------------------------------------------------+ \n|                                                   | \n|        IOVA                          HPA          | \n| +----------------+            +----------------+  | \n| |   0x00-0xF0    +-----------&gt;|    0x5000      |  | \n| +----------------+            +----------------+  | \n| |                |            |                |  | \n| |   0xF1-0xFF    |            +----------------+  | \n| |                +------+     |                |  | \n| |                |      |     +----------------+  | \n| +----------------+      |     |                |  | \n| |                |      |     +----------------+  | \n| |                |      +----&gt;|    0xF000      |  | \n| |                |            +----------------+  | \n| |  0x100-0xFFF   |            |                |  | \n| |                +------+     +----------------+  | \n| |                |      |     |                |  | \n| |                |      |     +----------------+  | \n| +----------------+      +----&gt;|    0xBF00      |  | \n| |                |            +----------------+  | \n| |                |      +----&gt;|    0xDEADC0DE  |  | \n| |  0x1000-0x10F0 |      |     +----------------+  | \n| |                +------+     |                |  | \n| +----------------+            +----------------+  | \n|                                                   | \n+---------------------------------------------------+ \n</code></pre> <p>When a PCIe device performs a memory read or write, it provides an IOVA to the IOMMU which gets translated to the host physical address. The transaction is then forwarded to the memory controller (for RAM) or back into the root complex (for MMIO). This provides three crucial functions to devices:</p> <ol> <li>It allows the OS/hypervisor to present a contiguous IOVA space even if the system\u2019s physical memory is fragmented, and even if the device can\u2019t address all physical memory directly.</li> <li>It provides a method for restricting where in host memory a device can DMA to.</li> <li>It also enforces access control based on physical address ranges. If a device attempts to access a physical region (including another device\u2019s BAR) that is not mapped into its IOMMU domain, the transaction is blocked.</li> </ol> <p>Number 2 is particularly crucial when it comes to device virtualization. When doing what's called direct passthrough to a virtual machine, the CPU will program the IOMMU such that a device is physically restricted to DMA to the memory allocated for the guest. This provides a hard level of hardware memory isolation. The components interact with each other like this:</p> <pre><code>                           +-----------------+                           \n                           |                 |                           \n                           |                 |                           \n                           |      CPU        |                           \n                           |                 |                           \n                           |                 |                           \n                           |                 |                           \n                           +--------+--------+                           \n                                    |                                    \n                                    |programs                            \n                                    |                                    \n                                    v                                    \n+-----------------+        +-----------------+        +-----------------+\n|                 |        |                 |        |                 |\n|                 |        |                 |        |                 |\n|                 | IOVA   |                 | HPA    |                 |\n|   PCIe Device   +-------&gt;|     IOMMU       +-------&gt;| RAM Controller  |\n|                 |        |                 |        |                 |\n|                 |        |                 |        |                 |\n+-----------------+        +-----------------+        +-----------------+\n</code></pre> <p>Just as the CPU MMU translates process virtual addresses to physical memory, the IOMMU translates device virtual addresses (IOVAs) to physical memory.</p>"},{"location":"blog/2026/02/15/gpu-virtualization-part-1-an-introduction-to-pcie-and-mmio/#bridges-and-switches","title":"Bridges and Switches","text":"<p>PCIe fabrics are built out of endpoints, bridges, and switches. At a high level:</p> <ul> <li>A bridge connects one upstream port to one downstream port.</li> <li>A switch connects one upstream port to multiple downstream ports.</li> </ul> <p>Bridges were originally used to connect different PCI variants (for example, PCIe to legacy PCI), and more generally to segment a hierarchy into multiple buses. In the PCI model, the topology consists only of devices and bridges.</p> <p>A PCIe switch is conceptually more powerful: it is responsible for routing transactions between multiple downstream ports. However, for software compatibility reasons, the PCIe specification defines a switch as a logical collection of PCI-to-PCI bridges. To the operating system, a switch appears as multiple downstream bridge devices connected to a single upstream bridge.</p> <p></p> <p>PCIe Base Specification Revision 5.0 Version 1.0 \u00a71.3.3</p> <p>A Switch is defined as a logical assembly of multiple virtual PCI-to-PCI Bridge devices as illustrated in Figure 1-3 . All Switches are governed by the following base rules.</p> <ul> <li>Switches appear to configuration software as two or more logical PCI-to-PCI Bridges.</li> <li>A Switch forwards transactions using PCI Bridge mechanisms; e.g., address-based routing except when engaged in a Multicast, as defined in Section 6.14.</li> </ul> <p>This detail matters because routing decisions, including whether a transaction is forwarded to a peer device or forced upstream (as with ACS), occur at these bridge/switch boundaries.</p>"},{"location":"blog/2026/02/15/gpu-virtualization-part-1-an-introduction-to-pcie-and-mmio/#transaction-layer-packets","title":"Transaction Layer Packets","text":"<p>We've talked about many of the components that are involved in PCIe, but we haven't talked about how the PCIe network handles routing decisions. PCIe uses packetized transactions. Payload sizes are bounded by negotiated limits like Max Payload Size and Max Read Request Size. Each packet is called a Transaction Layer Packet (TLP). TLPs are fairly analogous to TCP/IP packets: they contain headers with address information, payloads, and digests used for error correction. The header of a TLP looks like this:</p> <p></p> <p>This blog post provides information on the various types of TLP packets that can be sent:</p> TLP Type Format Type Description MR 000 / 001 0 0000 Memory Read Request MRL 000 / 001 0 0001 Memory Read Request Locked MW 010 0 0000 Memory Write Request IOR 000 0 0010 I / O Read Request IOW 010 0 0010 I / O Write Request CR0 000 0 0100 Configuration Read Type 0 CW0 010 0 0100 Configuration Write Type 0 CR1 000 0 0101 Configuration Read Type 1 CW1 010 0 0101 Configuration Write Type 1 Msg 001 1 0 r2 r1 r0 Message Request MsgD 011 1 0 r2 r1 r0 Message Request with Data"},{"location":"blog/2026/02/15/gpu-virtualization-part-1-an-introduction-to-pcie-and-mmio/#windows-and-routing","title":"Windows and Routing","text":"<p>Every bridge or switch has three specific \"windows\" that can be addressed by a TLP:</p> <ol> <li>Actions on configuration. In this scenario, the TLP is taking action on a specific device's configuration space.</li> <li>IO ports. This is used for legacy port-based IO and for the most part is not used in any modern systems with very small exceptions.</li> <li>Memory. This is used mainly in memory mapped IO (which we will also talk about later) to access the memory inside of a device.</li> </ol> <p>When the system BIOS boots and walks the PCIe tree, it assigns address to each of the devices and saves it into their configuration registers. A switch is aware not only of its address, but also the address windows for each of its downstream ports. Let's take a simple example of what this might look like:</p> <pre><code>CPU / Root Complex\n        |\n   Root Port\n        |\n   PCIe Switch\n   \u251c\u2500\u2500 Port 1 \u2192 GPU (MMIO: 0x4000_0000_0000\u20130x43FF_FFFF_FFFF)\n   \u251c\u2500\u2500 Port 2 \u2192 NIC (MMIO: 0x4400_0000_0000\u20130x44FF_FFFF_FFFF)\n   \u2514\u2500\u2500 Port 3 \u2192 NVMe (MMIO: 0x4500_0000_0000\u20130x4500_0FFF_FFFF)\n</code></pre> <p>When the TLP with an address of <code>0x4000_0000_0000</code> arrives to the PCIe switch, it immediately knows that the packet must be forwarded to port 1. In the opposite direction, if the GPU on port 1 sends a TLP with an address of <code>0x3000_0000_0000</code>, the switch is not aware of any port with such a window and thus forwards the packet to its upstream port. In the P2P case with ACS disabled, if Port 1 sends a packet to address <code>0x4400_0000_0000</code>, the switch will route it to port 2 without any involvement of the root complex.</p> <p>We can take a slightly more complicated example to see how this works in multi-layered switch topologies. Take for example this scenario:</p> <pre><code>CPU / Root Complex\n        |\n     Root Port\n        |\n     PCIe Switch S0\n   \u251c\u2500\u2500 Port 1 \u2192 PCIe Switch S1\n   \u2502              \u251c\u2500\u2500 Port 1 \u2192 GPU  (MMIO: 0x4000_0000_0000\u20130x4000_0FFF_FFFF)\n   \u2502              \u2514\u2500\u2500 Port 2 \u2192 NVMe (MMIO: 0x4000_1000_0000\u20130x4000_1000_FFFF)\n   \u2502\n   \u2502      (S0 Port 1 window: 0x4000_0000_0000\u20130x4000_1FFF_FFFF)\n   \u2502\n   \u2514\u2500\u2500 Port 2 \u2192 NIC (MMIO: 0x5000_0000_0000\u20130x5000_00FF_FFFF)\n</code></pre> <p>In this case, Port 1 of switch S0 is programmed with the window <code>0x4000_0000_0000\u20130x4000_1FFF_FFFF</code> so that it encompasses all of the devices in that subtree. In fact, you can view what windows these ports have been programmed with in your own system:</p> <pre><code>$ lspci | grep -i bridge\n00:0f.0 PCI bridge: Intel Corporation Device 1bbf (rev 11)\n$ lspci -vv -s '00:0f.0' |&amp; grep -i behind\n        I/O behind bridge: 2000-2fff [size=4K] [16-bit]\n        Memory behind bridge: 94000000-950fffff [size=17M] [32-bit]\n        Prefetchable memory behind bridge: [disabled] [64-bit]\n</code></pre> <p>This is just a random bridge, but let's find one that is directly in front of a GPU. We can use my pciex tool to easily find a good example:</p> <pre><code>\u251c\u2500\u2500 bridge | PCIBUS:0000:1e\n\u2502   \u2514\u2500\u2500 bridge | PCIBUS:0000:1f        \n\u2502       \u251c\u2500\u2500 bridge | PCIBUS:0000:20    \n\u2502       \u2502   \u2514\u2500\u2500 display | AD102GL [L40]\n</code></pre> <p>What we should see is that the bridge immediately adjacent to the GPU has an identical memory window. This is the GPU's memory regions:</p> <pre><code>lspci -vv -s '20:00.0' |&amp; grep Region\n        Region 0: Memory at a8000000 (32-bit, non-prefetchable) [size=16M]\n        Region 1: Memory at 6d000000000 (64-bit, prefetchable) [size=64G]\n        Region 3: Memory at 6f040000000 (64-bit, prefetchable) [size=32M]\n</code></pre> <p>This is the bridge just above it:</p> <pre><code>$ lspci -vv -s '1f:00.0' | grep -i behind\n        I/O behind bridge: [disabled] [32-bit]\n        Memory behind bridge: a8000000-a97fffff [size=24M] [32-bit]\n        Prefetchable memory behind bridge: 6d000000000-6f041ffffff [size=132128M] [32-bit]\n</code></pre> <p>We can see the <code>Prefetchable memory behind bridge</code> section starts exactly where <code>Region 1</code> of the device does. The end address of <code>0x6f041ffffff</code> not coincidentally happens to be the start of <code>Region 3</code> plus 32MiB. <code>0x6f040000000</code> + <code>0x2000000</code> = <code>6F042000000</code>. If we look in the <code>/proc/iomem</code> file, we see these values represented again, but this time with an even better representation of how these windows are laid out (with some annotations of my own):</p> <pre><code>60000000000-6ffffffffff : PCI Bus 0000:1d\n  6a000000000-6f041ffffff : PCI Bus 0000:1e    &lt;-- Bridge two levels above GPU\n    6a000000000-6f041ffffff : PCI Bus 0000:1f  &lt;-- Bridge immediately adjacent to GPU\n      6a000000000-6c041ffffff : PCI Bus 0000:21\n        6a000000000-6afffffffff : 0000:21:00.0\n        6b000000000-6bfffffffff : 0000:21:00.0\n        6c000000000-6c03fffffff : 0000:21:00.0\n        6c040000000-6c041ffffff : 0000:21:00.0\n      6c042000000-6c0421fffff : PCI Bus 0000:22\n      6c042200000-6c0423fffff : PCI Bus 0000:23\n      6c042400000-6c0425fffff : PCI Bus 0000:24\n      6d000000000-6f041ffffff : PCI Bus 0000:20  &lt;-- GPU in question\n        6d000000000-6dfffffffff : 0000:20:00.0\n        6e000000000-6efffffffff : 0000:20:00.0\n        6f000000000-6f03fffffff : 0000:20:00.0\n        6f040000000-6f041ffffff : 0000:20:00.0\n</code></pre> <p>Pretty cool! This confirms our mental model is accurate.</p>"},{"location":"blog/2026/02/15/gpu-virtualization-part-1-an-introduction-to-pcie-and-mmio/#access-control-services-acs","title":"Access Control Services (ACS)","text":"<p>If two devices live underneath the same switch, and if those two devices know the physical MMIO addresses assigned to each other, they can initiate a peer-to-peer PCIe transaction. Like we mentioned before, this may not be desirable when you have multiple tenancies on a node that need to be separated from each other. To enforce isolation, we remove the switch\u2019s ability to perform direct peer-to-peer routing. This is done by enabling Access Control Services.</p> <p>When ACS forces the transaction upstream, it reaches the root complex as a device-originated Memory Read/Write TLP. Because it originated from a PCIe endpoint, the transaction is subject to IOMMU checks. The IOMMU looks up the Requester ID (GPU A\u2019s BDF), determines the associated IOMMU domain, and verifies whether the target physical address range (GPU B\u2019s BAR window) is mapped in that domain. </p> <p>If the address is permitted, the transaction is routed back down the tree to GPU B. If not, the IOMMU blocks the request and raises a fault. Importantly, the IOMMU is enforcing access to physical address ranges, not abstract \u201cdevice-to-device\u201d permissions. Isolation emerges because each device\u2019s domain maps only the memory and MMIO ranges it is allowed to access.</p> <p>Let's first illustrate an example scenario. </p> <pre><code>        +----------------------------------+ \n        |           Root Complex           | \n        |                                  | \n        |       3: 0x4100       +-------+  | \n        |     +-----------------&gt;       |  | \n        |     |       4: 0x4100 | IOMMU |  | \n        |     |         +-------+       |  | \n        |     |         |       +-------+  | \n        |    ++---------v-+                | \n        |    |  Root Port |                | \n        +----|------------|----------------+ \n             |            |                  \n             +-----^----+-+                  \n                   |    |                    \n         2: 0x4100 |    |5: 0x4100           \n                   |    |                    \n           +-------+----v-----+              \n           |                  |              \n           |                  |              \n           |      Switch      |              \n           |                  |              \n           |                  |              \n           +-----^---------+--+              \n                 |         |                 \n       1: 0x4100 |         |6: 0x4100        \n                 |         |                 \n+----------------+---+ +---v----------------+\n|                    | |                    |\n| GPU A              | | GPU B              |\n|                    | |                    |\n| BDF: 0000:12:00.0  | | BDF: 0000:13:00.0  |\n| BAR: 0x4000-0x40FF | | BAR: 0x4100-0x41FF |\n|                    | |                    |\n+--------------------+ +--------------------+\n</code></pre> <p>We go through each step of the process:</p> <ol> <li>GPU A sends a TLP with a destination of <code>0x4100</code>. This address represents GPU B's base address (read more about Base Address Registers below). The switch receives the packet and sees that <code>0x4100</code> is on its downstream port, but ACS is enabled. So, it forwards it to step 2.</li> <li>The packet arrives at the root complex via the root port.</li> <li>Because the transaction originated from a PCIe endpoint, it is subject to IOMMU checks before further routing.</li> <li>The IOMMU sees the TLP's Requester ID (RID) which is just GPU A's BDF address of <code>0000:12:00.0</code>. It maps this RID to a specific IOMMU Domain and verifies that the target physical address (<code>0x4100</code>) is mapped in that domain\u2019s page tables. If this address maps to a valid region in this domain, it forwards the packet back down. In our case, the packet is allowed.</li> <li>The packet makes it back down to the switch.</li> <li>The switch sees an incoming packet for <code>0x4100</code> on the upstream port and forwards it to the appropriate downstream port. If the address had not been mapped into GPU A\u2019s IOMMU domain, the IOMMU would have raised a fault and the transaction would never reach GPU B.</li> </ol> <p>Tip</p> <p>In this example, the target address (<code>0x4100</code>) is already a host physical address corresponding to GPU B\u2019s BAR. No IOVA to HPA translation is required. The IOMMU is performing a permission check, not remapping the address.</p> <p>To reiterate, ACS doesn\u2019t block traffic by itself; it forces certain peer-to-peer transactions to go upstream so the root complex/IOMMU can enforce policy. Without ACS, peer-to-peer transactions never reach the root complex, which means the IOMMU cannot enforce isolation. This is why ACS capability is a hard requirement for safe multi-tenant device passthrough.</p>"},{"location":"blog/2026/02/15/gpu-virtualization-part-1-an-introduction-to-pcie-and-mmio/#memory-mapped-io-mmio","title":"Memory Mapped IO (MMIO)","text":"<p>We've talked about how PCIe routing works, how P2P enforcement works via ACS, and the structure of TLP packets in order to gain an understanding of how data flows in a PCIe fabric. Now we will explore how devices expose data to the fabric and what that interface looks like.</p> <p>Historically, CPUs used what's called Port Mapped IO (PMIO) to talk to devices. In PMIO, CPUs issue special x86 instructions such as <code>IN</code> or <code>OUT</code> in to talk to ports that live in a separate address space from physical memory. This method of device communication had many inherent limitations. For one, the port address space was limited to 16 bits or 65536 total ports. PMIO instructions could only operate on a small number of bytes at a time. It's also harder to virtualize because it requires trapping specific instructions and then emulating the behavior of that device. MMIO on the other hand looks like memory access and can be more easily emulated through page tables. Because I/O devices appear as memory locations, any instruction that manipulates memory can also manipulate a device register. This simplifies compiler design and allows for complex operations directly on the hardware, unlike the restrictive, specialized instructions required for PMIO.</p> <p>Modern systems nowadays rely almost entirely on MMIO to interact with peripherals. The historic bifurcation of IO devices and main memory led to unnecessary complexities, and unifying them under a single instruction set, a single address space, and a single data structure (page tables) allowed compilers, virtualization technologies, and CPU design to all be simplified.</p>"},{"location":"blog/2026/02/15/gpu-virtualization-part-1-an-introduction-to-pcie-and-mmio/#base-address-registers","title":"Base Address Registers","text":"<p>All devices in PCIe expose a large set of registers that provide various information about the device itself, its capabilities, and how much addressable memory space it needs on the host system. These registers comprise a device's Configuration Space. You can see the set of common registers in the PCIe spec:</p> <p></p> <p>How does a device tell the system which MMIO ranges it needs? Through its configuration space, specifically, its Base Address Registers (BARs). When a system initially boots, the host firmware (BIOS, UEFI, OVMF etc) write all 1's to the Base Address Register. This prompts the device to then write a value that corresponds to the amount of bytes of address space it requires which is read back by the host firmware.<sup>5</sup> The host will allocate this region in its address space and then write the base of this address back into the device's BAR. This is where the term BAR comes from because it stores the base address of the physical address space that has been allocated for it.</p> <p>After the BAR has been written to with its final base address, the device will listen for PCIe transactions that fall within this BAR range. Transactions in this range will be claimed by the device, otherwise they will be ignored.</p> <p></p> <p>It should be noted that BARs are not indicating to the host how much physical RAM they want to allocate. Instead, they are asking for a certain slice of the host's 64 bit address space to be reserved for them. This means if a device requests 2MiB of address space, the host firmware might allocate the address space <code>0x200000</code> to <code>0x400000</code> for this device. If the CPU ever performs a memory read or write within this address range, the request is directed away from RAM and instead routed into the PCIe topology and eventually the device itself. Indeed, this is the core principle of how MMIO works.</p> <p>We can see an example of what this looks like using <code>lspci</code>:</p> <pre><code>$ lspci -s 20:00.0 -vv\nRegion 0: Memory at 3f80000000 (64-bit, prefetchable) [size=16G]\nRegion 2: Memory at 3f00000000 (64-bit, non-prefetchable) [size=32M]\n</code></pre> <p>There are two regions of memory that are each described by a separate BAR. The first shows prefetchable memory of size 16G and the other is non-prefetchable control registers. Large prefetchable BARs (e.g., 16\u201364 GiB) often correspond to device memory apertures, such as GPU VRAM mappings.</p> <p>Prefetchable vs Non-Prefetchable</p> <p>Prefetchable memory is a kind of memory that has the following properties:</p> <ul> <li>Reads have no side effect</li> <li>Data behaves like normal memory</li> <li>Speculative reads are safe</li> <li>Read merging/reordering is safe.</li> </ul> <p>This type of memory is how normal RAM operates. Non-prefetchable memory on the other hand can behave in unintuitive ways:</p> <ul> <li>Reads may have side effects.</li> <li>Strict ordering of reads is required.</li> </ul> <p>These slices of memory behave more like registers where the order in which you interact with it must be specific, and in some cases the mere act of reading can cause the underlying data to mutate. Often, this type of memory is reserved for control registers.</p> <p>The distinction between these two tells the CPU and root complex how aggressively they are allowed to reorder or speculate on accesses to that region.</p>"},{"location":"blog/2026/02/15/gpu-virtualization-part-1-an-introduction-to-pcie-and-mmio/#mmio-is-not-dma","title":"MMIO is Not DMA","text":"<p>MMIO is a mechanism for the CPU to access a device, not for the device to access the CPU\u2019s memory. To understand this conceptually, we must make a distinction between what is the physical address space of a system and the underlying RAM that may or may not back it. The address space of a 64 bit system is every value between <code>0x0000000000000000</code> and <code>0xFFFFFFFFFFFFFFFF</code>. When a system boots up, it's given an address map that looks something like this:</p> <pre><code>0x00000000 - 0x7fffffff    -&gt; RAM\n0x80000000 - 0x8fffffff    -&gt; PCIe MMIO\n[...]\n</code></pre> <p>If the CPU makes an access to 0x10000000, the request is routed to the system memory controller because that address range is backed by RAM. If the CPU makes a request to <code>0x80000000</code>, the Root Complex forwards the address down the root port whose window encompasses this value. This does not grant the device access to system memory. Devices write to system memory only through DMA, which is separately controlled by the IOMMU.</p> <p>It is important to keep the direction of the transaction in mind:</p> <ul> <li>MMIO: CPU -&gt; device (based on physical address windows)</li> <li>DMA: device -&gt; system memory (subject to IOMMU enforcement)</li> </ul>"},{"location":"blog/2026/02/15/gpu-virtualization-part-1-an-introduction-to-pcie-and-mmio/#system-memory-maps-in-linux","title":"System Memory Maps in Linux","text":"<p>You can view Linux's address map by inspecting the <code>/proc/iomem</code> special file (you need to read this file as root, otherwise you'll get all zeros). In a VM I'm running on my MacOS, we see in this file the MMIO regions allocated for the PCIe devices:</p> <pre><code>10000000-1000ffff : GICD\n10010000-1004ffff : GICR\n20060000-20060fff : ARMH0061:00\n  20060000-20060fff : ARMH0061:00 ARMH0061:00\n40000000-4fffffff : PCI ECAM\n50000000-6ffdffff : PCI Bus 0000:00\n  50000000-50000fff : 0000:00:0f.0\n    50000000-50000fff : xhci-hcd\n  50001000-500013ff : 0000:00:0f.0\n  50001400-5000147f : 0000:00:05.0\n  50001480-500014ff : 0000:00:0a.0\n  50001500-5000157f : 0000:00:0b.0\n  50001580-500015ff : 0000:00:0c.0\n  50001600-5000163f : 0000:00:01.0\n  50001640-5000167f : 0000:00:06.0\n  50001680-500016bf : 0000:00:07.0\n  500016c0-500016ff : 0000:00:08.0\n  50001700-5000173f : 0000:00:09.0\n  50001740-5000177f : 0000:00:0d.0\n  50001780-500017bf : 0000:00:0e.0\n</code></pre> <p>We also see the system RAM mapped to various regions:</p> <pre><code>ee330000-ee33ffff : reserved\nee340000-ee56ffff : System RAM\n  ee452000-ee452fff : reserved\nee570000-ee6fffff : reserved\nee700000-efafffff : System RAM\n  ef58e000-efafffff : reserved\nefb00000-efb8ffff : reserved\nefb90000-efb9ffff : System RAM\n  efb90000-efb9ffff : reserved\n</code></pre>"},{"location":"blog/2026/02/15/gpu-virtualization-part-1-an-introduction-to-pcie-and-mmio/#looking-forward","title":"Looking Forward","text":"<p>PCIe is a huge spec and there are a lot of interesting topics I won't cover here, but here are some other interesting concepts:</p> <ul> <li>SR-IOV: A technology that allows PCIe devices to expose multiple endpoints at once, each with their own set of BARs. This is used as a type of device virtualization that is mediated by the device firmware itself.</li> <li>Address Translation Service: A distributed cache of the result of page table walks in the IOMMU. This speeds up DMA transactions.</li> <li>Device-level interrupts: how do devices tell each other and the CPU when they must be attended to?</li> <li>Posted vs non-posted transactions: Posted transactions are essentially like UDP packets that don't expect an acknowledgement, where non-posted is more like TCP where it requires a response.</li> <li>Resizable BARs.</li> </ul> <p>We've learned in this post how the PCIe topology works, how packets are routed through the fabric, how the CPU talks to devices through MMIO, how devices safely perform DMA to the host memory, and how we can enforce P2P transactions via IOMMU Domains. This provides for us a solid foundation for understanding higher-level topics like GPU virtualization. In the next article, we'll explore more of the Linux kernel's perspective on device management and how it attaches drivers to PCIe endpoints. There, we will learn the common method for GPU interactions where NVIDIA kernel drivers are loaded directly into the host environment. We'll also learn what VFIO is and how it enables passthrough virtualization into VMs.</p> <p>This article was a long one, and if you made it this far: congratulations! You've learned more about one of the most foundational technologies in modern computing than most software engineers will ever know. You are well positioned to understand the higher levels of abstractions we'll explore. As always, I invite comments and corrections using the Giscus comment system below. All you need is to link your GitHub account.</p>"},{"location":"blog/2026/02/15/gpu-virtualization-part-1-an-introduction-to-pcie-and-mmio/#references","title":"References","text":"<ul> <li>https://www.edn.com/the-pci-express-switch-and-bridge-landscape/</li> <li>https://stackoverflow.com/questions/49050847/how-is-pci-segmentdomain-related-to-multiple-host-bridgesor-root-bridges#:~:text=Unfortunately%2C%20the%20word%20PCI%20domain,port%20is%20a%20PCIe%20link.</li> <li>https://assets.topofmind.dev/posts/2026-02-15-pcie-mmio/pci-sig-sr-iov-primer-sr-iov-technology-paper.pdf</li> <li>https://www.semisaga.com/2019/07/pcie-tlp-header-packet-formats-address.html</li> <li>https://assets.topofmind.dev/posts/2026-02-15-pcie-mmio/pcie_spec_rev5.pdf</li> </ul> <ol> <li> <p>Recall that I said before that traditional PCIe form factors for consumer-grade hardware can go up to 16 lanes, but these datacenter grade GPUs, which are often using SXM form factors, can go far beyond this limit!\u00a0\u21a9</p> </li> <li> <p>NVIDIA GPUs often support 48 bits to 52 bit IOVA widths, but other devices can support substantially smaller widths.\u00a0\u21a9</p> </li> <li> <p>An IOMMU Group is a Linux kernel concept that basically specifies which devices the kernel believes can be properly separated by ACS and which cannot. Devices that connect to a switch that doesn't have ACS capabilities cannot be isolated from each other, and will consequently be placed into the same IOMMU Group. This has specific implications. If two devices are placed into the same group and you try to pass only one of them into a VM, the kernel will refuse to let you do so. This is because you would be attempting to create an IOMMU Domain that does not have all of the members of that IOMMU Group present. This should make sense: devices which cannot be hardware-isolated from each other should not be shared amongst VMs. It's all or nothing.\u00a0\u21a9</p> </li> <li> <p>For more information on what this looks like, read the Linux VFIO documentation and look for references for IOMMU Groups.\u00a0\u21a9</p> </li> <li> <p>BARs can be either 32 or 64 bit. If the BAR is 64 bit, bits <code>2:1</code> are set to <code>10b</code> to indicate as such.\u00a0\u21a9</p> </li> </ol>"},{"location":"misc/suggested-readings/","title":"Suggested Readings","text":""},{"location":"misc/suggested-readings/#blogs","title":"Blogs","text":"<ul> <li>Charity Majors: https://charity.wtf/</li> <li>https://www.allthingsdistributed.com/2023/07/building-and-operating-a-pretty-big-storage-system.html</li> <li>https://staffeng.com/</li> <li>Ilya Grigorik</li> </ul>"},{"location":"misc/suggested-readings/#github-history","title":"Github History","text":"<ul> <li>GIANT BUG... so sorry</li> </ul>"},{"location":"misc/suggested-readings/#starlink-mobile-setup","title":"Starlink Mobile Setup","text":"<ul> <li>https://boathackers.com/starlink/</li> <li>https://www.amazon.com/MSAFF-Injector-Starlink-Protection-Developed/dp/B0CQRFCKGL/ref=asc_df_B0CQRFCKGL/?tag=hyprod-20&amp;linkCode=df0&amp;hvadid=693554129961&amp;hvpos=&amp;hvnetw=g&amp;hvrand=10529417453291349876&amp;hvpone=&amp;hvptwo=&amp;hvqmt=&amp;hvdev=c&amp;hvdvcmdl=&amp;hvlocint=&amp;hvlocphy=9021723&amp;hvtargid=pla-2283102838943&amp;psc=1&amp;mcid=0cd55bee729c37d1b1bfe082d65f36cd</li> <li>https://www.amazon.com/XLTTYWL-Starlink-Protection-Conversion-Converter/dp/B0D4DH9TDN/ref=asc_df_B0D4DH9TDN/?tag=hyprod-20&amp;linkCode=df0&amp;hvadid=702491679579&amp;hvpos=&amp;hvnetw=g&amp;hvrand=10529417453291349876&amp;hvpone=&amp;hvptwo=&amp;hvqmt=&amp;hvdev=c&amp;hvdvcmdl=&amp;hvlocint=&amp;hvlocphy=9021723&amp;hvtargid=pla-2341583128284&amp;psc=1&amp;mcid=a88e8fe5fb4c37759a6158a26ff4c15f</li> </ul>"},{"location":"misc/suggested-readings/#tech","title":"Tech","text":"<ul> <li>The BSD Packet Filter: A New Architecture for User-level Packet Capture</li> <li>The UNIX-Haters Handbook</li> </ul>"},{"location":"misc/suggested-readings/#poems","title":"Poems","text":"Good Timber \u2013 Douglas Malloch <p> The tree that never had to fight For sun and sky and air and light, But stood out in the open plain And always got its share of rain, Never became a forest king <p>But lived and died a scrubby thing.</p> <p>The man who never had to toil To gain and farm his patch of soil, Who never had to win his share Of sun and sky and light and air, Never became a manly man</p> <p>But lived and died as he began.</p> <p>Good timber does not grow with ease: The stronger wind, the stronger trees; The further sky, the greater length; The more the storm, the more the strength. By sun and cold, by rain and snow,</p> <p>In trees and men good timbers grow.</p> <p>Where thickest lies the forest growth, We find the patriarchs of both. And they hold counsel with the stars Whose broken branches show the scars Of many winds and much of strife.</p> <p>This is the common law of life. </p> If \u2013 Rudyard Kipling <p> If you can keep your head when all about you     Are losing theirs and blaming it on you, If you can trust yourself when all men doubt you,     But make allowance for their doubting too; If you can wait and not be tired by waiting,     Or being lied about, don\u2019t deal in lies, Or being hated, don\u2019t give way to hating,     And yet don\u2019t look too good, nor talk too wise: <p>If you can dream\u2014and not make dreams your master;     If you can think\u2014and not make thoughts your aim; If you can meet with Triumph and Disaster     And treat those two impostors just the same; If you can bear to hear the truth you\u2019ve spoken     Twisted by knaves to make a trap for fools, Or watch the things you gave your life to, broken,     And stoop and build \u2019em up with worn-out tools:</p> <p>If you can make one heap of all your winnings     And risk it on one turn of pitch-and-toss, And lose, and start again at your beginnings     And never breathe a word about your loss; If you can force your heart and nerve and sinew     To serve your turn long after they are gone, And so hold on when there is nothing in you     Except the Will which says to them: \u2018Hold on!\u2019</p> <p>If you can talk with crowds and keep your virtue,     Or walk with Kings\u2014nor lose the common touch, If neither foes nor loving friends can hurt you,     If all men count with you, but none too much; If you can fill the unforgiving minute     With sixty seconds\u2019 worth of distance run, Yours is the Earth and everything that\u2019s in it,     And\u2014which is more\u2014you\u2019ll be a Man, my son! </p>"},{"location":"misc/word-of-the-day/","title":"Word of the Day","text":""},{"location":"misc/word-of-the-day/#2025-05-08-uncurried","title":"2025-05-08 <code>uncurried</code>","text":"<p>In computing, \"uncurried\" refers to a function that takes all its arguments at once, as opposed to a curried function which takes arguments one at a time. Uncurrying is essentially the reverse of currying, transforming a series of functions that each take one argument into a single function that takes all arguments at once.</p> <p>Currying</p> <p>A curried function is one that is designed to be partially applied. This means you can call it with some of its arguments, and it will return a new function that is waiting for the remaining arguments. </p> <p>Uncurrying</p> <p>Uncurrying reverses this process. It takes a sequence of functions (each taking a single argument) and converts them into a single function that accepts all arguments as a tuple or list. </p> <p>Example</p> <p>Imagine a function <code>add(x, y)</code> which adds two numbers. A curried version of this might be <code>curriedAdd(x)(y)</code>. An uncurried version would be <code>uncurriedAdd(x, y)</code>. </p> <p>Benefits of Uncurrying</p> <p>In some programming environments, uncurried functions are generally preferred for efficiency because they avoid the overhead of creating closures and partial application during function calls, according to Wikipedia. </p>"},{"location":"misc/word-of-the-day/#2025-02-06-promulgate","title":"2025-02-06 <code>promulgate</code>","text":"<p>promote or make widely known (an idea or cause).</p> <p>\"these objectives have to be promulgated within the organization\"</p> similar <p>make known</p> <p>make public</p> <p>publicize</p> <p>spread</p> <p>communicate</p> <p>propagate</p> <p>disseminate</p> <p>circulate</p> <p>broadcast</p> <p>promote</p> <p>announce</p> <p>proclaim</p> <p>bruit about</p>"},{"location":"misc/word-of-the-day/#2024-06-20-recalcitrant","title":"2024-06-20 <code>recalcitrant</code>","text":"<p>having an obstinately uncooperative attitude toward authority or discipline.</p> similar <p>uncooperative</p> <p>obstinately disobedient</p> <p>intractable</p> <p>unmanageable</p> <p>ungovernable</p>"},{"location":"misc/word-of-the-day/#2024-04-25-comport","title":"2024-04-25 <code>comport</code>","text":"<p>FORMAL</p> <p>conduct oneself; behave.</p> <p>\"articulate students who comported themselves well in television interviews\"</p> similar <p>conduct oneself</p> <p>acquit oneself</p> <p>behave</p> <p>act</p> <p>perform</p> <p>deport oneself</p> <p>US</p> <p>accord or agree with.</p> <p>\"our outdated rules did not comport with 21st-century realities\"</p>"},{"location":"misc/word-of-the-day/#2024-04-04-clandestine","title":"2024-04-04 <code>clandestine</code>","text":"<p>kept secret or done secretively, especially because illicit.</p> similar <p>secret</p> <p>covert</p> <p>furtive</p> <p>surreptitious</p> <p>stealthy</p> <p>cloak-and-dagger</p> <p>hole-and-corner</p> <p>hole-in-the-corner</p> <p>closet</p> <p>behind-the-scenes</p> <p>backstairs</p> <p>back-alley</p> <p>under-the-table</p> <p>hugger-mugger</p> <p>concealed</p> <p>hidden</p> <p>private</p> <p>sly</p> <p>sneaky</p> <p>underhand</p> <p>undercover</p> <p>underground</p> <p>black</p> <p>hush-hush</p>"},{"location":"misc/word-of-the-day/#2024-03-28-vociferous","title":"2024-03-28 <code>vociferous</code>","text":"<p>(especially of a person or speech) vehement or clamorous.</p>"},{"location":"misc/word-of-the-day/#2024-03-19-reticent","title":"2024-03-19: <code>reticent</code>","text":"<p>Not revealing one's thoughts or feelings readily.</p>"},{"location":"notes/","title":"Notes","text":"<p>Herein lies all of my working notes. This is a broad category of topics that relate to what I do in my day-to-day profession.</p>"},{"location":"notes/programming/bash/","title":"Bash","text":""},{"location":"notes/programming/bash/#pipelines","title":"Pipelines","text":"<p>Pipelines are the bread and butter of Unix scripting. The stdout of a process can be redirected to the stdin of another process like so:</p> Text Only<pre><code>cat out.csv | grep cookies | cut -d',' -f 2 | sort | uniq -c\n</code></pre> <p>You can also include stderr in the pipe as such:</p> Text Only<pre><code>cat out.txt |&amp; grep cookies\n</code></pre>"},{"location":"notes/programming/bash/#stdoutstderr-redirection","title":"Stdout/Stderr redirection","text":"<p>Redirects just stdout:</p> Text Only<pre><code>echo hello &gt; hello.txt\n</code></pre> <p>Redirects stdout, then sets stderr to point to what stdout is pointing to:</p> Text Only<pre><code>grep chocolate cookies.csv &gt; choco_cookies.csv 2&gt;&amp;1\n</code></pre> <p>This is more succinctly written as:</p> Text Only<pre><code>grep chocolate cookies.csv &amp;&gt; choco_cookies.csv\n</code></pre>"},{"location":"notes/programming/bash/#here-string","title":"Here String","text":"Bash<pre><code>$ foobar &lt;&lt;&lt;\"hello world\"\n</code></pre> <p>Here, the string <code>\"hello world\"</code> is redirected to <code>foobar</code>'s stdin.</p>"},{"location":"notes/programming/bash/#here-document","title":"Here document","text":"Bash<pre><code>$ foobar &lt;&lt; END\n&gt; hello world\n&gt; today is a great day\n&gt; goodbye\n&gt; END\n</code></pre> <p>A heredoc is a multi-line string that is treated as a file literal. The heredoc itself is redirected into the command's stdin, as can be seen in this example:</p> Bash<pre><code>$ strace cat &lt;&lt; EOF\n&gt; hello world\n&gt; EOF\n...\nread(0, \"hello world\\n\", 131072)        = 12\nwrite(1, \"hello world\\n\", 12)           = 12\n</code></pre>"},{"location":"notes/programming/bash/#backgrounding-a-terminal-process","title":"Backgrounding a Terminal Process","text":"<p>Consider if we want to grep for the lines in  <code>cookies.csv</code> that contain <code>snickerdoodle</code>:</p> Text Only<pre><code>grep snickerdoodle cookies.csv\n</code></pre> <p>But we find this is taking a long time to complete because, as it turns out, our cookie addiction is 5 TiB large. You can send the Ctrl+Z combination to send the SIGTSTP signal (terminal stop), followed by executing <code>bg</code> to send the stopped process to the background.</p> Text Only<pre><code>$ bash /tmp/long_cookies.sh\nsnickerdoodle,cookies\nsnickerdoodle,cookies\nsnickerdoodle,cookies\n^Z\n[1]  + 82721 suspended  bash /tmp/long_cookies.sh\n$ bg\n[1]  + 82721 continued  bash /tmp/long_cookies.sh\nsnickerdoodle,cookies\n$ jobs\n[1]  + running    bash /tmp/long_cookies.sh\n</code></pre> <p>The <code>jobs</code> command indicates the background jobs being run. You can use the <code>fg</code> command to bring a specific background job to the foreground.</p> Text Only<pre><code>fg %1\n[1]  + 83010 running    bash /tmp/long_cookies.sh\nsnickerdoodle,cookies\nsnickerdoodle,cookies\n^C\n</code></pre>"},{"location":"notes/programming/bash/#process-substituion","title":"Process Substituion","text":"<p>Process substition allows the stdout of a process to be used as a file for the input of another process.</p> Text Only<pre><code>cat &lt;(echo hello)\n</code></pre> <p>Gets \"rendered\" as:</p> Text Only<pre><code>cat /dev/fd/11\n</code></pre> <p>Where <code>/dev/fd/11</code> is a file descriptor referencing the stdout of the <code>echo</code> command.</p> <p>You may also use process substitution to write to a file and pipe that file as an input to the stdout of another process:</p> Text Only<pre><code>ncdu -o &gt;(jq .)\n</code></pre> <p>This is roughly equivalent to:</p> Text Only<pre><code>ncdu -o out.json  &amp;\njq . &lt; out.json\n</code></pre>"},{"location":"notes/programming/bash/#double-bracket-test","title":"Double Bracket Test <code>[[</code>","text":"<p>This is an extended form of test.</p>"},{"location":"notes/programming/bash/#string-truthiness","title":"String Truthiness","text":"<p>Non-empty strings in <code>[[</code> are considered falsey:</p> Text Only<pre><code> $ [[ \"foobar\" ]] &amp;&amp; echo true\ntrue\n</code></pre> <p>Versus:</p> Text Only<pre><code> $ [[ \"\" ]] &amp;&amp; echo true\n</code></pre>"},{"location":"notes/programming/computer-science/","title":"Computer Science","text":"<p>This page describes various algorithmic techniques and terms related to computer science.</p>"},{"location":"notes/programming/computer-science/#memoization","title":"Memoization","text":"<p>Memoization refers to caching the result of an expensive function call given a set of inputs.</p>"},{"location":"notes/programming/computer-science/#optimal-substructure","title":"Optimal Substructure","text":"<p>Optimal Substructure is a term used to describe problems where the optimal solution can be found by finding the optimal solution to its subproblems. Greedy algorithms can be used to solve such problems if it can be proven by induction that the solution can be optimal at each step.</p>"},{"location":"notes/programming/computer-science/#trees","title":"Trees","text":""},{"location":"notes/programming/computer-science/#trie","title":"Trie","text":"<p>A Prefix Tree, or a trie, is a type of search tree used for locating specific keys from within a set. Each node of the trie represents a common prefix that all of its descendents contain. The name \"trie\" (pronounced try) comes from the word retrieval.</p> <p>Each node stores a single character, and it has at most 26 children, one for each letter of the alphabet.</p>"},{"location":"notes/programming/computer-science/#heap","title":"Heap","text":"<p>A heap is a type of tree that satisfies the heap property: in max heaps, each node is the maximum-valued node for its subtree. For min-heaps, each node is the minimum-valued node for its subtree. Heaps are good for:</p> <ul> <li>Priority queues</li> <li>Heapsort</li> <li>Graph algorithms (Dijkstra)</li> <li>Finding the largest or smallest element in a dataset</li> </ul> <p>When encountering problems requiring us to find the largest or smallest element, it's often insufficient to simply sort the entire dataset because the dataset may be growing/shrinking over time. Sorting the dataset during each change causes a large number of operations. This is why heaps are useful in situations like this because adding or removing an element is \\(O(log(n))\\).</p>"},{"location":"notes/programming/computer-science/#backpressure","title":"Backpressure","text":"<p>Not necessarily a computer science topic, but it describes a scenario where a consumer of some resource (whether it be a queue, or a proxy, etc) cannot consume faster than things are pushing to this resource. This situation will eventually cause the resource to become overloaded as it queues the requests in-memory (or disk) for the consumer.</p> <p>Strategies to mitigate backpressure:</p> <ol> <li>Vertical scaling (just make your queue bigger lulz)</li> <li>Increase consumer throughput. This could happen by vertically scaling the consumer if resources are a constraint, or by adding more consumers (horizontal scaling).</li> <li>Drop some sample of incoming requests. Eventually, this will happen anyway as your resource begins to time out, so maybe it's better for you to have explicit control on what gets dropped.</li> <li>Throttle incoming requests.</li> </ol>"},{"location":"notes/programming/computer-science/#data-locality","title":"Data Locality","text":"<p>Data locality refers to how data is laid out in memory or disk. In general, read requests for a particular data structure will read things sequentially, meaning that a request for an element at index <code>n</code>, for example, will often be followed up by requests at <code>n+1</code>. In most memory and storage systems, data retrieval for <code>n</code> will cause large blocks of data to be pulled from the backing storage and cached in memory, so we generally want <code>n+1</code> to be local to <code>n</code>. This data locality trait will allow us to take advantage of these caching systems and improve performance.</p> <p>In computer memory, the CPU has three layers of cache: L1 (typically the smallest), L2 (larger), and L3 (the largest, shared by all cores). Main memory access is extremely slow while each layer of the CPU cache is progressively faster than the higher layer. This is why it's preferrable to take full advantage of each caching layer, instead of missing back to main memory on every access.</p>"},{"location":"notes/programming/computer-science/#2s-complement","title":"2's Complement","text":"<p>This is one of the most fundamental ways to represent a signed integer in binary format.</p> binary decimal integer <code>100</code> <code>-4</code> <code>101</code> <code>-3</code> <code>110</code> <code>-2</code> <code>111</code> <code>-1</code> <code>000</code> <code>0</code> <code>001</code> <code>1</code> <code>010</code> <code>2</code> <code>011</code> <code>3</code> <p>The range of negative integers 2's complement can support is \\(\\frac{2^n}{2}\\) while the positive integers are \\(\\frac{2^n}{2} - 1\\).</p>"},{"location":"notes/programming/computer-science/#np-hardness","title":"NP-hardness","text":"<p>Stands for Non-deterministic Polynomial-time hardness. Refers to a set of problems where the solution is \"at least as hard as the hardest problems in NP\".</p>"},{"location":"notes/programming/golang/","title":"Golang","text":""},{"location":"notes/programming/golang/#garbage-collector","title":"Garbage Collector","text":"<p>https://tip.golang.org/doc/gc-guide</p> <p>Go is a garbage collected language. When a new variable is allocated (either with <code>make</code> or through literal declarations), the Go escape analyzer (which is run at compile time) determines if a particular variable should escape to the heap, or if it can remain on the stack. The garbage collector cleans heap-allocated data.</p>"},{"location":"notes/programming/golang/#oop-in-go","title":"OOP (in Go)","text":"<p>The four pillars of OOP:</p> <ol> <li>Polymorphism</li> <li>Abstraction</li> <li>Encapsulation</li> <li>Inheritance</li> </ol> <p>Go has all of the pillars except for 4. A common misconception is that Go's type embedding constitutes inheritance, but embedding is only syntatic sugar around composition. For example, an embedded type does not have access to the \"children\" structs. Thus, it doesn't have true inheritance.</p>"},{"location":"notes/programming/golang/#goroutines","title":"Goroutines","text":"<p>A goroutine is essentially a \"thread\" of execution (not to be confused with an OS thread) that is managed by the Go runtime. goroutines are multiplexed and scheduled onto OS threads according to the needs of the goroutine. goroutines that are waiting on IO are put into a sleep state until they are ready to handle the IO.</p>"},{"location":"notes/programming/golang/#functional-options","title":"Functional Options","text":"<p>Functional Options are a style of specifying optional arguments. This is needed to get around the fact that Go does not have default parameter values. It works by creating a type that is defined as a function that takes, as its input a config struct, and returns nothing. The function will set a specific attribute in the config to the value passed in to the constructor. For example, <code>type FuncOpt func(c *Config)</code>. Here is an example:</p> Go<pre><code>package main\n\nimport \"fmt\"\n\ntype Person struct {\n    name   string\n    age    int\n    height int\n}\n\ntype PersonOpt func(p *Person)\n\nfunc PersonName(name string) PersonOpt {\n    return func(p *Person) {\n        p.name = name\n    }\n}\n\nfunc PersonAge(age int) PersonOpt {\n    return func(p *Person) {\n        p.age = age\n    }\n}\n\nfunc PersonHeight(height int) PersonOpt {\n    return func(p *Person) {\n        p.height = height\n    }\n}\n\nfunc NewPerson(opts ...PersonOpt) Person {\n    person := Person{\n        name:   \"Landon\",\n        age:    2,\n        height: 500,\n    }\n\n    for _, opt := range opts {\n        opt(&amp;person)\n    }\n    return person\n}\nfunc main() {\n    person := NewPerson(PersonHeight(175))\n    fmt.Printf(\"%+v\\n\", person)\n}\n</code></pre> <pre><code>{name:Landon age:2 height:175}\n</code></pre> <p>The meat of this methodology lies within the <code>PersonOpt</code> constructor. Let's zoom in:</p> Go<pre><code>type Person struct {\n    name   string\n    age    int\n    height int\n}\n\ntype PersonOpt func(p *Person)\n\nfunc PersonName(name string) PersonOpt {\n    return func(p *Person) {\n        p.name = name\n    }\n}\n</code></pre> <p>You can see that <code>PersonName</code> is one such constructor that takes in a string, and it returns a function that modifies the value of the <code>Person</code> struct with the value \"<code>name</code>\" passed into the <code>PersonOpt</code> constructor. It relies on the fact that Go functions can be closures</p>"},{"location":"notes/programming/golang/#tools-as-dependencies","title":"Tools as Dependencies","text":"<p>stub</p> <p>TODO: Show the <code>tools/</code> sub-module approach to setting tool dependencies.</p>"},{"location":"notes/programming/golang/#types","title":"Types","text":""},{"location":"notes/programming/golang/#rune","title":"rune","text":"<p>A rune is an alias for <code>int32</code>. It represents a code point in the Unicode standard. The elements of a string (which are just singular bytes) can be converted to a rune through a simple <code>rune(elem)</code> type conversion. It's important to know that some characters in the Unicode specification can be represented by different bytes. For example, \u00e0 can be represented in two ways: either it's explicit code point U+00E0, or through a sequence of two separate code points of U+0061 (which is the lowercase <code>a</code>) and U+0300 (which is the \"combining\" grave access point, which can be applied to any character). This form of character representation would require two elements in a string type in Go because it requires two bytes, but can be easily represented by a single rune.</p>"},{"location":"notes/programming/golang/#channels","title":"Channels","text":"<p>Receive</p> Go<pre><code>foo := &lt;-chanVar\n</code></pre> <p>Send</p> Go<pre><code>chanVar &lt;- 5\n</code></pre> <p>Channel directions</p> Go<pre><code>func receiveOnly(chanVar &lt;-chan int){}\nfunc sendOnly(chanVar chan&lt;- int){}\nfunc sendOrReceive(chanVar chan int){}\n</code></pre>"},{"location":"notes/programming/golang/#maps","title":"Maps","text":"Go<pre><code>mapVar := map[string]int{}\nmapVar = make(map[string]int)\n</code></pre>"},{"location":"notes/programming/golang/#slices","title":"Slices","text":"<p>https://go.dev/blog/slices-intro</p> <p>Slices are a metadata view over a dynamically-sized array. A slice contains:</p> <ol> <li>A reference to the underlying array</li> <li>The length of our view in the underlying array (which serves as a boundary)</li> <li>A capacity, which is the the number of elements between our pointer to the underlying array, and its end.</li> </ol> <p>Slices are automatically expanded by the runtime when we run out of capacity. It does this by first allocating a new array that is twice the size of the original, then copies the contents of the new array. You can see this expansion behavior as such:</p> Go<pre><code>package main\n\nimport \"fmt\"\n\nfunc main() {\n    s := []int{}\n    for i := 0; i &lt; 100; i++ {\n        s = append(s, i)\n        fmt.Printf(\"length: %v capacity: %v\\n\", len(s), cap(s))\n    }\n}\n</code></pre> <pre><code>length: 1 capacity: 1\nlength: 2 capacity: 2\nlength: 3 capacity: 4\nlength: 4 capacity: 4\nlength: 5 capacity: 8\nlength: 6 capacity: 8\nlength: 7 capacity: 8\nlength: 8 capacity: 8\nlength: 9 capacity: 16\nlength: 10 capacity: 16\nlength: 11 capacity: 16\nlength: 12 capacity: 16\n...\n</code></pre> <p>When you allocate slices, you can specify the capacity of the underlying array:</p> Go<pre><code>package main\n\nimport \"fmt\"\n\nfunc main() {\n    s := make([]int, 0, 100)\n    for i := 0; i &lt; 100; i++ {\n        s = append(s, i)\n        fmt.Printf(\"length: %v capacity: %v\\n\", len(s), cap(s))\n    }\n}\n</code></pre> <pre><code>length: 1 capacity: 100\nlength: 2 capacity: 100\nlength: 3 capacity: 100\nlength: 4 capacity: 100\n</code></pre>"},{"location":"notes/programming/golang/#preferred-readings","title":"Preferred Readings","text":""},{"location":"notes/programming/golang/#go-interface-values-deep-dive","title":"Go Interface Values Deep Dive","text":"<p>https://github.com/akutz/go-interface-values/tree/main</p>"},{"location":"notes/programming/golang/#a-guide-to-the-go-garbage-collector","title":"A Guide to the Go Garbage Collector","text":"<p>https://tip.golang.org/doc/gc-guide</p>"},{"location":"notes/programming/golang/#faster-software-through-register-based-calling","title":"Faster software through register based calling","text":"<p>https://menno.io/posts/golang-register-calling/</p> <p>My coworker Menno wrote about register-based calling in Go 1.17. He uses this example program:</p> Go<pre><code>package main\n\nimport \"fmt\"\n\nfunc add(i, j int) int {\n    return i + j\n}\n\nfunc main() {\n    z := add(22, 33)\n    fmt.Println(z)\n}\n</code></pre> <p>When adding two integers in go, the compiler will make something like this:</p> Text Only<pre><code>    movq   $0x16,(%rsp)\n    movq   $0x21,0x8(%rsp)\n    call   4976e0 &lt;main.add&gt;\n    mov    0x10(%rsp),%rax\n    mov    %rax,0x30(%rsp)\n</code></pre> <p>It moves the literal values <code>$0x16</code> and <code>$0x21</code> onto the memory location pointed to in <code>(%rsp)</code> and <code>(%rsp)+0x8</code>. The results are then retrieved off of the stack and are put into <code>%rax</code>.</p> <p>In 1.18, Go prefers passing arguments through registers:</p> Text Only<pre><code>    mov    $0x16,%eax\n    mov    $0x21,%ebx\n    xchg   %ax,%ax\n    call   47e260 &lt;main.add&gt;\n</code></pre> <p>Something else Menno noted was that Go doesn't use the <code>push</code> instruction and instead uses <code>mov</code> to push values onto the stack. I was initially confused by this as well, but he notes that <code>mov</code> is generally faster: https://agner.org/optimize/instruction_tables.pdf</p>"},{"location":"notes/programming/golang/#generics-degrade-performance","title":"Generics Degrade Performance","text":"<p>https://planetscale.com/blog/generics-can-make-your-go-code-slower</p> <p>Accessing the function pointer if you call a generic function with a pointer to an interface requires three dereferences:</p> Text Only<pre><code>MOVQ \"\"..dict+48(SP), CX # load the virtual table from the stack into CX\n0094  MOVQ 64(CX), CX.    # dereference the dictionary to get the *itab\n0098  MOVQ 24(CX), CX.    # Offset by 24 bytes to load the function pointer within *itab\n</code></pre> <p>Another interesting point, passing an interface to a generic function where the interface is a superset of the type constraint is what the author calls a \"footcannon,\" because the compiler does this nonsense:</p> Text Only<pre><code>00b6  LEAQ type.io.ByteWriter(SB), AX\n00bd  MOVQ \"\"..autotmp_8+40(SP), BX\n00c2  CALL runtime.assertI2I(SB)\n00c7  MOVQ 24(AX), CX\n00cb  MOVQ \"\".buf+80(SP), AX\n00d0  MOVL $92, BX\n00d5  CALL CX\n</code></pre> <p>The compiler is asserting that the interface you passed does indeed implement the interface in the type constraint (runtimeassertI2I), and it grabs the *itab of the interface in the type constraint. This tanks performance.</p>"},{"location":"notes/programming/golang/#rob-pike-the-design-of-the-go-assmebler","title":"Rob Pike - The Design of the Go Assmebler","text":"<p>https://www.youtube.com/watch?v=KINIAgRpkDA</p>"},{"location":"notes/programming/javascript/","title":"Javascript","text":"<p>Note: Javascript and Typescript will be used somewhat interchangeably in this page. Although, preference will be given to Typescript because why tf would you not type your languages?</p>"},{"location":"notes/programming/javascript/#keyword-arguments","title":"Keyword Arguments","text":"<p>Typescript does not have keyword arguments like Python, but you can emulate it with Object Destructuring:</p> TypeScript<pre><code>function greet({ firstName, lastName }) {\n    console.log(`Hello, ${firstName} ${lastName}!`);\n}\n\n// Calling the function with an object\ngreet({ firstName: \"John\", lastName: \"Doe\" }); // Output: Hello, John Doe!\n</code></pre> <p>You can provide default values:</p> TypeScript<pre><code>function greet({ firstName = \"John\", lastName = \"Doe\" } = {}) {\n    console.log(`Hello, ${firstName} ${lastName}!`);\n}\n\n// Calling the function with missing properties\ngreet({ firstName: \"Jane\" }); // Output: Hello, Jane Doe!\ngreet({}); // Output: Hello, John Doe!\ngreet(); // Output: Hello, John Doe!\n</code></pre>"},{"location":"notes/programming/leetcode/","title":"Leetcode","text":"<p>Warning</p> <p>The code in this page represents, essentially, a \"code golf\" solution to the problem statements. These solutions are not necessarily how I would code something in a production setting, as I tend to emphasize readability and maintainability over speed.</p>"},{"location":"notes/programming/leetcode/#arraystring","title":"Array/String","text":""},{"location":"notes/programming/leetcode/#merged-sorted-array-easy","title":"Merged Sorted Array (easy)","text":""},{"location":"notes/programming/leetcode/#problem-statement","title":"Problem Statement","text":"<p>You are given two integer arrays nums1 and nums2, sorted in non-decreasing order, and two integers m and n, representing the number of elements in nums1 and nums2 respectively.</p> <p>Merge nums1 and nums2 into a single array sorted in non-decreasing order.</p> <p>The final sorted array should not be returned by the function, but instead be stored inside the array nums1. To accommodate this, nums1 has a length of m + n, where the first m elements denote the elements that should be merged, and the last n elements are set to 0 and should be ignored. nums2 has a length of n.</p> <p>Example 1:</p> Text Only<pre><code>Input: nums1 = [1,2,3,0,0,0], m = 3, nums2 = [2,5,6], n = 3\nOutput: [1,2,2,3,5,6]\nExplanation: The arrays we are merging are [1,2,3] and [2,5,6].\nThe result of the merge is [1,2,2,3,5,6] with the underlined elements coming from nums1.\n</code></pre> <p>Example 2:</p> Text Only<pre><code>Input: nums1 = [1], m = 1, nums2 = [], n = 0\nOutput: [1]\nExplanation: The arrays we are merging are [1] and [].\nThe result of the merge is [1].\n</code></pre> <p>Example 3:</p> Text Only<pre><code>Input: nums1 = [0], m = 0, nums2 = [1], n = 1\nOutput: [1]\nExplanation: The arrays we are merging are [] and [1].\nThe result of the merge is [1].\n</code></pre> <p>Note that because m = 0, there are no elements in nums1. The 0 is only there to ensure the merge result can fit in nums1.</p> <p>Constraints:</p> <ul> <li>nums1.length == m + n</li> <li>nums2.length == n</li> <li>0 &lt;= m, n &lt;= 200</li> <li>1 &lt;= m + n &lt;= 200</li> <li>-109 &lt;= nums1[i], nums2[j] &lt;= 109</li> </ul> <p>Follow up: Can you come up with an algorithm that runs in O(m + n) time?</p>"},{"location":"notes/programming/leetcode/#solution","title":"Solution","text":""},{"location":"notes/programming/leetcode/#intuition","title":"Intuition","text":"<p>Because we know that both arrays come pre-sorted, we can take advantage of the fact that <code>i+1</code> will always be &gt;= <code>i</code> for each array.</p>"},{"location":"notes/programming/leetcode/#approach","title":"Approach","text":"<p>I decide to use a cursor-based approach. We will allocate a new slice of size <code>m+n</code> and use two individual cursors that point into each array respectively. Whenever we \"consume\" an element from either array, we'll increment the corresponding cursor. We'll need to account for edge cases where we have consumed all available values in a particular array.</p>"},{"location":"notes/programming/leetcode/#complexity","title":"Complexity","text":"<ul> <li> <p>Time complexity: \\(O(m+n)\\)</p> </li> <li> <p>Space complexity: \\(O(m+n)\\)</p> </li> </ul>"},{"location":"notes/programming/leetcode/#code","title":"Code","text":"Go<pre><code>func merge(nums1 []int, m int, nums2 []int, n int)  {\n    merged := make([]int, m+n)\n    m_cursor := 0\n    n_cursor := 0\n    for i := 0; i &lt; m+n; i++ {\n        if m_cursor &gt;= m {\n            merged[i] = nums2[n_cursor]\n            n_cursor++\n            continue\n        }\n        if n_cursor &gt;= n {\n            merged[i] = nums1[m_cursor]\n            m_cursor++\n            continue\n        }\n\n\n        // Asumption in this block is that both m_cursor &lt; m and\n        // n_cursor &lt; n. So we need to account for the cases where\n        // we've consumed all the values from one or the other\n        if nums1[m_cursor] &lt; nums2[n_cursor] {\n            merged[i] = nums1[m_cursor]\n            m_cursor++\n        } else if nums2[n_cursor] &lt; nums1[m_cursor] {\n            merged[i] = nums2[n_cursor]\n            n_cursor++\n        } else {\n            // they must be equal, so pick an element arbitrarily\n            merged[i] = nums1[m_cursor]\n            m_cursor++\n        }\n    }\n    for i := 0; i &lt; m+n; i++ {\n        nums1[i] = merged[i]\n    }\n}\n</code></pre>"},{"location":"notes/programming/leetcode/#two-pointers","title":"Two Pointers","text":""},{"location":"notes/programming/leetcode/#valid-palindrome-easy","title":"Valid Palindrome (easy)","text":""},{"location":"notes/programming/leetcode/#problem-statement_1","title":"Problem Statement","text":"<p>A phrase is a palindrome if, after converting all uppercase letters into lowercase letters and removing all non-alphanumeric characters, it reads the same forward and backward. Alphanumeric characters include letters and numbers.</p> <p>Given a string s, return true if it is a palindrome, or false otherwise.</p> <p>Example 1:</p> Text Only<pre><code>Input: s = \"A man, a plan, a canal: Panama\"\nOutput: true\nExplanation: \"amanaplanacanalpanama\" is a palindrome.\n</code></pre> <p>Example 2:</p> Text Only<pre><code>Input: s = \"race a car\"\nOutput: false\nExplanation: \"raceacar\" is not a palindrome.\n</code></pre> <p>Example 3:</p> Text Only<pre><code>Input: s = \" \"\nOutput: true\nExplanation: s is an empty string \"\" after removing non-alphanumeric characters.\nSince an empty string reads the same forward and backward, it is a palindrome.\n</code></pre> <p>Constraints:</p> <ol> <li>1 &lt;= s.length &lt;= 2 * 105</li> <li>s consists only of printable ASCII characters.</li> </ol>"},{"location":"notes/programming/leetcode/#solution_1","title":"Solution","text":""},{"location":"notes/programming/leetcode/#intuition_1","title":"Intuition","text":"<p>This is a fairly simple problem. We need to normalize the input string to disregard non-alphanumeric characters. There are a few ways you can do this, but I intend to use an approach that utilizes the least amount of data copying.</p>"},{"location":"notes/programming/leetcode/#approach_1","title":"Approach","text":"<p>A naive approach would be to normalize the input string by copying each alphanumeric element into a new string and setting it to its lowercase representation. Then you could iterate over the normalized string and compare it to the opposing end of the string. However, this approach is costly as it requires lots of data copying and computation.</p> <p>Instead, I used a cursor approach where we iterate over each element of the string. We will <code>continue</code> the loop if we found a non-alphanumeric character. Additionally, we keep track of the \"opposing index\" of the string, what we'll call <code>oppositeCursor</code>. In the <code>for</code> loop, the <code>oppositeCursor</code> is decremented until we find an alphanumeric character. Once that is found, we compare the lower-case representation at <code>s[i]</code> and <code>s[oppositeCursor]</code> and if they don't match, then it is not a valid palindrome.</p>"},{"location":"notes/programming/leetcode/#complexity_1","title":"Complexity","text":"<ul> <li> <p>Time complexity: \\(O(n)\\)</p> </li> <li> <p>Space complexity: \\(O(1)\\)</p> </li> </ul>"},{"location":"notes/programming/leetcode/#code_1","title":"Code","text":"Go<pre><code>func isPalindrome(s string) bool {\n    oppositeCursor := len(s)-1\n    for i := 0; i &lt; len(s); i++ {\n        if i &gt; oppositeCursor {\n            break\n        }\n        if !isAlphaNumeric(s[i]) {\n            continue\n        }\n        for;!isAlphaNumeric(s[oppositeCursor]) &amp;&amp; i&lt;oppositeCursor ; oppositeCursor-- {}\n        if i &gt; oppositeCursor || strings.ToLower(string(s[i])) != strings.ToLower(string(s[oppositeCursor])) {\n            return false\n        }\n        oppositeCursor--\n    }\n    return true\n}\n\nfunc isAlphaNumeric(c byte) bool {\n    r := rune(c)\n    return unicode.IsLetter(r) || unicode.IsNumber(r)\n}\n</code></pre>"},{"location":"notes/programming/leetcode/#sliding-window","title":"Sliding Window","text":""},{"location":"notes/programming/leetcode/#minimum-size-subarray-sum-medium","title":"Minimum Size Subarray Sum (Medium)","text":""},{"location":"notes/programming/leetcode/#problem-statement_2","title":"Problem Statement","text":"<p>Given an array of positive integers nums and a positive integer target, return the minimal length of a subarray  whose sum is greater than or equal to target. If there is no such subarray, return 0 instead.</p> <p>Example 1:</p> Text Only<pre><code>Input: target = 7, nums = [2,3,1,2,4,3]\nOutput: 2\nExplanation: The subarray [4,3] has the minimal length under the problem constraint.\n</code></pre> <p>Example 2:</p> Text Only<pre><code>Input: target = 4, nums = [1,4,4]\nOutput: 1\n</code></pre> <p>Example 3:</p> Text Only<pre><code>Input: target = 11, nums = [1,1,1,1,1,1,1,1]\nOutput: 0\n</code></pre> <p>Constraints:</p> <p>1 &lt;= target &lt;= 109 1 &lt;= nums.length &lt;= 105 1 &lt;= nums[i] &lt;= 104</p> <p>Follow up: If you have figured out the O(n) solution, try coding another solution of which the time complexity is O(n log(n)).</p>"},{"location":"notes/programming/leetcode/#solution_2","title":"Solution","text":"<p>Naive (brute force):</p> Go<pre><code>func minSubArrayLen(target int, nums []int) int {\n    minLength := 0\n\nouterloop:\n    for left := 0; left &lt; len(nums); left++ {\n        cumulativeSum += left\n        sum := 0\n\n        for right := left; right &lt; len(nums); right++ {\n            sum += nums[right]\n            if sum &gt;= target {\n                length := right - left + 1\n                if minLength == 0 || minLength &gt; length {\n                    minLength = length\n                }\n                continue outerloop\n            }\n        }\n    }\n    return minLength\n}\n</code></pre> <p>Complexity Analysis</p> <ul> <li>Time: $O(n^2)</li> <li>The lefthand pointer of our subarray iterates over the entire <code>nums</code> array, which is \\(O(n)\\).</li> <li>For every position of <code>left</code>, we find every subarray starting from that point, which is \\(O(n)\\).</li> <li>Together, these operations are multiplied to become \\(O(n^2)\\)</li> <li>Space: \\(O(1)\\)</li> </ul> status language runtime memory Accepted Go 2884ms 8.4MB <p>Sliding Window:</p> Go<pre><code>func minSubArrayLen(target int, nums []int) int {\n    minLength := len(nums) + 1\n    sum := 0\n    left := 0\n    for right := 0; right &lt; len(nums); right++ {\n        sum += nums[right]\n        for sum &gt;= target {\n            minLength = min(minLength, right - left + 1)\n            sum -= nums[left]\n            left++\n        }\n    }\n\n    if minLength == len(nums) + 1 {\n        return 0\n    }\n    return minLength\n}\n</code></pre> status language runtime memory Accepted Go 28ms 7.8MB <p>This solution uses a dynamically-sized array. We start with the smallest non-zero subarray at the lefthand side and increase the array until its total sum is greater than or equal to the target. Then, we increment the lefthand pointer until the sum is below the target again. During each iteration where we are incrementing the lefthand pointer, we set <code>minLength</code> equal to the current subarray length if it's smaller than the last recorded <code>minLength</code>.</p> <p>The effect of this algorithm is that we only iterate over the entire subarray at most twice, which gives us \\(O(n)\\).</p>"},{"location":"notes/programming/leetcode/#matrix","title":"Matrix","text":""},{"location":"notes/programming/leetcode/#valid-sudoku-medium","title":"Valid Sudoku (Medium)","text":""},{"location":"notes/programming/leetcode/#problem-statement_3","title":"Problem Statement","text":"<p>Determine if a 9 x 9 Sudoku board is valid. Only the filled cells need to be validated according to the following rules:</p> <ol> <li>Each row must contain the digits 1-9 without repetition.</li> <li>Each column must contain the digits 1-9 without repetition.</li> <li>Each of the nine 3 x 3 sub-boxes of the grid must contain the digits 1-9 without repetition.</li> </ol> <p>Note:</p> <p>A Sudoku board (partially filled) could be valid but is not necessarily solvable. Only the filled cells need to be validated according to the mentioned rules.</p>"},{"location":"notes/programming/leetcode/#solution_3","title":"Solution","text":"<p>There are a few ways this could be solved. A naive solution would be to first iterate over every <code>(x,y)</code> coordinate in the puzzle, and for each coordinate, traverse the entire column at <code>x</code> and the entire row at <code>y</code> to see if an integer repeats. This algoirthm would require \\(O(n^2)\\) to iterate over every element, then \\(O(2n)=O(n)\\) to iterate over the respective column/row, for a total time complexity of \\(O(n^3)\\). The space complexity would be \\(O(n)\\) as we would want to allocate a set that contains the values seen in the entire column/row for <code>(x,y)</code>.</p> <p>Another solution would be as follows:</p> <ol> <li>Allocate two sets: one for every column, and one for every row. This gives a space complexity of \\(O(n^2)\\).</li> <li>Iterate over every <code>(x,y)</code> coordinate. This gives time complexity of \\(O(n^2)\\)    a. If a value is present in the coordinate, add the value to the <code>cols[x]</code> set, and to the <code>cols[y]</code> set. Time complexity is \\(O(1)\\) as hashing is constant time.    b. If the value previously exists in either set, return <code>false</code>.</li> <li>If we made it through the entire puzzle and a value was not repeated for each <code>(col,row)</code> tuple, then return <code>true</code>.</li> </ol> <p>https://leetcode.com/problems/valid-sudoku/submissions/1124533660?envType=study-plan-v2&amp;envId=top-interview-150</p> Go<pre><code>func isValidSudoku(board [][]byte) bool {\n    type set map[byte]struct{}\n    type index int\n    cols := make([]set, 9)\n    rows := make([]set, 9)\n\n    // The subBox that a particular coordinate belongs to is calculated using\n    // the formula: subBox = floor(x/3) + (floor(y/3) * 3). The constraint says\n    // that `board.length == 9` so we know there will always be 9 sub-boxes.\n    subBox := make([]set, 9)\n\n    boardLen := index(len(board))\n\n    for y := index(0); y &lt; boardLen; y++ {\n        if rows[y] == nil {\n            rows[y] = set{}\n        }\n        for x := index(0); x &lt; boardLen;  x++ {\n            // Initialize the cols[x] set if it is nil\n            if cols[x] == nil {\n                cols[x] = set{}\n            }\n            // Calculate the 3x3 sub-box that we're in. If it doesn't exist,\n            // allocate it.\n            subBoxIdx := (x/index(3)) + ((y/index(3)) * 3)\n            subBoxElement := subBox[subBoxIdx]\n            if subBoxElement == nil {\n                subBoxElement = set{}\n                subBox[subBoxIdx] = subBoxElement\n            }\n\n            val := board[x][y]\n            if val == byte('.') {\n                continue\n            }\n            // Has this number been seen in this row before?\n            if _, existsInRow := rows[y][val]; existsInRow {\n                return false\n            }\n            rows[y][val] = struct{}{}\n            // Has this number been seen in this column before?\n            if _, existsCol := cols[x][val]; existsCol {\n                return false\n            }\n            cols[x][val] = struct{}{}\n            // Has this number been seen in our subBox before?\n            if _, existsInSubBox := subBox[subBoxIdx][val]; existsInSubBox {\n                return false\n            }\n            subBox[subBoxIdx][val] = struct{}{}\n        }\n    }\n    return true\n}\n</code></pre> status language runtime memory Accepted Go 5ms (28.94%) 3.4MB (35.58%) <p>In this solution, our row/col/subBox hashmaps are stored inside an array of fixed length <code>9</code>. We could have used a structure like <code>map[index]set</code> but because we know beforehand the values that would go into the map, it's more efficient to instead use an array of length <code>9</code>: <code>[9]set</code>.</p> <p>As you can see, this solution lies within the lower half percentile of all submissions in terms of runtime and memory performance. Perhaps there is a better way? I noticed one silly thing that was being done in my original solution: the initialization of the rows/copls/subBox sets were being done inside the main <code>for</code> loop. This means that there will be a lot of unnecessary branching done in the <code>if</code> statements that check if the set at a particular index had been initialized yet. Let's see how it performs with this change:</p> <p>https://leetcode.com/problems/valid-sudoku/submissions/1124540277?envType=study-plan-v2&amp;envId=top-interview-150</p> Go<pre><code>type set map[byte]struct{}\ntype index int\n\nfunc isValidSudoku(board [][]byte) bool {\n\n    cols := make([]set, 9)\n    rows := make([]set, 9)\n\n    // The subBox that a particular coordinate belongs to is calculated using\n    // the formula: subBox = floor(x/3) + (floor(y/3) * 3). The constraint says\n    // that `board.length == 9` so we know there will always be 9 sub-boxes.\n    subBox := make([]set, 9)\n\n    boardLen := index(len(board))\n\n    for i := index(0); i &lt; boardLen; i++ {\n        rows[i] = set{}\n        cols[i] = set{}\n        subBox[i] = set{}\n    }\n\n    for y := index(0); y &lt; boardLen; y++ {\n        for x := index(0); x &lt; boardLen;  x++ {\n            // Calculate the 3x3 sub-box that we're in. If it doesn't exist,\n            // allocate it.\n            subBoxIdx := (x/index(3)) + ((y/index(3)) * 3)\n\n            val := board[x][y]\n            if val == byte('.') {\n                continue\n            }\n            // Has this number been seen in this row before?\n            if _, existsInRow := rows[y][val]; existsInRow {\n                return false\n            }\n            rows[y][val] = struct{}{}\n            // Has this number been seen in this column before?\n            if _, existsCol := cols[x][val]; existsCol {\n                return false\n            }\n            cols[x][val] = struct{}{}\n            // Has this number been seen in our subBox before?\n            if _, existsInSubBox := subBox[subBoxIdx][val]; existsInSubBox {\n                return false\n            }\n            subBox[subBoxIdx][val] = struct{}{}\n        }\n    }\n    return true\n}\n</code></pre> status language runtime memory Accepted Go 0ms (100.00%) 3.56MB (31.71%) <p>As we can see, the runtime is now much better. However, being somewhat of a perfectionist, our memory consumption is still not where I'd like it to be. If you look at the <code>Memory</code> distribution for submissions, a large number of Leetcoders were able to get the usage down to ~2.6MB. Let's see what we can do to resolve this.</p> <p>The two prior solutions are relying on a <code>type set map[byte]struct{}</code> to represent a set of bytes that we've seen. However, one property in this particular problem is that we already know the maximum size that this set could ever be, which is 9 (due to the fact that Sudoku cells can only be from 1-9). When we know the size of the hashmap beforehand, we can instead use a fixed-length array. We'll set the value of the array to be a <code>bool</code>, which in Go is a single byte.</p> <p>https://leetcode.com/problems/valid-sudoku/submissions/1124547528?envType=study-plan-v2&amp;envId=top-interview-150</p> Go<pre><code>type index int\n\nfunc isValidSudoku(board [][]byte) bool {\n    cols := make([][9]bool, 9)\n    rows := make([][9]bool, 9)\n\n    // The subBox that a particular coordinate belongs to is calculated using\n    // the formula: subBox = floor(x/3) + (floor(y/3) * 3). The constraint says\n    // that `board.length == 9` so we know there will always be 9 sub-boxes.\n    subBox := make([][9]bool, 9)\n\n    boardLen := index(len(board))\n\n    for i := index(0); i &lt; boardLen; i++ {\n        rows[i] = [9]bool{}\n        cols[i] = [9]bool{}\n        subBox[i] = [9]bool{}\n    }\n\n    for y := index(0); y &lt; boardLen; y++ {\n        for x := index(0); x &lt; boardLen;  x++ {\n            // Calculate the 3x3 sub-box that we're in. If it doesn't exist,\n            // allocate it.\n            subBoxIdx := (x/index(3)) + ((y/index(3)) * 3)\n\n            val := board[x][y]\n            if val == byte('.') {\n                continue\n            }\n            valInt, _ := strconv.Atoi(string(val))\n            valInt--\n            // Has this number been seen in this row before?\n            if rows[y][valInt] {\n                return false\n            }\n            rows[y][valInt] = true\n            // Has this number been seen in this column before?\n            if cols[x][valInt] {\n                return false\n            }\n            cols[x][valInt] = true\n            // Has this number been seen in our subBox before?\n            if subBox[subBoxIdx][valInt] {\n                return false\n            }\n            subBox[subBoxIdx][valInt] = true\n        }\n    }\n    return true\n}\n</code></pre> status language runtime memory Accepted Go 0ms (100.00%) 2.66MB (73.09%) <p>There's a way to make this even more memory efficient. For each row/col/box, we're storing a single <code>bool</code> for each integer <code>1-9</code>. This totals to 9 bytes for each row/column/box. Ideally, we'd like to distill down our bool to a single bit, but there is no data type that is a single bit large. Instead, we could rely on bitmasking over a sufficiently sized data type (perhaps a uint16) to store our bool. Our data structures would look something like:</p> Go<pre><code>cols := [9]uint16{}\nrows := [9]uint16{}\nsubBox := [9]uint16{}\n</code></pre> <p>Notice that in this solution, I'll elect to use an array instead of a slice, as arrays take up even less memory in Go.</p> <p>The bit mask is going to need to be equal to its location in the <code>uint16</code>, or more specifically, <code>1 &lt;&lt; valInt</code></p> <p>https://leetcode.com/problems/valid-sudoku/submissions/1124554856?envType=study-plan-v2&amp;envId=top-interview-150</p> Go<pre><code>type index int\n\nfunc isValidSudoku(board [][]byte) bool {\n    cols := [9]uint16{}\n    rows := [9]uint16{}\n    subBox := [9]uint16{}\n    valMap := map[byte]int{}\n\n    boardLen := index(len(board))\n\n    for y := index(0); y &lt; boardLen; y++ {\n        for x := index(0); x &lt; boardLen;  x++ {\n            // Calculate the 3x3 sub-box that we're in. If it doesn't exist,\n            // allocate it.\n            subBoxIdx := (x/index(3)) + ((y/index(3)) * 3)\n\n            val := board[x][y]\n            if val == byte('.') {\n                continue\n            }\n            var valInt int\n            var exists bool\n            if valInt, exists = valMap[val]; !exists {\n                v, _ := strconv.Atoi(string(val))\n                v--\n                valMap[val] = v\n            }\n            valInt = valMap[val]\n\n            mask := (uint16(1)&lt;&lt;valInt)\n            // Has this number been seen in this row before?\n            if rows[y] &amp; mask != 0 {\n                return false\n            }\n            rows[y] |= mask\n            // Has this number been seen in this column before?\n            if cols[x] &amp; mask != 0 {\n                return false\n            }\n            cols[x] |= mask\n            // Has this number been seen in our subBox before?\n            if subBox[subBoxIdx] &amp; mask != 0 {\n                return false\n            }\n            subBox[subBoxIdx] |= mask\n        }\n    }\n    return true\n}\n</code></pre> status language runtime memory Accepted Go 0ms (100.00%) 2.64MB (73.09%) <p>While this solution is slightly more efficient with memory, it was barely noticable. For a <code>9x9</code> grid, we were able to shave off \\((9+9+9)*(1*9) - (9+9+9)*1 = 216\\) bytes (in the prior solution, each row/col/box required a single byte for each value 1-9, or 9 bytes total, but in this solution, each row/col/box needs only 2 bytes).</p>"},{"location":"notes/programming/leetcode/#hashmap","title":"Hashmap","text":""},{"location":"notes/programming/leetcode/#group-anagram-medium","title":"Group Anagram (Medium)","text":""},{"location":"notes/programming/leetcode/#problem-statement_4","title":"Problem Statement","text":"<p>Given an array of strings strs, group the anagrams together. You can return the answer in any order.</p> <p>An Anagram is a word or phrase formed by rearranging the letters of a different word or phrase, typically using all the original letters exactly once.</p> <p>Example 1:</p> Text Only<pre><code>Input: strs = [\"eat\",\"tea\",\"tan\",\"ate\",\"nat\",\"bat\"]\nOutput: [[\"bat\"],[\"nat\",\"tan\"],[\"ate\",\"eat\",\"tea\"]]\n</code></pre> <p>Example 2:</p> Text Only<pre><code>Input: strs = [\"\"]\nOutput: [[\"\"]]\n</code></pre> <p>Example 3:</p> Text Only<pre><code>Input: strs = [\"a\"]\nOutput: [[\"a\"]]\n</code></pre> <p>Constraints:</p> <ul> <li>1 &lt;= strs.length &lt;= 104</li> <li>0 &lt;= strs[i].length &lt;= 100</li> <li>strs[i] consists of lowercase English letters.</li> </ul>"},{"location":"notes/programming/leetcode/#solution_4","title":"Solution","text":"<p>In this problem, we are wanting to map words that contain the same letters into the same group. The intuition for this is that somehow the letters within the word are going to be used as a hash key into some sort of group. The first problem we run into is the fact that the hash is going to vary based on the order of the words, so we want our hashing function to be invariate to the letter ordering. An easy way to fix this is to simply sort the word alphabetically, then use that sequence of letters as the hash key.</p> <p>You can see in the solution below that we do just that. We first allocate a <code>groups</code> dictionary that maps a string to a list. We iterate over every word in the <code>strs</code> variable, sort the word, then append that word to the list that it mapped to.</p> <p>The problem requires us to return a list of a list of strings, so the last step is to modify the <code>groups</code> dictionary into the format the problem requires.</p> <p>https://leetcode.com/problems/group-anagrams/submissions/1129043806?envType=study-plan-v2&amp;envId=top-interview-150</p> Python<pre><code>from typing import Dict\n\nclass Solution:\n    def groupAnagrams(self, strs: List[str]) -&gt; List[List[str]]:\n        groups: Dict[str, list] = {}\n        for word in strs:\n            sorted_word = ''.join(sorted(word))\n            if sorted_word not in groups:\n                groups[sorted_word] = []\n            groups[sorted_word].append(word)\n        groups_list = []\n        for key, value in groups.items():\n            groups_list.append(value)\n        return groups_list\n</code></pre> status language runtime memory Accepted Python 90ms (88.31%) 20.43MB (45.15%) <p>We can simplify the logic of allocating the list in <code>groups</code> by using <code>collections.defaultdict</code>. Let's try that:</p> Python<pre><code>from typing import Dict\nfrom collections import defaultdict\n\nclass Solution:\n    def groupAnagrams(self, strs: List[str]) -&gt; List[List[str]]:\n        groups: Dict[str, list] = defaultdict(list)\n        for word in strs:\n            sorted_word = ''.join(sorted(word))\n            groups[sorted_word].append(word)\n        groups_list = []\n        for key, value in groups.items():\n            groups_list.append(value)\n        return groups_list\n</code></pre> status language runtime memory Accepted Python 77ms (99.63%) 20.52MB (42.00%) <p>The runtime here is a bit of a red herring because I can run the same code multiple times and get wildly different results. For example, when I run a second time I get 88ms. So, it's hard to judge just how good a solution really is without running it multiple times.</p>"},{"location":"notes/programming/leetcode/#intervals","title":"Intervals","text":""},{"location":"notes/programming/leetcode/#merge-intervals-medium","title":"Merge Intervals (Medium)","text":""},{"location":"notes/programming/leetcode/#problem-statement_5","title":"Problem Statement","text":"<p>Given an array of intervals where intervals[i] = [starti, endi], merge all overlapping intervals, and return an array of the non-overlapping intervals that cover all the intervals in the input.</p> <p>Example 1:</p> Text Only<pre><code>Input: intervals = [[1,3],[2,6],[8,10],[15,18]]\nOutput: [[1,6],[8,10],[15,18]]\nExplanation: Since intervals [1,3] and [2,6] overlap, merge them into [1,6].\n</code></pre> <p>Example 2:</p> Text Only<pre><code>Input: intervals = [[1,4],[4,5]]\nOutput: [[1,5]]\nExplanation: Intervals [1,4] and [4,5] are considered overlapping.\n</code></pre> <p>Constraints:</p> <ul> <li>1 &lt;= intervals.length &lt;= 104</li> <li>intervals[i].length == 2</li> <li>0 &lt;= starti &lt;= endi &lt;= 104</li> </ul>"},{"location":"notes/programming/leetcode/#solution_5","title":"Solution","text":"<p>This solution has a wide range of possibilities. A naive solution might be to create a hashmap whereby each integer value is mapped to the current \"largest\" interval. You iterate through every integer in the range to see if you are overlapping with another range. If you are, extend the range if appropriate and continue to the next interval. However, this solution is a bit of a non-starter because the time complexity would be \\(O(n*m)\\) where \\(n\\) is the number of intervals, and \\(m\\) is the size of the largest interval. The space complexity is also \\(O(n*m)\\) because the hashmap needs to store a mapping for every integer in every range.</p> <p>Another solution we could try is to first sort the intervals by starting integer. Once sorted, we iterate through the intervals and check each successive interval to see if it overlaps with a prior interval. This would be \\(O(n)\\) because you have to iterate through the intervals once (to sort), then again to compare to the previous interval.</p> <p>Let's try the second solution:</p> Python<pre><code>class Solution:\n    def merge(self, intervals: List[List[int]]) -&gt; List[List[int]]:\n        sorted_intervals = sorted(intervals, key=lambda entry: entry[0])\n\n        # We initialize `ranges` with the first element in sorted_intevals.\n        # we can assume that `sorted_intervals` contains at least one element\n        # because of the constraints listed in the problem statement.\n        ranges: List[List[int]] = [sorted_intervals[0]]\n\n        current_interval = 0\n        # We start from index 1 because we already added the first element\n        # to ranges.\n        for interval in sorted_intervals[1:]:\n            prev_range_start = ranges[current_interval][0]\n            prev_range_end = ranges[current_interval][1]\n            cur_range_start = interval[0]\n            cur_range_end = interval[1]\n            if cur_range_start &lt;= prev_range_end:\n                # We know we have an overlapping range.\n                if cur_range_end - prev_range_start &gt;= prev_range_end - prev_range_start:\n                    # If we were to extend the interval, we get a larger range.\n                    ranges[current_interval][1] = cur_range_end\n                continue\n            ranges.append(interval)\n            current_interval += 1\n\n        return ranges\n</code></pre> <p>This performs quite well:</p> status language runtime memory Accepted Python 116ms (99.21%) 21.50MB (10.16%)"},{"location":"notes/programming/leetcode/#stack","title":"Stack","text":""},{"location":"notes/programming/leetcode/#simplify-path-medium","title":"Simplify Path (Medium)","text":""},{"location":"notes/programming/leetcode/#problem-statement_6","title":"Problem Statement","text":"<p>Given a string path, which is an absolute path (starting with a slash '/') to a file or directory in a Unix-style file system, convert it to the simplified canonical path.</p> <p>In a Unix-style file system, a period '.' refers to the current directory, a double period '..' refers to the directory up a level, and any multiple consecutive slashes (i.e. '//') are treated as a single slash '/'. For this problem, any other format of periods such as '...' are treated as file/directory names.</p> <p>The canonical path should have the following format:</p> <ul> <li>The path starts with a single slash '/'.</li> <li>Any two directories are separated by a single slash '/'.</li> <li>The path does not end with a trailing '/'.</li> <li>The path only contains the directories on the path from the root directory to the target file or directory (i.e., no period '.' or double period '..')</li> </ul> <p>Return the simplified canonical path.</p> <p>Example 1:</p> Text Only<pre><code>Input: path = \"/home/\"\nOutput: \"/home\"\nExplanation: Note that there is no trailing slash after the last directory name.\n</code></pre> <p>Example 2:</p> Text Only<pre><code>Input: path = \"/../\"\nOutput: \"/\"\nExplanation: Going one level up from the root directory is a no-op, as the root level is the highest level you can go.\n</code></pre> <p>Example 3:</p> Text Only<pre><code>Input: path = \"/home//foo/\"\nOutput: \"/home/foo\"\nExplanation: In the canonical path, multiple consecutive slashes are replaced by a single one.\n</code></pre> <p>Constraints:</p> <ul> <li>1 &lt;= path.length &lt;= 3000</li> <li>path consists of English letters, digits, period '.', slash '/' or '_'.</li> <li>path is a valid absolute Unix path.</li> </ul>"},{"location":"notes/programming/leetcode/#solution_6","title":"Solution","text":"Python<pre><code>class Solution:\n    def simplifyPath(self, path: str) -&gt; str:\n        elems = path.split(\"/\")\n        out_path = list()\n        cur_elem = 0\n        for elem in elems:\n            if elem == \"..\":\n                if len(out_path) &gt; 0:\n                    out_path.pop()\n                continue\n            if elem in (\"\", \".\"):\n                continue\n            out_path.append(elem)\n\n        return \"/\" + \"/\".join(out_path)\n</code></pre> status language runtime memory Accepted Python 34ms (89.48%) 17.55MB (5.52%)"},{"location":"notes/programming/leetcode/#strings","title":"Strings","text":""},{"location":"notes/programming/leetcode/#strong-password-checker-hard","title":"Strong Password Checker (hard)","text":""},{"location":"notes/programming/leetcode/#problem-statement_7","title":"Problem Statement","text":"<p>A password is considered strong if the below conditions are all met:</p> <ul> <li>It has at least 6 characters and at most 20 characters.</li> <li>It contains at least one lowercase letter, at least one uppercase letter, and at least one digit.</li> <li>It does not contain three repeating characters in a row (i.e., \"Baaabb0\" is weak, but \"Baaba0\" is strong).</li> </ul> <p>Given a string password, return the minimum number of steps required to make password strong. if password is already strong, return 0.</p> <p>In one step, you can:</p> <ul> <li>Insert one character to password,</li> <li>Delete one character from password, or</li> <li>Replace one character of password with another character.</li> </ul> <p>Example 1:</p> Text Only<pre><code>Input: password = \"a\"\nOutput: 5\n</code></pre> <p>Example 2:</p> Text Only<pre><code>Input: password = \"aA1\"\nOutput: 3\n</code></pre> <p>Example 3:</p> Text Only<pre><code>Input: password = \"1337C0d3\"\nOutput: 0\n</code></pre> <p>Constraints:</p> <ul> <li>1 &lt;= password.length &lt;= 50</li> <li>password consists of letters, digits, dot '.' or exclamation mark '!'.</li> </ul>"},{"location":"notes/programming/leetcode/#solution_7","title":"Solution","text":"<p>This problem is pretty fucked, and I admittedly struggled to get it. The check to determine if the password is strong is easy, but figuring out the minimum number of steps to make it strong is very difficult. My solution passed 44/53 test cases, which is still a B- mind you </p> Python<pre><code>from typing import List\n\n\nclass Solution:\n    def strongPasswordChecker(self, password: str) -&gt; int:\n        too_short = len(password) &lt; 6\n        too_long = len(password) &gt; 20\n        has_lowercase_letter = False\n        has_uppercase_letter = False\n        has_digit = False\n        has_excessive_repeating_character = False\n\n        num_excessive_repeating_sequences = 0\n        num_excessive_contiguous_characters = 0\n\n        # Contains the character we've seen contiguously, and the number of times\n        # it has been contiguous\n        contiguous_character: List[string, int] | None = None\n\n        for char in password:\n            if char.isalpha():\n                if char.islower():\n                    has_lowercase_letter = True\n                else:\n                    has_uppercase_letter = True\n            if char.isnumeric():\n                has_digit = True\n            if not contiguous_character:\n                contiguous_character = [char, 1]\n            else:\n                if char == contiguous_character[0]:\n                    contiguous_character[1] += 1\n                    if contiguous_character[1] &gt;= 3:\n                        has_excessive_repeating_character = True\n                        num_excessive_repeating_sequences += 1\n                        contiguous_character = None\n                else:\n                    contiguous_character = [char, 1]\n\n        is_strong = (\n            not too_short\n            and not too_long\n            and has_lowercase_letter\n            and has_uppercase_letter\n            and has_digit\n            and not has_excessive_repeating_character\n        )\n        if is_strong:\n            return 0\n\n        steps = 0\n        if too_long:\n            num_deleted = 0\n            while (\n                num_excessive_repeating_sequences &gt; 0\n                and num_deleted &lt; len(password) - 20\n            ):\n\n                steps += 1\n                num_excessive_repeating_sequences -= 1\n                num_deleted += 1\n                print(\"deleted character from repeating sequence\")\n            steps += len(password) - 20 - num_deleted\n            print(\"deleted character\")\n\n        if too_short:\n            num_added = 0\n            while num_added &lt; 6 - len(password):\n                if num_excessive_repeating_sequences &gt; 0:\n\n                    if not has_lowercase_letter:\n                        has_lowercase_letter = True\n                        print(\"added character in repeating sequence: lowercase\")\n                    elif not has_uppercase_letter:\n                        has_uppercase_letter = True\n                        print(\"added character in repeating sequence: uppercase\")\n                    elif not has_digit:\n                        has_digit = True\n                        print(\"added character in repeating sequence: digit\")\n                    else:\n                        print(\"added character in repeating sequence: arbitrary\")\n                    num_excessive_repeating_sequences -= 1\n                elif not has_lowercase_letter:\n                    has_lowercase_letter = True\n                    print(\"added lowercase\")\n                elif not has_uppercase_letter:\n                    has_uppercase_letter = True\n                    print(\"added uppercase\")\n                elif not has_digit:\n                    has_digit = True\n                    print(\"added digit\")\n                else:\n                    print(\"added arbitrary\")\n                num_added += 1\n                steps += 1\n\n        # We need to replace one character for each repeating\n        # sequence.\n        # By doing a replacement, we could also add missing upper/lower/digits\n        # if possible.\n        while num_excessive_repeating_sequences &gt; 0:\n            if not has_lowercase_letter:\n                has_lowercase_letter = True\n                print(\"replaced repeating sequence with lowercase\")\n            elif not has_uppercase_letter:\n                has_uppercase_letter = True\n                print(\"replaced repeating sequence with uppercase\")\n            elif not has_digit:\n                has_digit = True\n                print(\"replaced repeating sequence with digit\")\n            else:\n                print(\"replaced repeating sequence with arbitrary\")\n            num_excessive_repeating_sequences -= 1\n            steps += 1\n\n        if not has_lowercase_letter:\n            has_lowercase_letter = True\n            steps += 1\n            print(\"replaced character with lowercase\")\n        if not has_uppercase_letter:\n            has_uppercase_letter = True\n            steps += 1\n            print(\"replaced character with upper\")\n        if not has_digit:\n            has_digit = True\n            steps += 1\n            print(\"replaced character with digit\")\n\n        return steps\n</code></pre>"},{"location":"notes/programming/leetcode/#binary-tree-general","title":"Binary Tree General","text":""},{"location":"notes/programming/leetcode/#construct-binary-tree-from-preorder-and-inorder-traversal-medium","title":"Construct Binary Tree from Preorder and Inorder Traversal (Medium)","text":""},{"location":"notes/programming/leetcode/#problem-statement_8","title":"Problem Statement","text":"<p>Given two integer arrays preorder and inorder where preorder is the preorder traversal of a binary tree and inorder is the inorder traversal of the same tree, construct and return the binary tree.</p> <p>Example 1</p> <p></p> <p>Input: preorder = [3,9,20,15,7], inorder = [9,3,15,20,7]</p> <p>Output: [3,9,20,null,null,15,7]</p> <p>Example 2</p> <p>Input: preorder = [-1], inorder = [-1]</p> <p>Output: [-1]</p>"},{"location":"notes/programming/leetcode/#solution_8","title":"Solution","text":"<p>This question is asking us to create a data structure that represents the true shape of the tree, given the preorder and inorder traversals. For example, the Python3 code shows us this:</p> Python<pre><code># Definition for a binary tree node.\n# class TreeNode:\n#     def __init__(self, val=0, left=None, right=None):\n#         self.val = val\n#         self.left = left\n#         self.right = right\nclass Solution:\n    def buildTree(self, preorder: List[int], inorder: List[int]) -&gt; Optional[TreeNode]:\n</code></pre> <p>What we need to determine is what nodes are children of what other nodes. To do this, we should look for some kind of pattern in how preorder/inorder traversals work. Let's remind ourselves of what preorder/inorder is:</p> <p>Preorder: This first visits the current node, then it recurses into the left child, then into the right child.</p> <p>Inorder: First it recurses into the left child, then it visits the current node, then recurses into the right child.</p> <p>What we can guarantee is that a preorder traversal will always start with the root node. An in-order traversal will always give us the left-most node first. So we can do the simple part of first defining the root node:</p> Python<pre><code>class Solution:\n    def buildTree(self, preorder: List[int], inorder: List[int]) -&gt; Optional[TreeNode]:\n        root = TreeNode(val=preorder[0])\n</code></pre> <p>The next node in the preorder traversal is going to be the immediate left child (if it exists), then the right node (if it exists). How do we know if, in example 1, that 9 is a direct child of 3?</p> <ol> <li>9 would have to be situated left of 3 in the in-order traversal, and</li> <li>9 would have to be directly to the right of 3 in the pre-order traversal.</li> </ol> <p>To test these two conditions, consider if we deleted node <code>9</code> and replaced <code>15</code> with the value <code>9</code>. Then the traversals would look like this:</p> Text Only<pre><code>preorder: [3, 20, 15, 7]\ninorder: [3, 15, 20, 7]\n</code></pre> <p>How do we know that <code>20</code> is a direct child of <code>3</code>? It would have to satisfy the conditions:</p> <ol> <li><code>20</code> would need to occur right of <code>3</code> in the in-order traversal</li> <li><code>20</code> needs to occur at a location that is exactly \\(i+n\\) away from <code>3</code> in the preorder traversal, where \\(i\\) is the index of <code>3</code> in the preorder list, and \\(n\\) is the number of elements in the left subtree. We can find \\(n\\) by subtracting the index of <code>3</code> in the in-order list from the index of <code>9</code> in the in-order list. We can hand-check this math in example 1: \\(i=0\\) and \\(n=3-1=2\\). We find that at index \\(2\\) in the pre-order list, we indeed find the value <code>20</code>.</li> </ol> <p>Let's first create a naive implementation of this, just to check that our logic is sound. I fully expect that the runtime will be horrid, but that's okay for a proof of concept. Let's start with the simple case of populating the left subtree:</p> Python<pre><code>class Solution:\n    def buildTree(\n        self, preorder: List[int], inorder: List[int]\n    ) -&gt; Optional[TreeNode]:\n        if len(preorder) == 0:\n            return None\n\n        root = TreeNode(val=preorder[0])\n        if len(preorder) == 1:\n            return root\n\n        # Need to check if the next element in preorder is in the left subtree or the right\n        rootInorderIdx = inorder.index(preorder[0])\n        nextInorderIdx = inorder.index(preorder[1])\n\n        if nextInorderIdx &lt; rootInorderIdx:\n            # We need to find the ending preorder index to pass to the bottom function. We can\n            # find this by including all the elements from preorder that appear to the left of 3\n            # in the inorder list.\n            maxPreorderIdx = 1\n            for elem in inorder[:rootInorderIdx]:\n                preorderIdx = preorder.index(elem)\n                if preorderIdx &gt; maxPreorderIdx:\n                    maxPreorderidx = preorderIdx\n\n            root.left = self.buildTree(\n                preorder=preorder[1:maxPreorderIdx],\n                inorder=inorder[:rootInorderIdx],\n            )\n\n        return root\n</code></pre> <p>What we're doing here is first creating the root node, which we know will always be the first element of <code>preorder</code>. Then, we know that the next element of <code>preorder</code> is going to be a direct left descendent if and only if that next element appears to the left in the inorder array. So, we do some simple checks to ensure this is true.</p> <p>We run the code to see what happens and ensure that we receive the nodes <code>[3, 9]</code> in the output. Note, that this is obviously not the correct answer, but it will prove to us that the left subtree logic is working. The code above is confirmed to return <code>[3, 9]</code>, so we can be reasonably confident the left subtree logic is working.</p> <p>Now, let's move onto the right subtree logic.</p> <p>Python<pre><code>class Solution:\n    def buildTree(\n        self, preorder: List[int], inorder: List[int], depth=0\n    ) -&gt; Optional[TreeNode]:\n        if len(preorder) == 0:\n            return None\n\n        root = TreeNode(val=preorder[0])\n        if len(preorder) == 1:\n            return root\n\n        # Need to check if the next element in preorder is in the left subtree or the right\n        rootInorderIdx = inorder.index(preorder[0])\n        nextInorderIdx = inorder.index(preorder[1])\n\n        if nextInorderIdx &lt; rootInorderIdx:\n            # We need to find the ending preorder index to pass to the bottom function. We can\n            # find this by including all the elements from preorder that appear to the left of 3\n            # in the inorder list.\n            maxPreorderIdx = 1\n            for elem in inorder[:rootInorderIdx]:\n                preorderIdx = preorder.index(elem)\n                if preorderIdx &gt; maxPreorderIdx:\n                    maxPreorderIdx = preorderIdx\n\n            root.left = self.buildTree(\n                preorder=preorder[1 : maxPreorderIdx + 1],\n                inorder=inorder[:rootInorderIdx],\n                depth=depth + 1,\n            )\n\n\n        if len(inorder) &lt; 2:\n            return root\n\n        # Now we need to check if the right subtree exists. To figure out our \"candidate\" direct\n        # right child of root, we need to consider two truths:\n        # 1. In the inorder traversal, everything that comes right of the root node in the list\n        #    is in the right subtree. So, we can narrow down the nodes we're looking for that way.\n        # 2. The right child is going to be the _first_ element we encounter in preorder, starting\n        #    from the root node and iterating right, that appears in the set we found in step 1.\n\n        rightSubtree = inorder[inorder.index(root.val) + 1 :] # (1)\n        rightChild = None\n        rightChildIdx = None\n\n        for idx, elem in enumerate(preorder[1:]): # (2)\n            if elem in rightSubtree:\n                rightChild = elem\n                rightChildIdx = (\n                    idx + 1\n                )  # it's +1 because we've excluded the first element (which is the root)\n                break\n        if rightChild: # (3)\n            root.right = self.buildTree(\n                preorder=preorder[rightChildIdx:], # (4)\n                inorder=rightSubtree,\n                depth=depth + 1,\n            )\n\n        return root\n</code></pre></p> <ol> <li>In order to get the right subtree from the inorder traversal, we need to find the index of the current root. The root is always <code>inorder[0]</code> (or <code>root.val</code>, equivalently). So, we find the index that contains that value, and add one to it so we get the right subtree.</li> <li>The next step is to find what node is the right child of the root. To do this, we need have to skip over all of the nodes from the left subtree. The information on what nodes are in the left subtree is contained in the inorder list: everything to the left of <code>3</code> is the left subtree is the left, and everything to the right is the right subtree. We also know that the first element of the right subtree in the preorder traversal will be the root's direct descendent. So, loop over the values of preorder until we find the first right subtree element.</li> <li>It's possible we didn't find any right children.</li> <li>We now know the index of the new subtree's root, and the right subtree. Pass these values into the recursive function call.</li> </ol> <p>This solution passes the two test cases provided, so let's see if we get an accepted solution!</p> <p>Wrong Answer</p> <p>201 / 203 testcases passed</p> <p>This is not a bad result because we got the vast majority of the cases. Let's inspect one of the failing test cases.</p> Text Only<pre><code>Input\npreorder =\n[3,2,1,0,-1,-2]\ninorder =\n[3,2,1,0,-1,-2]\n\nOutput\n[3,null,2,null,1]\nExpected\n[3,null,2,null,1,null,0,null,-1,null,-2]\n</code></pre> <p>By constructing this tree by hand, we can tell that it's a tree with only right children. So why does the algorithm break after node <code>1</code>?</p> <p></p> <p>It turns out, I made a really stupid mistake. In line 54 above, I'm checking the truthiness of <code>rightChild</code> instead of checking that it's <code>not None</code>. This means that the integer <code>0</code> would evaluate to <code>False</code>, which isn't what I intended. After fixing this, this test case now passed. However, we run into another problem: the last test case times out.</p> <p>Our solution is doing some unoptimal things: we iterate over <code>preorder</code> and <code>inorder</code> multiple times linearly in various places. We can instead make use of sets to cut down on the \\(O(n^2)\\) operations we're doing (like for example in the cases where we're trying to determine the existence of a value in a particular subtree).</p> <p>Another unoptimal thing we're doing is making judicious use of <code>inorder.index</code> to find the index of an element. We can instead create a hashmap (aka dict) to quickly find the location of the elements. We fix both of those issues here:</p> Python<pre><code>class Solution:\n    def buildTree(\n        self, preorder: List[int], inorder: List[int], depth=0\n    ) -&gt; Optional[TreeNode]:\n        if len(preorder) == 0:\n            return None\n\n        inorderIndexMap = dict()\n        preOrderIndexMap = dict()\n        for idx in range(len(preorder)):\n            inorderIndexMap[inorder[idx]] = idx\n            preOrderIndexMap[preorder[idx]] = idx\n\n        root = TreeNode(val=preorder[0])\n        if len(preorder) == 1:\n            return root\n\n        # Need to check if the next element in preorder is in the left subtree or the right\n        rootInorderIdx = inorderIndexMap[preorder[0]]\n        nextInorderIdx = inorderIndexMap[preorder[1]]\n\n        if nextInorderIdx &lt; rootInorderIdx:\n            # We need to find the ending preorder index to pass to the bottom function. We can\n            # find this by including all the elements from preorder that appear to the left of 3\n            # in the inorder list.\n            maxPreorderIdx = 1\n            for elem in inorder[:rootInorderIdx]:\n                preorderIdx = preOrderIndexMap[elem]\n                if preorderIdx &gt; maxPreorderIdx:\n                    maxPreorderIdx = preorderIdx\n\n            root.left = self.buildTree(\n                preorder=preorder[1 : maxPreorderIdx + 1],\n                inorder=inorder[:rootInorderIdx],\n                depth=depth + 1,\n            )\n\n        if len(inorder) &lt; 2:\n            return root\n\n        rightSubtree = inorder[inorderIndexMap[root.val] + 1 :]\n        rightSubtreeSet = set(rightSubtree)\n        rightChild = None\n        rightChildIdx = None\n\n        for idx, elem in enumerate(preorder[1:]):\n            if elem in rightSubtreeSet:\n                rightChild = elem\n                rightChildIdx = (\n                    idx + 1\n                )  # it's +1 because we've excluded the first element (which is the root)\n                break\n        if rightChild is not None:\n            root.right = self.buildTree(\n                preorder=preorder[rightChildIdx:],\n                inorder=rightSubtree,\n                depth=depth + 1,\n            )\n\n        return root\n</code></pre> status language runtime memory Accepted Python 663ms (5.03%) 548.04 MB (5.25%) <p>Our solution has now been accepted, but it's quite inefficient. Let's think critically about some of its shortcomings:</p> <ol> <li>We regenerate the <code>inorderIndexMap</code> and <code>preorderIndexMap</code> during every step of recursion. Is there a way we can generate this once?</li> <li>We iterate over the entire left subtree in this block to find the beginning of the right subtree. Do we have the information to not do that?</li> </ol> <p>When fixing (1) above, we get a better memory result:</p> Note Python<pre><code>class Solution:\ndef _buildTree(\n    self,\n    preorder: List[int],\n    inorder: List[int],\n    preorderOffset: int = 0,\n    inorderOffset: int = 0,\n    depth=0,\n) -&gt; Optional[TreeNode]:\n    if len(preorder) == 0:\n        return None\n\n    root = TreeNode(val=preorder[0])\n    if len(preorder) == 1:\n        return root\n\n    # Need to check if the next element in preorder is in the left subtree or the right\n    rootInorderIdx = self.inorderIndexMap[preorder[0]] - inorderOffset\n    nextInorderIdx = self.inorderIndexMap[preorder[1]] - inorderOffset\n\n    if nextInorderIdx &lt; rootInorderIdx:\n        maxPreorderIdx = 1\n        for elem in inorder[:rootInorderIdx]:\n            preorderIdx = self.preOrderIndexMap[elem] - preorderOffset\n            if preorderIdx &gt; maxPreorderIdx:\n                maxPreorderIdx = preorderIdx\n\n        root.left = self._buildTree(\n            preorder=preorder[1 : maxPreorderIdx + 1],\n            inorder=inorder[:rootInorderIdx],\n            preorderOffset=preorderOffset + 1,\n            inorderOffset=inorderOffset,\n            depth=depth + 1,\n        )\n\n    if len(inorder) &lt; 2:\n        return root\n\n    rightSubtree = inorder[\n        self.inorderIndexMap[root.val] - inorderOffset + 1 :\n    ]\n    rightSubtreeSet = set(rightSubtree)\n    rightChild = None\n    rightChildIdx = None\n\n    for idx, elem in enumerate(preorder[1:]):\n        if elem in rightSubtreeSet:\n            rightChild = elem\n            rightChildIdx = (\n                idx + 1\n            )  # it's +1 because we've excluded the first element (which is the root)\n            break\n    if rightChild is not None:\n        root.right = self._buildTree(\n            preorder=preorder[rightChildIdx:],\n            inorder=rightSubtree,\n            preorderOffset=preorderOffset + rightChildIdx,\n            inorderOffset=inorderOffset\n            + (len(inorder) - len(rightSubtree)),\n            depth=depth + 1,\n        )\n\n    return root\n\ndef buildTree(\n    self, preorder: List[int], inorder: List[int]\n) -&gt; Optional[TreeNode]:\n    self.inorderIndexMap = dict()\n    self.preOrderIndexMap = dict()\n    for idx in range(len(preorder)):\n        self.inorderIndexMap[inorder[idx]] = idx\n        self.preOrderIndexMap[preorder[idx]] = idx\n\n    return self._buildTree(preorder, inorder)\n</code></pre> <p>https://leetcode.com/problems/construct-binary-tree-from-preorder-and-inorder-traversal/submissions/1143697388?envType=study-plan-v2&amp;envId=top-interview-150</p> status language runtime memory Accepted Python 436ms (5.03%) 90.29 MB (36.34%) <p>I'm going to leave the second optimization for another day, because this problem has already taken a huge number of hours of my day . I don't seem to be alone, as many other people in the discussion tab are crying just like me.</p>"},{"location":"notes/programming/leetcode/#1d-dynamic-programming","title":"1D Dynamic Programming","text":""},{"location":"notes/programming/leetcode/#climbing-stairs","title":"Climbing Stairs","text":""},{"location":"notes/programming/leetcode/#problem","title":"Problem","text":"<p>You are climbing a staircase. It takes n steps to reach the top.</p> <p>Each time you can either climb 1 or 2 steps. In how many distinct ways can you climb to the top?</p> <p>Example 1:</p> Text Only<pre><code>Input: n = 2\nOutput: 2\nExplanation: There are two ways to climb to the top.\n1. 1 step + 1 step\n2. 2 steps\n</code></pre> <p>Example 2:</p> Text Only<pre><code>Input: n = 3\nOutput: 3\nExplanation: There are three ways to climb to the top.\n1. 1 step + 1 step + 1 step\n2. 1 step + 2 steps\n3. 2 steps + 1 step\n</code></pre> <p>Constraints:</p> Text Only<pre><code>1 &lt;= n &lt;= 45\n</code></pre>"},{"location":"notes/programming/leetcode/#thought-dump","title":"Thought Dump","text":"<p>This problem can be broken down into a series of subproblems. Each staircase can be thought of as being a linked chain of steps. Those steps can either be combined together so that we climb two steps at once, or they can remain by themselves. The goal here is to find the number of ways we could combine the steps together (or not combine them) to reach the nth step.</p> <p>Let's take another example where <code>n=4</code>. The possible solutions are:</p> Text Only<pre><code>1. 1 + 1 + 1 + 1\n2. 1 + 1 + 2\n3. 2 + 2\n4. 1 + 2 + 1\n5. 2 + 1 + 1\n</code></pre> <p>What about when <code>n=5</code>?</p> Text Only<pre><code>1. 1 + 1 + 1 + 1 + 1\n2. 1 + 1 + 1 + 2\n3. 1 + 2 + 2\n4. 2 + 1 + 2\n5. 1 + 1 + 2 + 1\n6. 2 + 2 + 1\n7. 1 + 2 + 1 + 1\n8. 2 + 1 + 1 + 1\n</code></pre> <p>Another way to look at this problem is in the links between the steps. Every flight of stairs will have <code>n-1</code> links in them. We can model combining the links together through an array. Take for example <code>n=5</code> above:</p> Text Only<pre><code>| 0 | 0 | 0 | 0 |\n</code></pre> <p>This array represents whether a pair of steps have been linked together,  where <code>0</code> means \"not linked\" and <code>1</code> equals \"linked\". We can iterate through the possible ways these can be linked together. Note that in this model, we can't have two <code>1</code>s directly adjacent to each other because that would mean we would have skipped over more than two steps at once. Going back to <code>n=5</code>, let's see the ways in which this can be done:</p> Text Only<pre><code>| 0 | 0 | 0 | 0 | # 1 + 1 + 1 + 1 + 1\n\n| 0 | 0 | 0 | 1 | # 1 + 1 + 1 + (1+1)\n\n| 0 | 1 | 0 | 1 | #  1 + (1+1) + (1+1)\n\n| 1 | 0 | 0 | 1 | # (1+1) + 1 + (1+1)\n\n| 0 | 0 | 1 | 0 | # 1 + 1 + (1+1) + 1\n\n| 1 | 0 | 1 | 0 | # (1+1) + (1+1) + 1\n\n| 0 | 1 | 0 | 0 | # 1 + (1+1) + 1 + 1\n\n| 1 | 0 | 0 | 0 | # (1+1) + 1 + 1 + 1\n</code></pre> <p>To confirm the idea that the 1's can't be adjacent, let's try the <code>n=4</code> example:</p> Text Only<pre><code>| 0 | 0 | 0 |\n\n| 0 | 0 | 1 |\n\n| 1 | 0 | 1 |\n\n| 0 | 1 | 0 |\n\n| 1 | 0 | 0 |\n</code></pre> <p>This also checks out. The other pattern to note is that when we set a specific bit to 1, we've created a subproblem where we need to figure out how many combinations in the rest of the array we can have. Let's go back to the example of <code>n=5</code>, specifically this combination:</p> Text Only<pre><code>| 0 | 0 | 0 | 1 |\n</code></pre> <p>The subproblem is to figure out which of the remaining bits can be 1. We already know that the bit to the left of 1 can't be 1, so the subproblem is to figure out how many combinations can exist in <code>n=3</code>.</p> <p>You can see how we can take advantage of dynamic programming to solve this. Let's consider the steps the program would take. Let's call <code>n</code> the number of steps, and <code>l</code> the number of links. <code>l=n-1</code>.</p> <ol> <li>Solve for <code>n=1, l=0</code>: 1.</li> <li>Solve for <code>n=2, l=1</code>: 2.</li> <li>Solve for <code>n=3, l=2</code>: <code>climbStairs(n=3) == climbStairs(n=2) + climbStairs(n=1)</code> = 3.</li> <li>Solve for <code>n=4, l=3</code>: <code>climbStairs(n=4) == climbStairs(n=3) + climbStairs(n=2)</code> = 5.</li> <li>Solve for <code>n=5, l=4</code>: <code>climbStairs(n=5) == climbStairs(n=4) + climbStairs(n=3)</code> = 8.</li> </ol> <p>Our calculations seem to work, so let's go ahead and implement it.</p>"},{"location":"notes/programming/leetcode/#solution_9","title":"Solution","text":"Go<pre><code>func climbStairs(n int) int {\n    prev1 := 1\n    prev2 := 1\n    answer := 1\n    for i := 2; i &lt;= n; i++ {\n        answer = prev1 + prev2\n        prev2 = prev1\n        prev1 = answer\n    }\n    return answer\n}\n</code></pre> status language runtime memory Accepted Go 1ms (78.30%) 1.95 MB (49.19%) <p>The distribution of runtime/memory is so tight that it doesn't seem there is much room for improvemet. This is probably going to be the best result we can get.</p>"},{"location":"notes/programming/leetcode/#backtracking","title":"Backtracking","text":""},{"location":"notes/programming/leetcode/#letter-combinations-of-a-phone-number","title":"Letter Combinations of a Phone Number","text":"<p>Given a string containing digits from 2-9 inclusive, return all possible letter combinations that the number could represent. Return the answer in any order.</p> <p>A mapping of digits to letters (just like on the telephone buttons) is given below. Note that 1 does not map to any letters.</p> <p>Example 1:</p> Text Only<pre><code>Input: digits = \"23\"\nOutput: [\"ad\",\"ae\",\"af\",\"bd\",\"be\",\"bf\",\"cd\",\"ce\",\"cf\"]\n</code></pre> <p>Example 2:</p> Text Only<pre><code>Input: digits = \"\"\nOutput: []\n</code></pre> <p>Example 3:</p> Text Only<pre><code>Input: digits = \"2\"\nOutput: [\"a\",\"b\",\"c\"]\n</code></pre>"},{"location":"notes/programming/leetcode/#thought-dump_1","title":"Thought dump","text":"<p>This problem involves finding all possible combinations of letters given a sequence of digits on a phone number pad. This can be thought of as a recursive problem, as we want to iterate over every possible letter for each digit, and prepend that letter to all the combinations found to the right of the current digit. The steps might look something like this:</p> <ol> <li>Grab the first digit in the string, <code>s[0]</code></li> <li>Create a for loop that iterates over the possible letters for that digit</li> <li>For each iteration in the for loop, find the possible combinations of all remaining numbers in <code>s[1:]</code></li> <li>Ensure base cases are accounted for, i.e. when <code>len(digits) == 0</code> or when there are no remaining digits to the right.</li> </ol> <p>We can take into consideration the fact that the length of the digits string is between 0 and 4 (inclusive), and that the digits can only be between 2 and 9 (inclusive).</p>"},{"location":"notes/programming/leetcode/#solution_10","title":"Solution","text":"Go<pre><code>var mapping map[byte][]string = map[byte][]string{\n    byte('2'): {\"a\", \"b\", \"c\"},\n    byte('3'): {\"d\", \"e\", \"f\"},\n    byte('4'): {\"g\", \"h\", \"i\"},\n    byte('5'): {\"j\", \"k\", \"l\"},\n    byte('6'): {\"m\", \"n\", \"o\"},\n    byte('7'): {\"p\", \"q\", \"r\", \"s\"},\n    byte('8'): {\"t\", \"u\", \"v\"},\n    byte('9'): {\"w\", \"x\", \"y\", \"z\"},\n}\nfunc letterCombinations(digits string) []string {\n    if len(digits) == 0 {\n        return []string{}\n    }\n    digit := digits[0]\n    if len(digits) == 1 {\n        return mapping[digit]\n    }\n    combinations := []string{}\n    subCombos := letterCombinations(digits[1:])\n\n    for _, char := range mapping[digit] {\n        for _, subCombo := range subCombos {\n            combinations = append(combinations, char + subCombo)\n        }\n\n    }\n    return combinations\n}\n</code></pre> status language runtime memory Accepted Go 1ms (78.99%) 2.08 MB (18.78%)"},{"location":"notes/programming/leetcode/#binary-tree-bfs","title":"Binary Tree BFS","text":""},{"location":"notes/programming/leetcode/#binary-tree-right-side-view","title":"Binary Tree Right Side View","text":""},{"location":"notes/programming/leetcode/#problem-statement_9","title":"Problem Statement","text":"<p>Given the root of a binary tree, imagine yourself standing on the right side of it, return the values of the nodes you can see ordered from top to bottom.</p> <p>Example 1</p> <p></p> Text Only<pre><code>Input: root = [1,2,3,null,5,null,4]\nOutput: [1,3,4]\n</code></pre> <p>Example 2:</p> Text Only<pre><code>Input: root = [1,null,3]\nOutput: [1,3]\n</code></pre> <p>Example 3:</p> Text Only<pre><code>Input: root = []\nOutput: []\n</code></pre>"},{"location":"notes/programming/leetcode/#solution_11","title":"Solution","text":""},{"location":"notes/programming/leetcode/#incorrect-interpretation","title":"Incorrect interpretation","text":"<p>I coded up a solution, but it turns out that my solution was incorrect because the problem statement is ambiguous. I interpreted the problem statement to mean that we want to always be looking to the right of the tree and return the nodes that appear farther right than the previous maximum \"rightness\" value we've seen. This means that we would want to keep track of the rightmost node as we're traversing the tree and add the nodes to the list that are greater than the previous maximum right. However, it appears this is not what the question is asking, and it appears I am not alone in my confusion. The Editorial states:</p> <p>Quote</p> <p>The problem is to return a list of the last elements from all levels, so it's way more natural to implement BFS here.</p> <p>Nonetheless, here is my solution:</p> Python<pre><code># Definition for a binary tree node.\n# class TreeNode:\n#     def __init__(self, val=0, left=None, right=None):\n#         self.val = val\n#         self.left = left\n#         self.right = right\nclass Solution:\n    def rightSideView(self, root: Optional[TreeNode]) -&gt; List[int]:\n        # What does it mean to \"see\" the right nodes? Consider the tree:\n        #           1\n        #         /   \\\n        #        2     3\n        #             /\n        #            4\n        #             \\\n        #              5\n        #               \\\n        #                6\n        # Would we be able to see #6? I think the answer would be yes, as we are able to see\n        # only the nodes which appear to the _right_ of 3. In this case, we need to keep track of:\n        # 1. What's the farthest right node we've compile\n        # 2. What's the depth of the farthest right node we've seen?\n        # A node would be _visible_ if it is both farther right than the last previously seen node\n        # and it's at a lower depth. So as we traverse the tree, we want to keep track of the \"rightness\"\n        # and the depth of each node we've traversed.\n        #\n        # Proposed implementation:\n        # 1. Do a breadth first search\n        # 2. At every node, calculate the \"rightness\" and depth of the node. Initialize the \"rightness\" to 0\n        #    and the depth to 0.\n        # 3. If the node we're iterating over has a \"rightness\" greater than the previous \"rightness\", then\n        #    add that node to the list, and update the max \"rightness\" seen.\n\n        # Initialize our variables. The root node will always be visible.\n        if root is None:\n            return []\n        nodes = [root.val]\n        maxRight = 0\n        root.rightness = 0\n\n        nodesToIterate = [root]\n        while nodesToIterate:\n            curNode = nodesToIterate.pop(0)\n\n            if curNode.rightness &gt; maxRight:\n                nodes.append(curNode.val)\n\n            if curNode.left:\n                left = curNode.left\n                left.rightness = curNode.rightness - 1\n                nodesToIterate.append(left)\n            if curNode.right:\n                right = curNode.right\n                right.rightness = curNode.rightness + 1\n                nodesToIterate.append(right)\n        return nodes\n</code></pre> <p> 00:18:18</p>"},{"location":"notes/programming/leetcode/#correct-solution","title":"Correct solution","text":"<p>Given I had to peek at the editorial to figure out what the description meant, I attempted this a second time.</p> Python<pre><code># Definition for a binary tree node.\n# class TreeNode:\n#     def __init__(self, val=0, left=None, right=None):\n#         self.val = val\n#         self.left = left\n#         self.right = right\nclass Solution:\n    def rightSideView(self, root: Optional[TreeNode]) -&gt; List[int]:\n        if root is None:\n            return []\n        root.level = 0\n        curLevel = 0\n        nodes = [root.val]\n        queue = [root]\n\n        while queue:\n            curNode = queue.pop(0)\n\n            # When we enter into a new level, reset the maxRightness seen.\n            # We want to add the rightmost element at each level\n            if curNode.level &gt; curLevel:\n                curLevel = curNode.level\n                # It's possible this will be overwritten by a more right node.\n                nodes.append(curNode.val)\n\n            nodes[-1] = curNode.val\n\n            left = curNode.left\n            if left:\n                left.level = curNode.level + 1\n                queue.append(left)\n            right = curNode.right\n            if right:\n                right.level = curNode.level + 1\n                queue.append(right)\n        return nodes\n</code></pre> <p>In this solution, we use BFS to iterate over each level individually. We keep track of the current level we're iterating at, and because we're iterating over the levels from left to right, we simply add each element to the end of the <code>nodes</code> list. When we've entered into a new level, we append a new element onto our list. Within a level, this will continue to overwrite the previous value such that only the rightmost value will not be overwritten. This is an efficient solution because we're taking advantage of how the BFS is ordered, because we know that the rightmost node will be iterated last at each level.</p> status language runtime memory solution timer Accepted Python 22ms (99.81%) 16.56 MB (59.91%)  00:15:01"},{"location":"notes/programming/python/","title":"Python","text":""},{"location":"notes/programming/python/#floor-division","title":"Floor Division","text":"<p><code>5 // 2</code> will divide 5 by 2 and round the result down to the nearest integer.</p>"},{"location":"notes/programming/python/#lists","title":"Lists","text":"<p>Lists are implemented as an array of pointers.</p> <p>https://github.com/python/cpython/blob/5c22476c01622f11b7745ee693f8b296a9d6a761/Include/listobject.h#L22</p> C<pre><code>typedef struct {\n    PyObject_HEAD\n    Py_ssize_t ob_size;\n\n    /* Vector of pointers to list elements.  list[0] is ob_item[0], etc. */\n    PyObject **ob_item;\n\n    /* ob_item contains space for 'allocated' elements.  The number\n     * currently in use is ob_size.\n     * Invariants:\n     *     0 &lt;= ob_size &lt;= allocated\n     *     len(list) == ob_size\n     *     ob_item == NULL implies ob_size == allocated == 0\n     */\n    Py_ssize_t allocated;\n} PyListObject;\n</code></pre> <p>A list is grown not by doubling, but by a pattern that looks like <code>0, 4, 8, 16, 25, 35, 46, 58, 72, 88, ...</code>:</p> <p>https://github.com/python/cpython/blob/5c22476c01622f11b7745ee693f8b296a9d6a761/Objects/listobject.c#L22</p> C<pre><code>/* Ensure ob_item has room for at least newsize elements, and set\n * ob_size to newsize.  If newsize &gt; ob_size on entry, the content\n * of the new slots at exit is undefined heap trash; it's the caller's\n * responsibility to overwrite them with sane values.\n * The number of allocated elements may grow, shrink, or stay the same.\n * Failure is impossible if newsize &lt;= self.allocated on entry, although\n * that partly relies on an assumption that the system realloc() never\n * fails when passed a number of bytes &lt;= the number of bytes last\n * allocated (the C standard doesn't guarantee this, but it's hard to\n * imagine a realloc implementation where it wouldn't be true).\n * Note that self-&gt;ob_item may change, and even if newsize is less\n * than ob_size on entry.\n */\nstatic int\nlist_resize(PyListObject *self, Py_ssize_t newsize)\n{\n    PyObject **items;\n    size_t new_allocated, num_allocated_bytes;\n    Py_ssize_t allocated = self-&gt;allocated;\n\n    /* Bypass realloc() when a previous overallocation is large enough\n       to accommodate the newsize.  If the newsize falls lower than half\n       the allocated size, then proceed with the realloc() to shrink the list.\n    */\n    if (allocated &gt;= newsize &amp;&amp; newsize &gt;= (allocated &gt;&gt; 1)) {\n        assert(self-&gt;ob_item != NULL || newsize == 0);\n        Py_SIZE(self) = newsize;\n        return 0;\n    }\n\n    /* This over-allocates proportional to the list size, making room\n     * for additional growth.  The over-allocation is mild, but is\n     * enough to give linear-time amortized behavior over a long\n     * sequence of appends() in the presence of a poorly-performing\n     * system realloc().\n     * The growth pattern is:  0, 4, 8, 16, 25, 35, 46, 58, 72, 88, ...\n     * Note: new_allocated won't overflow because the largest possible value\n     *       is PY_SSIZE_T_MAX * (9 / 8) + 6 which always fits in a size_t.\n     */\n    new_allocated = (size_t)newsize + (newsize &gt;&gt; 3) + (newsize &lt; 9 ? 3 : 6);\n    if (new_allocated &gt; (size_t)PY_SSIZE_T_MAX / sizeof(PyObject *)) {\n        PyErr_NoMemory();\n        return -1;\n    }\n\n    if (newsize == 0)\n        new_allocated = 0;\n    num_allocated_bytes = new_allocated * sizeof(PyObject *);\n    items = (PyObject **)PyMem_Realloc(self-&gt;ob_item, num_allocated_bytes);\n    if (items == NULL) {\n        PyErr_NoMemory();\n        return -1;\n    }\n    self-&gt;ob_item = items;\n    Py_SIZE(self) = newsize;\n    self-&gt;allocated = new_allocated;\n    return 0;\n}\n</code></pre>"},{"location":"notes/programming/python/#decorators","title":"Decorators","text":"<p>Decorators are a special syntax in python that allow you to wrap a function with another function. For example, consider if we have the function:</p> Python<pre><code>def addFoo(string: str) -&gt; str:\n    return string + \"foo\"\n</code></pre> <p>We can wrap this function in another function that extends the behavior.</p> Python<pre><code>from typing import Callable\n\ndef addBarDecorator(func: Callable[[str], str]) -&gt; Callable[[str], str]:\n    def wrapper(string: str) -&gt; str:\n        string += \"bar\"\n        return func(string)\n    return wrapper\n\naddFoo = addBarDecorator(addFoo)\nprint(addFoo(\"hello\"))\n</code></pre> <pre><code>hellobarfoo\n</code></pre> <p>You can see that our decorator returns a function <code>wrapper</code>, which contains the same signature of the wrapped function. <code>wrapper</code> performs some operation on the input before passing it along to the wrapped function, which is why the result becomes <code>hellobarfoo</code>: <code>bar</code> is added to the string first (in <code>wrapper</code>), then <code>foo</code> is added in <code>addFoo</code>.</p> <p>There is syntatic sugar you can use to avoid having to specify <code>addFoo = addBarDecorator(addFoo)</code>:</p> Python<pre><code>from typing import Callable\n\ndef addBarDecorator(func: Callable[[str], str]) -&gt; Callable[[str], str]:\n    def wrapper(string: str) -&gt; str:\n        string += \"bar\"\n        return func(string)\n    return wrapper\n\n@addBarDecorator\ndef addFoo(string: str) -&gt; str:\n    return string + \"foo\"\n\nprint(addFoo(\"hello\"))\n</code></pre> <pre><code>hellobarfoo\n</code></pre>"},{"location":"notes/programming/python/#context-manager-with-statement","title":"Context Manager (<code>with</code> statement)","text":"<p>A context manager is a magic method you can add to a class that allows you to perform setup/teardown logic that is scoped within a particular lexical block. For example:</p> Python<pre><code>from pathlib import Path\n\npath = Path(\"/Users/landonclipp/test.txt\")\nwith path.open(\"w\") as f:\n    f.write(\"foobar\\n\")\n</code></pre> <p>The <code>with</code> operator first calls the magic method <code>__enter__</code> on the object returned by <code>path.open</code>, which is a <code>TextIOWrapper</code>. When the enclosing block exits, the context manager calls <code>__exit__</code>, which in this case will flush any remaining bytes to the file and close it.</p> <p>We can implement our own context manager. Take this example:</p> Python<pre><code>class Foo:\n    def __init__(self): pass\n\n    def __enter__(self):\n        print(\"enter\")\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        print(\"exit\")\n\nwith Foo() as foo:\n    print(\"enclosing block\")\n</code></pre> <pre><code>enter\nenclosing block\nexit\n</code></pre> <p>The <code>__exit__</code> function takes some additional parameters, as you've noticed. These are explained here:</p> <ol> <li><code>exc_type</code>: The type of exception raised</li> <li><code>exc_value</code>: The value of the exception.</li> <li><code>exc_tb</code>: The traceback of the exception.</li> </ol> <p>Let's see what happens when we raise an exception in our <code>with</code> block:</p> Python<pre><code>class Foo:\n    def __init__(self): pass\n\n    def __enter__(self):\n        print(\"enter\")\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        print(f\"{exc_type = }\")\n        print(f\"{type(exc_type) = }\")\n        print(f\"{exc_val = }\")\n        print(f\"{type(exc_val) = }\")\n        print(f\"{exc_tb = }\")\n        print(f\"{type(exc_tb) = }\")\n        print(\"exit\")\n\nwith Foo() as foo:\n    raise ValueError(\"uh-oh\")\n</code></pre> <pre><code>enter\nexc_type = &lt;class 'ValueError'&gt;\ntype(exc_type) = &lt;class 'type'&gt;\nexc_val = ValueError('uh-oh')\ntype(exc_val) = &lt;class 'ValueError'&gt;\nexc_tb = &lt;traceback object at 0x7f363b472f40&gt;\ntype(exc_tb) = &lt;class 'traceback'&gt;\nexit\nTraceback (most recent call last):\n  File \"main.py\", line 18, in &lt;module&gt;\n    raise ValueError(\"uh-oh\")\nValueError: uh-oh\n\n\n** Process exited - Return Code: 1 **\nPress Enter to exit terminal\n</code></pre> <p>You can see that the value is simply a <code>type</code> object (or just a class). The value of the exception in this case is an instance of the <code>ValueError</code> class, and the traceback is something we can use to determine specifically where the exception happened.</p>"},{"location":"notes/programming/python/#dictionary","title":"dictionary","text":""},{"location":"notes/programming/python/#literal-declaration","title":"Literal declaration","text":"Python<pre><code>print({\"foo\": \"bar\"})\n</code></pre>"},{"location":"notes/programming/python/#function-declaration","title":"Function declaration","text":"Python<pre><code>print(dict(foo=\"bar\"))\n</code></pre>"},{"location":"notes/programming/python/#dict-merging","title":"dict merging","text":""},{"location":"notes/programming/python/#dict-unpacking","title":"Dict unpacking","text":"<p>(this is my preferred way of doing it, for no good reason in particular):</p> Python<pre><code>a = {0: \"a\", 1: \"a\", 2: \"a\"}\nb = {2: \"b\", 3: \"b\"}\nprint({**a, **b})\n</code></pre> <pre><code>{0: 'a', 1: 'a', 2: 'b', 3: 'b'}\n</code></pre>"},{"location":"notes/programming/python/#using-update","title":"Using <code>.update()</code>","text":"Python<pre><code>a = {0: \"a\", 1: \"a\", 2: \"a\"}\nb = {2: \"b\", 3: \"b\"}\na.update(b)\nprint(a)\n</code></pre> <pre><code>{0: 'a', 1: 'a', 2: 'b', 3: 'b'}\n</code></pre>"},{"location":"notes/programming/python/#using-union","title":"Using union","text":"<p>This requires Python &gt;= 3.9</p> Python<pre><code>a = {0: \"a\", 1: \"a\", 2: \"a\"}\nb = {2: \"b\", 3: \"b\"}\nprint(a | b)\n</code></pre> <pre><code>{0: 'a', 1: 'a', 2: 'b', 3: 'b'}\n</code></pre>"},{"location":"notes/programming/python/#async","title":"async","text":""},{"location":"notes/programming/python/#async-functions","title":"async functions","text":"<p>Defined as</p> Python<pre><code>async def foo():\n    print(\"foo\")\n</code></pre>"},{"location":"notes/programming/python/#async-with","title":"<code>async with</code>","text":"<p>This context manager calls the <code>__aenter__</code> and <code>__aexit__</code> magic methods instead of the regular <code>__enter__</code> and <code>__exit__</code>. This is useful when your enter/exit methods rely on potentially expensive external calls.</p> Python<pre><code>async with Connection() as conn:\n    conn.get_item()\n</code></pre>"},{"location":"notes/programming/python/#typing","title":"Typing","text":""},{"location":"notes/programming/python/#typingprotocol","title":"<code>typing.Protocol</code>","text":"<p>The <code>typing.Protocol</code> type allows you to define what is essentially a Go interface. Instead of a function taking a specific implementation of a class like this:</p> Python<pre><code>class RedisCounter:\n    def __init__(self): ...\n\n    def increment(self): ...\n\n    def decrement(self): ...\n\ndef do_stuff(counter: RedisCounter):\n    counter.increment()\n    counter.decrement()\n\ndo_stuff(counter=RedisCounter())\n</code></pre> <p>We can instead define a <code>typing.Protocol</code> type that will define the shape of the class that we accept:</p> Python<pre><code>import typing\n\nclass Counter(typing.Protocol):\n    def increment(self): ...\n\n    def decrement(self): ...\n\nclass RedisCounter:\n    def __init__(self): ...\n\n    def increment(self): ...\n\n    def decrement(self): ...\n\ndef do_stuff(counter: Counter):\n    counter.increment()\n    counter.decrement()\n\ndo_stuff(counter=RedisCounter())\n</code></pre> <p><code>mypy</code> understands and accepts this even though <code>RedisCounter</code> doesn't inherit <code>Counter</code>.</p>"},{"location":"notes/programming/tools/","title":"Tools","text":""},{"location":"notes/programming/tools/#ajax","title":"AJAX","text":"<p>AJAX stands for Asynchronous JavaScript and XML. It is a term used to describe a set of technologies that allow webpages to be dynamically modified without reloading the whole page. Modern usage of AJAX concepts tend to employ JSON instead of XML. AJAX is not a language, nor a framework, but a set of correlated ideas related to dynamic page rendering.</p>"},{"location":"notes/programming/vim/","title":"Vim","text":""},{"location":"notes/programming/vim/#keyboard-shortcuts","title":"Keyboard Shortcuts","text":""},{"location":"notes/programming/vim/#cursor-movement","title":"Cursor Movement","text":"Shortcut Description W Jump forward to start of a word. Shift+W Jump fowrward to start of a word (words can contain punctuation) E Jump forward to the end of a word. Shift+E Jump forwards to the end of a word (words can contain punctuation) B jump backwards to the start of a word Shift+B jump backwards to the start of a word (words can contain punctuation) 0 jump to the start of the line ^ jump to the first non-blank character of the line $ jump to the end of the line G+G go to the first line of the document Shift+G go to the last line of the document"},{"location":"notes/programming/yaml/","title":"YAML","text":"<p>YAML... Yet another markup language. YAML Ain't a Markup Language!</p>"},{"location":"notes/programming/yaml/#explicit-mappings","title":"Explicit Mappings","text":"<p>Mappings in YAML can be defiend either as implicit, such as:</p> YAML<pre><code>foo: bar\none: two\n</code></pre> <p>Or using the explicit notation:</p> YAML<pre><code>?foo\n: bar\n?one\n: two\n</code></pre> <p>Why ever use the explicit notation? Well, you can use any kind of yaml structure as the key:</p> YAML<pre><code>%YAML 1.2\n---\n? - Detroit Tigers\n  - Chicago cubs\n: - 2001-07-23\n</code></pre> <pre><code>&gt;&gt;&gt; from ruamel.yaml import YAML\n&gt;&gt;&gt; yaml=YAML()\n&gt;&gt;&gt; yaml.load(\"\"\"%YAML 1.2\n... ---\n... ? - Detroit Tigers\n...   - Chicago cubs\n... : - 2001-07-23\"\"\")\n{('Detroit Tigers', 'Chicago cubs'): [datetime.date(2001, 7, 23)]}\n</code></pre> <p>Note that this requires a parser capable of loading YAML 1.2. As of 2023-12-20, the more popular <code>yaml</code> and <code>PyYaml</code> packages don't support this syntax.</p>"},{"location":"notes/sysadmin/bmc/","title":"BMC","text":"<p>Baseboard Management Controller is an external computing system that lives on the same chassis as a server. It provides a remote monitoring solution for the host hardware. Dell's BMC is called iDRAC, but most other datacenter server manufacturers provide their own.</p>"},{"location":"notes/sysadmin/bmc/#ipmitool","title":"<code>ipmitool</code>","text":"<p>This is a tool on most Linux distributions that can be used to interact with the BMC through the IPMI interface.</p>"},{"location":"notes/sysadmin/bmc/#get-bmc-address","title":"Get BMC Address","text":"Text Only<pre><code>ipmitool lan print\n</code></pre>"},{"location":"notes/sysadmin/bmc/#get-power-status","title":"Get Power Status","text":"Text Only<pre><code>ipmitool -U ADMIN -H 10.8.54.133 -P &lt;PASS&gt; chassis power status\n</code></pre>"},{"location":"notes/sysadmin/bmc/#redfish","title":"Redfish","text":"<p>Redfish is an HTTP service that runs on a lot of modern BMCs that provides you with a REST endpoint for querying the BMC.</p>"},{"location":"notes/sysadmin/bmc/#authentication","title":"Authentication","text":"<p>You must authenticate with Redfish to obtain an Auth token:</p> Text Only<pre><code>curl --insecure -H \"Content-Type: application/json\" -X POST -D headers.txt https://${bmc_ip}/redfish/v1/SessionService/Sessions -d '{\"UserName\":\"admin\", \"Password\":\"password\"}\n</code></pre> <p>A Python script, created by one of my coworkers at Lambda, can help automate this process.</p>"},{"location":"notes/sysadmin/cpu/","title":"CPU","text":""},{"location":"notes/sysadmin/cpu/#cpu-branch-prediction","title":"CPU Branch Prediction","text":"<p>Branch prediction is a digital circuit on most modern CPUs that attempts to guess which direction an if/else statement (a branch) will take. The accuracy of the prediction plays a major role into improving the pipelining efficiency of a CPU. The predicted instructions will be preemptively executed. If the result of the branch is different from the predicted path, the preemptive result is thrown away and execution is resumed from the true branch path.</p>"},{"location":"notes/sysadmin/cpu/#iommu","title":"IOMMU","text":"<p>IOMMU is a memory-management unit that connects a direct-memory-access-capable (DMA-capable) IO bus to the main memory. IOMMUs are similar to CPU MMUs in that it translates device-visible virtual addresses to phyiscal addresses.</p> <p>An example IOMMU is the graphics address remapping table used by AGP and PCIe graphics cards on Intel and AMD computers.</p>"},{"location":"notes/sysadmin/cpu/#green-thread","title":"Green Thread","text":"<p>A green thread is a thread whose execution is controlled by a runtime or virtual machine instead of by the OS kernel.</p> <p>The Python asyncio event loop is an example of a runtime executing multiple coroutines on a single green thread.</p>"},{"location":"notes/sysadmin/cvmfs/","title":"CVMFS","text":"<p>https://cvmfs.readthedocs.io/en/stable/</p> <p>CVMFS stands for the CERN VM Filesystem. While it was indeed made at CERN, it is not inteded just for VMs. This is just a historical vestige of what it was originally designed for.</p> <p>CVMFS is a semi-POSIX-compliant filesystem that utilizes a Merkle Tree for its metadata storage. The nodes within the Merkle Tree reference file data by content hash. This filesystem is designed to handle large amounts of small files, and more specifically, for the software distribution use case.</p> <p>CVMFS communicates through HTTP both for its metadata and data. This makes it easily cacheable using off-the-shelf caching solutions, such as Varnish. It is designed to be eventually consistent, highly available, and partition tolerant.</p> <p>I have significant experience with CVMFS in designing software distribution CDNs, so please feel free to ask me about it!</p>"},{"location":"notes/sysadmin/databases/","title":"Databases","text":"<p>This is just a dumping ground for various database notes I may have.</p>"},{"location":"notes/sysadmin/databases/#sql","title":"SQL","text":""},{"location":"notes/sysadmin/databases/#dolthub","title":"Dolthub","text":"<p>https://www.dolthub.com/blog/2021-09-17-database-version-control/</p> <p>Dolthub is a SQL-compliant (specifically, Postgresql) relational database that tracks changes using version control semantics, not unlike Git. You can branch changes, merge, fork, commit, undo commits, etc just like in Git. It's a pretty neat idea, but from reading the docs, it appears that write operations don't scale well due to the nature of how changes are persisted. However for non-write heavy workloads that could benefit from having comprehensive change history, this is a great solution.</p>"},{"location":"notes/sysadmin/databases/#views","title":"Views","text":"<p>Most SQL databases have a concept of a \"materialized\" and \"non-materialized\" view. A materialized view is a query that is pre-computed and stored in a temporary table. This temporary table can be queried like any normal table, however the results typically don't change until the view is re-computed.</p> <p>A non-materialized view is simply a query that is performed on another table and presented, usually, as a different schema. Again, the non-materialized view can be queried just like a normal table. The main difference is that the query on the backend tables is run each and every time a query is done on the view.</p>"},{"location":"notes/sysadmin/gpg/","title":"GPG","text":""},{"location":"notes/sysadmin/gpg/#subkeys","title":"Subkeys","text":"<p>Subkeys can be generated from root keys. Subkeys are separate public/private keypairs in their own right, but are mathematically bound to the parent keys.</p> <p>When creating new keys, GPG creates the primary key as signing-only and automatically creates encryption subkeys.</p>"},{"location":"notes/sysadmin/gpg/#why-use-subkeys","title":"Why Use Subkeys?","text":"<p>Subkeys make management easier. Primary keys should be kept incredibly safe, which usually means storing the keys on a Yubikey device that can only be accessed through physical means (and possibly hidden behind lawyers and/or security guards). Subkeys can be generated from the primary key, which can then be used in your security infrastructure. Subkeys can be revoked and re-created if needed by gaining access to the primary key.</p>"},{"location":"notes/sysadmin/gpg/#primary-key-replication","title":"Primary Key Replication","text":"<p>When generating your primary key, there are two main methods:</p> <ol> <li>Generate the key on the Yubikey itself so that the private key at no point has ever touched an external device.</li> <li>Generate the key on an external device and upload it to the Yubikey.</li> </ol> <p>Option 1 is the safest, however it is impossible to copy the Yubikey HSM-generated private key to another Yubikey. Thus it concentrates your chain of trust to a single piece of hardware that if lost or stolen will compromise your entire chain of trust.</p> <p>Option 2 at least allows you to copy the private key to multiple Yubikeys, however you should perform the private key generation on an airgapped, live-loaded OS that lives on a USB thumbdrive. After the key has been generated and copied to the Yubikeys, the thumbdrive's contents should be destroyed through a softwipe of the SSD, and possibly even further by physical destruction.</p>"},{"location":"notes/sysadmin/gpg/#revocation-certificates","title":"Revocation Certificates","text":"<p>https://www.ias.edu/security/creating-revocation-key</p> <p>Revocation certificates are used to invalidate a public key. This certificate is read by clients of a keyserver that they should no longer trust the key. The revocation itself is not necessarily doing anything to mathematically invalidate the key, but it's rather an advisory signal to clients that it should be invalidated and not trusted.</p> <p>These certificates must be signed by the private primary key, and thus if you are using a subkey scheme, you must gain access to the primary key. This is done because clients must use the public primary key to verify the authenticity of the certificate and that such a key does indeed have the authority to revoke the subkey.</p>"},{"location":"notes/sysadmin/gpg/#key-sharding","title":"Key Sharding","text":"<p>If you're using a primary/subkey configuration, it's a good idea to shard your primary key so that it can only be reconstructed through the combination of the requisite hardware tokens.</p> <p>I have not been able to find documentation on how to do this in GPG, so at the moment I only note this concept as a theory.</p>"},{"location":"notes/sysadmin/gpg/#gpg-server","title":"GPG Server","text":"<ul> <li>https://sequoia-pgp.org/blog/2019/06/14/20190614-hagrid/</li> <li>https://github.com/hockeypuck/hockeypuck</li> </ul>"},{"location":"notes/sysadmin/gpu/","title":"GPU","text":""},{"location":"notes/sysadmin/gpu/#architectures","title":"Architectures","text":"Name Launch Date Description Blackwell March 18, 2024 Grace Hopper Unknown Combines the Grace-based 72-core CPU and Hopper-based H200 GPU on a single module. CPU+GPU are connected via NVLink Hopper Sept 20, 2022"},{"location":"notes/sysadmin/gpu/#models","title":"Models","text":"Type Release Date Description GH200 Unknown Combines Grace and Hopper architectures using NVLink-C2C"},{"location":"notes/sysadmin/gpu/#driver","title":"Driver","text":""},{"location":"notes/sysadmin/gpu/#procdrivernvidia","title":"<code>/proc/driver/nvidia</code>","text":"<p>You can use this filesystem to query device information directly using the NVIDIA driver.</p> Text Only<pre><code>$ cat /proc/driver/nvidia/gpus/0000\\:62\\:00.0/information\nModel:           NVIDIA H100 80GB HBM3\nIRQ:             10\nGPU UUID:        GPU-9221d5d1-b60b-a624-bf14-0a1819cfda3b\nVideo BIOS:      96.00.99.00.01\nBus Type:        PCIe\nDMA Size:        52 bits\nDMA Mask:        0xfffffffffffff\nBus Location:    0000:62:00.0\nDevice Minor:    5\nGPU Firmware:    535.129.03\nGPU Excluded:    No\n</code></pre>"},{"location":"notes/sysadmin/gpu/#procsysbuspci","title":"<code>/proc/sys/bus/pci</code>","text":"<p>You can use this to query various PCI-related things.</p>"},{"location":"notes/sysadmin/gpu/#config","title":"<code>config</code>","text":"<p>The config file contains the PCIe config space</p> Text Only<pre><code>$ hexdump /sys/bus/pci/devices/0000\\:62\\:00.0/config\n0000000 10de 2330 0547 0010 00a1 0302 0000 0000\n0000010 000c 0200 38c0 0000 000c 0000 38a0 0000\n0000020 000c 0000 38c0 0000 0000 0000 10de 16c1\n0000030 0000 0000 0040 0000 0000 0000 010a 0000\n0000040 4801 0013 0008 0000 6005 0188 0000 0000\n0000050 0000 0000 0000 0000 0000 0000 0000 0000\n</code></pre> <p>The first two bytes contain the vendor ID. You'll see this should match with the <code>vendor</code> file.</p>"},{"location":"notes/sysadmin/gpu/#vendor","title":"<code>vendor</code>","text":"Text Only<pre><code>$ cat /sys/bus/pci/devices/0000\\:62\\:00.0/vendor\n0x10de\n</code></pre>"},{"location":"notes/sysadmin/gpu/#resource","title":"<code>resource</code>","text":"<p>https://docs.kernel.org/PCI/sysfs-pci.html</p> <p>This is a file that contains PCI host resource addresses.</p>"},{"location":"notes/sysadmin/gpu/#resource0","title":"<code>resource0</code>","text":"<p><code>resourceN</code> are files that can be mmap-ed to in order to read device memory.</p> Python<pre><code>    resource_path = Path(\"/sys/bus/pci/devices/0000:62:00.0/resource0\")\n\n    with resource_path.open(mode=\"rb\") as f:\n        mem = mmap.mmap(f.fileno(), 16 * 1024 * 1024, access=mmap.ACCESS_READ)\n        boot0_region = struct.unpack(\"&lt;L\", mem[:4])[0]\n        ptherm_region = struct.unpack(\"&lt;L\", mem[0x020400:0x020404])[0]\n</code></pre> <p>The exact memory locations depend on the device manufacturer. Consult the manufacturer for more information on what memory locations mean.</p>"},{"location":"notes/sysadmin/gpu/#nvswitch","title":"nvswitch","text":"<p>nvswitch is an NVLink switch that is used to enable cross-GPU communication on NVLink. You can view the available nvswitches on the PCI bus:</p> Text Only<pre><code>$ lspci | grep NVIDIA\n05:00.0 Bridge: NVIDIA Corporation Device 22a3 (rev a1)\n06:00.0 Bridge: NVIDIA Corporation Device 22a3 (rev a1)\n07:00.0 Bridge: NVIDIA Corporation Device 22a3 (rev a1)\n08:00.0 Bridge: NVIDIA Corporation Device 22a3 (rev a1)\n</code></pre> <p>Viewing verbose information of these devices:</p> Text Only<pre><code>$ lspci -vvv -s '05:00.0'\n05:00.0 Bridge: NVIDIA Corporation Device 22a3 (rev a1)\n        Subsystem: NVIDIA Corporation Device 1796\n        Physical Slot: 1\n        Control: I/O- Mem+ BusMaster- SpecCycle- MemWINV- VGASnoop- ParErr+ Stepping- SERR+ FastB2B- DisINTx-\n        Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast &gt;TAbort- &lt;TAbort- &lt;MAbort- &gt;SERR- &lt;PERR- INTx-\n        Interrupt: pin A routed to IRQ 16\n        NUMA node: 0\n        IOMMU group: 5\n[...]\n</code></pre> <p>Unhealthy nvswitch</p> <p>Often, an nvswitch can be detected as being unhealthy if the lspci output shows <code>(rev ff)</code>. This indicates corruption on the device.</p>"},{"location":"notes/sysadmin/infiniband/","title":"Infiniband","text":""},{"location":"notes/sysadmin/infiniband/#bandwidth-test","title":"Bandwidth Test","text":"<p>On one node, start a listener:</p> Text Only<pre><code>ib_write_bw -d mlx5_1 --report_gbits -s 8388608 -F -f 0 --duration 30 -q 4\n</code></pre> <p>On another:</p> Text Only<pre><code>ib_write_bw -d mlx5_1 [listener_ip] --report_gbits -s 8388608 -F -f 0 --duration 30 -q 4\n</code></pre> <p>The listener IP does not need to be an IPoIB address. It's a connection that is only used to negotiate the IB connection.</p>"},{"location":"notes/sysadmin/infiniband/#pkey","title":"Pkey","text":"<p>A pkey, or Partition Key, is analogous to a VLAN tag. It allows you to partition your network amongst multiple tenancies such that HCAs within a pkey can only talk to other HCAs in that pkey.</p>"},{"location":"notes/sysadmin/infiniband/#virtualization","title":"Virtualization","text":"<p>Infiniband cards can take advantage of SR-IOV just like most other PCIe devices.</p> <p>https://docs.nvidia.com/networking/display/mlnxofedv522230/single+root+io+virtualization+(sr-iov)</p>"},{"location":"notes/sysadmin/k8s/","title":"Kubernetes","text":""},{"location":"notes/sysadmin/k8s/#irsa","title":"IRSA","text":"<p>AWS's EKS clusters provide the IAM Roles for Service Accounts (IRSA) mechanism for attaching k8s service accounts to an IAM role.</p>"},{"location":"notes/sysadmin/k8s/#iam-authentication-flow","title":"IAM Authentication Flow","text":""},{"location":"notes/sysadmin/k8s/#iam-token-authentication-process","title":"IAM Token Authentication Process","text":"<p>AWS services use the following process to verify the validity of the IAM token passed to them:</p> <p></p>"},{"location":"notes/sysadmin/k8s/#aws_web_identity_token_file","title":"<code>AWS_WEB_IDENTITY_TOKEN_FILE</code>","text":"<p>This environment variable is injected into EKS pods that have an attached service account. This points to a file that contains a JWT. I created an example JWT:</p> JWTDecoded JWTPrivate KeyPublic Key <pre><code>eyJhbGciOiJSUzI1NiIsImtpZCI6Ijg4MjAxNDY0N2JkZWQ0OWIzNzk3MWY0OGNhMTEwMzdlNWM3MzQ2MWIifQ.eyJhdWQiOlsic3RzLmFtYXpvbmF3cy5jb20iXSwiZXhwIjoxNzQ5ODM5NTA0LCJpYXQiOjE3NDk3NTMxMDQsImlzcyI6Imh0dHBzOi8vb2lkYy5la3MudXMtd2VzdC0yLmFtYXpvbmF3cy5jb20vaWQvMTIzNCIsImp0aSI6ImQyZTEyNzk5LTE3NDctNGFmMy05M2U5LWFlOTJlODcxMzI0OSIsImt1YmVybmV0ZXMuaW8iOnsibmFtZXNwYWNlIjoiazhzLW5hbWVzcGFjZSIsIm5vZGUiOnsibmFtZSI6ImlwLTE5Mi0xNjgtMC0xLnVzLXdlc3QtMi5jb21wdXRlLmludGVybmFsIiwidWlkIjoiZDJlMTI3OTktMTc0Ny00YWYzLTkzZTktYWU5MmU4NzEzMjQ5In0sInBvZCI6eyJuYW1lIjoicG9kLW5hbWUtNWJjNDc2ZjliLWNmdm1iIiwidWlkIjoiZDJlMTI3OTktMTc0Ny00YWYzLTkzZTktYWU5MmU4NzEzMjQ5In0sInNlcnZpY2VhY2NvdW50Ijp7Im5hbWUiOiJzZXJ2aWNlLWFjY291bnQtbmFtZSIsInVpZCI6ImQyZTEyNzk5LTE3NDctNGFmMy05M2U5LWFlOTJlODcxMzI0OSJ9fSwibmJmIjoxNzQ5NzUzMTA0LCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6azhzLW5hbWVzcGFjZTpzZXJ2aWNlLWFjY291bnQtbmFtZSJ9.i4nzWi3liux_yKiRRUqaeB-hOOBy7gEG5oyhaxDLaSUpueybSUWQKy1BNWMi-CPOzp7J66k3JAb1AvUOMPv0Uolwu_gut6-mVlhyNp5DmwcCk6zpXlHqVF06yeSxj0Ix3v-K-J8Z18TwoHUMKgcDyic0c-YVTSTaM8q9x9oXfQr76HcHsI2JZLsdiYNquC4JS8sAq6xR1FJ3PmBUrq45cGnpVU73_q9E4dmtDzoo360k5RqG2kRWZmGBZw76WXdExY_ZkyA9WptEfk9cWajv_sFqaQonA5GqDuP57nwOrv2_h4Qi0Emju-soLJbn6vTYj4xGEcP7vyCPjCdk3pa5Eg\n</code></pre> Header<pre><code>{\n\"alg\": \"RS256\",\n\"kid\": \"882014647bded49b37971f48ca11037e5c73461b\"\n}\n</code></pre> Payload<pre><code>{\n\"aud\": [\n    \"sts.amazonaws.com\"\n],\n\"exp\": 1749839504,\n\"iat\": 1749753104,\n\"iss\": \"https://oidc.eks.us-west-2.amazonaws.com/id/1234\",\n\"jti\": \"d2e12799-1747-4af3-93e9-ae92e8713249\",\n\"kubernetes.io\": {\n    \"namespace\": \"k8s-namespace\",\n    \"node\": {\n    \"name\": \"ip-192-168-0-1.us-west-2.compute.internal\",\n    \"uid\": \"d2e12799-1747-4af3-93e9-ae92e8713249\"\n    },\n    \"pod\": {\n    \"name\": \"pod-name-5bc476f9b-cfvmb\",\n    \"uid\": \"d2e12799-1747-4af3-93e9-ae92e8713249\"\n    },\n    \"serviceaccount\": {\n    \"name\": \"service-account-name\",\n    \"uid\": \"d2e12799-1747-4af3-93e9-ae92e8713249\"\n    }\n},\n\"nbf\": 1749753104,\n\"sub\": \"system:serviceaccount:k8s-namespace:service-account-name\"\n}\n</code></pre> <pre><code>-----BEGIN RSA PRIVATE KEY-----\nMIIEpQIBAAKCAQEAmKQLQ5xf0WdB9Wo1H0ahUlJfvQA2UyIOI4lmXiRUHudVCSl5\nzayH8R0aVY+rjFvpjNb4CUV8AOBk9W7dZ96WUF3zysFSUfFTVucPTdi1vZ416rhP\nFt9faJbYrV/zgLzo3pMxaooZZLY/e2U74+GJx+Obd+MpBh55ZGFSpGzhEPivAhmt\n3qGdPvotsmDpl0BU1hzrUBP9Tlfk0AiQUtBWBc9B69l7UQ+IfUnpjubVvw+uKW8e\nwODe5srfXMM+G4P8imxbZ5bE5M71HblyP5EwpykGQk8KZfxbOkMQduUO3hL+o3+o\nJ70hHwCwy/mM7Q9X/nB8W/CazKbkDuaqlmHDjwIDAQABAoIBAGTHeoBrUIArksr8\nEpLRyVVW+csJxtRgmcEwyJvDlJ7K0cQ41CjNjvvM4UQ4lWUYkjzg+eb1L4hMn5vE\nVP5tYgUx4vKtbTKZCRNJfztAp83IFYUhp2ZMmOuvHORrg5QuJfo+aNQU1es+wO8x\nybcNhDCrwEwJeV6G5FM4Rp41HOxcmVg1heUqdncId5sLgZ5DqL/bElQ0Ur8cwkP6\n9d/IKHzCC2Emg2oXnaWYLrZCnf8UlPD+VaCjhhDiUaQMpcwCtsPP9J2LmsJkdDVq\nlFHrIz/49etkSeFdny6m5Ko1IM7xg8L2OEO4pSmWd1c+lPlGodltoOR4Gbpw5kWI\na5QVntECgYEA94Hi8HqG3SO0lW/ux9tv0W4V+HioQNwPlfff7cfYr5SCUuJUov52\n7HFqdYGmcf4+QFcC9kILEqhXJmAWHIbL5ztSW1w07WMBoWaRWWQjKhlNR65V5LMO\n04mgJriteCfxRKVz7qGSNkxtXjW9D16GRnDObH7LsM3xLhZ1mWaOuw0CgYEAneDY\nnz/Gq2oGT1BgzgfX/1U22J/i36z2sJoBmYutWedE6Q37ALdAKW2Kcg+uYn0nyKPp\nmsULH6BdfGPCyS/as3kpl9ZGnO9vvSIwvIwnb8+ijsijpgzP06d3MgKxryI4qnoW\nfzyWL+Z9+JDIKX+Ovdk5KdL8Ys2r5bZ4a6UIIgsCgYEAiuk9I1bBRm/5tD1kOIsy\nEMrGTfP6Cpz3qmW6KkwVk0W4dyhL9Eb5G+7znwurDSNycM3k/a1HZatRblpTTqNg\n4WBr8panpewBSEK2pQMMsV4N/4Ma9gaA9byyQ8k2os9YG/z4OiA4XX07jLqAZ5XX\nqsU5Na9cRtju2b0FN5lV1ikCgYEAmBbo19C6dJs45ONLvm7z97eBlIHCLzQSmeJi\ntCZIoxkV22VqbjAakU5DVsogdiSXVMQC4OP9aUQ1iwUXunRtPQP1u53ifIB0pkDv\nrlNeEmp18RL8A5TICN+FBhpuTYasDB582anmDNMAe5oOGVwWiHFgjhgAprX3aN21\neA2Nni0CgYEA7PM1oM86XqPrNbZFKExMWlifGOoqervoqB/fm9rUJt14AvfyE5ST\ngt/UnvS43aI32Adw1nG991DskYLrQ8WcmirCZIbaxFreuM2aKgbpBqit4q1AFcp0\neO2DcbvHyDU0aucSkJZAPYxXujFLWQ+3wnVnrz9eN+nqKiKvQxcHADs=\n-----END RSA PRIVATE KEY-----\n</code></pre> <pre><code>-----BEGIN PUBLIC KEY-----\nMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAmKQLQ5xf0WdB9Wo1H0ah\nUlJfvQA2UyIOI4lmXiRUHudVCSl5zayH8R0aVY+rjFvpjNb4CUV8AOBk9W7dZ96W\nUF3zysFSUfFTVucPTdi1vZ416rhPFt9faJbYrV/zgLzo3pMxaooZZLY/e2U74+GJ\nx+Obd+MpBh55ZGFSpGzhEPivAhmt3qGdPvotsmDpl0BU1hzrUBP9Tlfk0AiQUtBW\nBc9B69l7UQ+IfUnpjubVvw+uKW8ewODe5srfXMM+G4P8imxbZ5bE5M71HblyP5Ew\npykGQk8KZfxbOkMQduUO3hL+o3+oJ70hHwCwy/mM7Q9X/nB8W/CazKbkDuaqlmHD\njwIDAQAB\n-----END PUBLIC KEY-----\n</code></pre> <p>This JWT is generated by the OIDC provider.</p>"},{"location":"notes/sysadmin/k8s/#oidc-provider","title":"OIDC Provider","text":"<p>The EKS OIDC provider acts as the trust bridge between your Kubernetes cluster and AWS IAM. Here's what it does:</p>"},{"location":"notes/sysadmin/k8s/#identity-provider","title":"Identity Provider","text":"<ol> <li>Each EKS cluster gets a unique OIDC provider URL (like https://oidc.eks.us-east-1.amazonaws.com/id/EXAMPLED539D4633E)</li> <li>This provider issues and signs JWT tokens for service accounts</li> <li>The tokens contain claims about which service account is making the request</li> </ol>"},{"location":"notes/sysadmin/k8s/#cryptographic-trust","title":"Cryptographic Trust","text":"<ol> <li>The OIDC provider has a private key for signing tokens</li> <li>It publishes the corresponding public key at /.well-known/jwks.json</li> <li>AWS STS fetches these public keys to verify token signatures</li> </ol>"},{"location":"notes/sysadmin/k8s/#token-contents","title":"Token Contents","text":"<p>The JWT tokens include critical claims:</p> <ul> <li>sub (subject): <code>system:serviceaccount:namespace:serviceaccount-name</code></li> <li>iss (issuer): The OIDC provider URL</li> <li>aud (audience): <code>sts.amazonaws.com</code></li> <li>exp (expiration): Token validity period</li> </ul>"},{"location":"notes/sysadmin/k8s/#trust-establishment","title":"Trust Establishment","text":"<p>When you create an IAM role for IRSA:</p> JSON<pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [{\n    \"Effect\": \"Allow\",\n    \"Principal\": {\n      \"Federated\": \"arn:aws:iam::123456789012:oidc-provider/oidc.eks.region.amazonaws.com/id/EXAMPLED539D4633E\"\n    },\n    \"Action\": \"sts:AssumeRoleWithWebIdentity\",\n    \"Condition\": {\n      \"StringEquals\": {\n        \"oidc.eks.region.amazonaws.com/id/EXAMPLED539D4633E:sub\": \"system:serviceaccount:default:my-service-account\",\n        \"oidc.eks.region.amazonaws.com/id/EXAMPLED539D4633E:aud\": \"sts.amazonaws.com\"\n      }\n    }\n  }]\n}\n</code></pre>"},{"location":"notes/sysadmin/k8s/#why-this-design","title":"Why This Design?","text":"<p>Security Benefits:</p> <ul> <li>No AWS credentials stored in the cluster</li> <li>Tokens are short-lived and auto-rotate</li> <li>Fine-grained access control per service account</li> <li>Standard OIDC protocol that AWS already supports</li> </ul> <p>Operational Benefits:</p> <ul> <li>No credential rotation needed</li> <li>Kubernetes-native (uses service accounts)</li> <li>Auditable through CloudTrail</li> <li>Works with existing IAM policies</li> </ul> <p>The OIDC provider is essentially what makes it possible for AWS to trust that a request is genuinely coming from a specific service account in a specific EKS cluster, without requiring long-lived AWS credentials to be stored anywhere in the cluster.</p>"},{"location":"notes/sysadmin/k8s/#helm","title":"Helm","text":""},{"location":"notes/sysadmin/k8s/#show-values","title":"Show values","text":"<p>For an installed release:</p> Text Only<pre><code>lclipp@CW-HP216DG9DT-L kata-gpu-experiment % helm get values kata-deploy -n kata-system\n</code></pre> <p>For all computed values of an installed release:</p> Text Only<pre><code>lclipp@CW-HP216DG9DT-L kata-gpu-experiment % helm get values --all kata-deploy -n kata-system\n</code></pre> <p>For a chart (before installation)</p> Text Only<pre><code>lclipp@CW-HP216DG9DT-L kata-gpu-experiment % helm show values oci://ghcr.io/kata-containers/kata-deploy-charts/kata-deploy\n</code></pre>"},{"location":"notes/sysadmin/misc/","title":"Misc","text":""},{"location":"notes/sysadmin/misc/#numa","title":"NUMA","text":"<p>Most modern datacenter servers have multiple CPU sockets. Each socket is generally physically close to a portion of the main memory. Each socket is capable of accessing the memory of another NUMA node, however this incurs performance penalties as it has to traverse the memory bus.</p>"},{"location":"notes/sysadmin/misc/#dmidecode","title":"dmidecode","text":"<p>Use <code>dmidecode</code> to read verbose information on all hardware being used on the computer. This tool decodes the SMBIOS table in a human-readable format.</p> <p>The acronym <code>DMI</code> refers to the Desktop Management Interface which is a closely-related standard to SMBIOS that <code>dmidecode</code> was originally written for.</p>"},{"location":"notes/sysadmin/misc/#vfio","title":"VFIO","text":"<p>VFIO, or Virtual Function Input/Output, is a userspace device driver framework that allows userspace processes to act like privileged device drivers for hardware. It makes use of IOMMU memory protection semantics so that virtual machines, for example, can have exclusive and isolated access to device memory.</p>"},{"location":"notes/sysadmin/network-tracing/","title":"Tracing","text":"<p>This section describes how various path tracing utilities work (in general).</p> <p>When using ICMP mode, the tools will follow this general pattern:</p> <ol> <li>Set the TTL field of the ICMP packet to 1.</li> <li>Send this packet to the destination address.</li> <li>The first router in the path will decrement the TTL by 1, making it 0. This indicates to the router that the packet should be discarded.</li> <li>The router replies with an \"ICMP Time Exceeded\" message that will identify the router to the tracepath utility.</li> <li>The utility sends a new packet to the destination with TTL 1 higher than before, repeat from step 2.</li> <li>Once the packet reaches the destination host, it replies with an ICMP echo reply.</li> </ol> <p>Some of the challenges with ICMP:</p> <ol> <li>Not all routers participate in ICMP Time Exceeded replies.</li> <li>Some routers won't even decrement at all!</li> <li>Asymmetric routing may differ from the forward path.</li> <li>Some firewalls block ICMP.</li> </ol> <p>To get around ICMP being blocked, there are other options, such as:</p> <ol> <li>Using UDP packets (which are almost never blocked). <code>traceroute</code> does UDP probing by default.</li> <li>Using TCP SYN probes.</li> </ol>"},{"location":"notes/sysadmin/network-tracing/#utilities","title":"Utilities","text":"<p>The common utilities for network tracing are:</p> <ol> <li>tracepath</li> <li>traceroute</li> <li>mtr</li> </ol> <p>MTR is probably the most featured option; I recommend it for most use-cases. You can install it with most typical repositories:</p> brewaptyum <pre><code>brew install mtr\n</code></pre> <pre><code>apt install mtr\n</code></pre> <pre><code>yum install mtr\n</code></pre>"},{"location":"notes/sysadmin/network-tracing/#icmp-tracing","title":"ICMP Tracing","text":"<pre><code>sequenceDiagram\n    Client-&gt;&gt;Hop 1: ICMP ECHO TTL 1\n    Hop 1-&gt;&gt;Client: ICMP TIME EXCEEDED\n    Client-&gt;&gt;Hop 1: ICMP ECHO TTL 2\n    Hop 1-&gt;&gt;Hop 2: ICMP ECHO TTL 1\n    Hop 2-&gt;&gt;Client: ICMP TIME EXCEEDED\n    Client-&gt;&gt;Hop 1: ICMP ECHO TTL 3\n    Hop 1-&gt;&gt;Hop 2: ICMP ECHO TTL 2\n    Hop 2-&gt;&gt;Destination: ICMP TTL 1\n    Destination-&gt;&gt;Client: ICMP DESTINATION UNREACHABLE</code></pre> <p>The <code>ICMP TIME EXCEEDED</code> message contains the IP of the router in which the TTL was exceeded. ICMP packets are encapsulated in IP headers. The header includes the first 64 bits of the original datagram's data, which allows the client to match the message to the appropriate process that initiated the tracing procedure.</p> <p>The interesting thing to note is that ICMP tracing involves sending packets to a high-numbered port that is likely to not have any process attached. When the ICMP packet arrives at the destination, a <code>DESTINATION UNREACHABLE</code> message is sent in reply, indicating the end of the packet's network path.</p>"},{"location":"notes/sysadmin/network-tracing/#tcp-tracing","title":"TCP Tracing","text":"<pre><code>sequenceDiagram\n    Client-&gt;&gt;Hop 1: TCP SYN PORT 80 TTL 1\n    Hop 1-&gt;&gt;Client: ICMP TIME EXCEEDED\n    Client-&gt;&gt;Hop 1: TCP SYN PORT 80 TTL 2\n    Hop 1-&gt;&gt;Hop 2: TCP SYN PORT 80 TTL 1\n    Hop 2-&gt;&gt;Client: ICMP TIME EXCEEDED\n    Client-&gt;&gt;Hop 1: TCP SYN PORT 80 TTL 3\n    Hop 1-&gt;&gt;Hop 2: TCP SYN PORT 80 TTL 2\n    Hop 2-&gt;&gt;Destination: TCP SYN PORT 80 TTL 1\n    Destination-&gt;&gt;Client: TCP SYN/ACK\n    Client-&gt;&gt;Destination: TCP RST</code></pre> <p>TCP tracing works similarly to ICMP. Instead of using a plain ICMP packet, it uses the TCP protocol (which itself is also encapsulated in IP packets, of course) to appear to firewalls as legitimate traffic. It targets a specific port, usually 80 although it can be whatever port you'd like. Each hop in the route will send back the typical <code>ICMP TIME EXCEEDED</code> message when the TTL expires. The main difference is that the destination will reply with either a <code>TCP SYN/ACK</code> (in the case there is a process accepting connections on the port) or a <code>TCP RST</code> (in the case the destination rejects the connection). In either case, this is confirmation to the client that the packet has reached its final destination.</p>"},{"location":"notes/sysadmin/network-tracing/#asymmetric-routing","title":"Asymmetric Routing","text":"<p>These tools will often report when the packets go through different routers between the forward and return trips. Asymmetry can be detected in the following ways:</p> <ol> <li>Inconsistent IPs for the same hop during subsequent probes.</li> <li>Inconsistent RTT (round-trip-time) latency. Hops at higher indicies should always have higher latency. If this is not the case, it indicates possible routing asymmetry.</li> <li>ICMP Time Exceeded response TTL is unexpected. The TTL in the response packet header should contain more or less the same number of decrements as expected for the number of hops.</li> </ol>"},{"location":"notes/sysadmin/networking/","title":"Networking","text":""},{"location":"notes/sysadmin/networking/#anycast","title":"Anycast","text":"<p>Anycast is a form of network routing whereby multiple servers share the same IP address, and messages to that IP address are routed to the server that is topographically the closest. Traffic to the IP address is routed according to the routing tables set up in the network's router. Administrators will often configure their routers to point to the geographically nearest server, although the scheme is flexible enough to allow re-routing to a different physical location should the need arise.</p> <pre><code>flowchart TD\n    client\n    router\n    server\n\n    clientNY[client]\n    routerNY[router]\n    serverNY[server]\n\n    subgraph Chicago\n    client --&gt;|service-anycast.landon.com| router --&gt; server\n    end\n\n    subgraph New York\n    clientNY --&gt;|service-anycast.landon.com| routerNY --&gt; serverNY\n    end</code></pre> <p>Administrators have the ability to change the routing in Chicago to New York, without needing any changes to the client.</p> <pre><code>flowchart TD\n    client\n    router\n\n    clientNY[client]\n    routerNY[router]\n    serverNY[server]\n\n    subgraph New York\n    clientNY --&gt;|service-anycast.landon.com| routerNY --&gt; serverNY\n    end\n\n    subgraph Chicago\n    client --&gt;|service-anycast.landon.com| router --&gt; serverNY\n    end\n</code></pre> <p>Anycast is a popular way to route to DNS servers, as it removes the need for clients to understand where they are geographically, or to maintain a list of appropriate DNS servers. This is maintained by the network administrators via the routing tables.</p>"},{"location":"notes/sysadmin/networking/#multicast","title":"Multicast","text":"<p>Multicast is a routing mechanism that acts similar to a pub-sub topic. The typical IPv4 range is 224.0.0.0/4, which encompasses 224.0.0.0 to 239.255.255.255. Multicast is implemented using the Internet Group Management Protocol (IGMP). Clients must issue an IGMP JOIN command to the nearest switch or router to indicate it wants to be a member of the particular multicast group, identified as one of the aforementioned IP addresses. The router/switch maintains a list of hosts subscribed to each group and forwards the traffic to each IP in the list.</p>"},{"location":"notes/sysadmin/networking/#kernel-routing-tables","title":"Kernel Routing Tables","text":"Text Only<pre><code>$ netstat -rn\nKernel IP routing table\nDestination     Gateway         Genmask         Flags   MSS Window  irtt Iface\n0.0.0.0         7.151.164.131   0.0.0.0         UG        0 0          0 eno1\n7.0.0.0         7.151.164.131   255.0.0.0       UG        0 0          0 eno1\n7.151.164.128   0.0.0.0         255.255.255.128 U         0 0          0 eno1\n$ ip r s\ndefault via 7.151.164.131 dev eno1 proto static metric 100\n7.0.0.0/8 via 7.151.164.131 dev eno1 proto static metric 65024000\n7.151.164.128/25 dev eno1 proto kernel scope link src 7.151.164.166 metric 100\n7.151.176.0/21 dev ib0 proto kernel scope link src 7.151.181.31 metric 150\n</code></pre>"},{"location":"notes/sysadmin/networking/#dhcp","title":"DHCP","text":"<p>DHCP, or Dynamic Host Configration Protocol, is a protocol used to dynamically assign IP address to hosts within its network. DHCP can also be used to send live kernel images, which is how many netbooting is performed.</p> <pre><code>sequenceDiagram\n    Client-&gt;&gt;DHCP Server: DISCOVER: Discover all DHCP servers on subnet\n    DHCP Server--&gt;&gt;Client: OFFER: Server receives ethernet broadcast and offers IP address\n    Client-&gt;&gt;DHCP Server: REQUEST: Client sends REQUEST broadcast on subnet using offered IP.\n    DHCP Server--&gt;&gt;Client: ACK: Server responds with unicast and ACKs request.</code></pre>"},{"location":"notes/sysadmin/networking/#discover","title":"DISCOVER","text":"<p>Discover messages are sent out on the subnet the host is configured with using the broadcast address specific to the client's subnet. For example, in the case the subnet is located at 192.168.0.0/16, the broadcast address would be 192.168.255.255.</p> <p>Clients can also request a specific IP address if desired. If not requested, the DHCP server will offer a random address from within its pool.</p>"},{"location":"notes/sysadmin/networking/#decline","title":"DECLINE","text":"<p>There can be situations in which two clients are errnoneously allocated the same IP address. When a client receives an ACK from its DHCP server, indicating that particular server granted the IP lease, the client should use ARP to discover if any other computer on the subnet is using the same address. In the case the client determines this to be true, it should send a DECLINE broadcast to reneg the lease.</p>"},{"location":"notes/sysadmin/networking/#release","title":"RELEASE","text":"<p>When a client is done with an IP address, it can send a RELEASE broadcast to reneg the lease.</p>"},{"location":"notes/sysadmin/networking/#netbootingpxe","title":"Netbooting/PXE","text":"<p>DHCP servers can be configured to provide a <code>next-server</code> address, which is provided to the DHCP client when an IP address is given. Netbooting is not strictly part of the DHCP spec, but is rather executed as part of the PXE (Preboot Execution Environment) specification. A PXE boot utilizes the <code>next-server</code> address and <code>filename</code> parameters provided from DHCP that indicates an TFTP server that serves a loadable bootstrap program. When a client receives these parameters, it then initiates a download from the listed TFTP server and loads the bootstrap program. This interaction is usually done entirely in firmware on the NIC.</p>"},{"location":"notes/sysadmin/networking/#arista-metawatch","title":"Arista MetaWatch","text":"<p>https://www.arista.com/en/products/7130-meta-watch</p> <p>Arista is a company that provides various networking solutions. They have developed a switch application called MetaWatch that you can install into your Arista 7130L Series devices. This application allows you to dynamically tap incoming ports in hardware instead of needing to use physical optical taps. The application can be configured to aggregate incoming ports into outgoing ports, which is a form of port multiplexing, AKA link aggregation, AKA port aggregation.</p> <p>The power of in-hardware tapping is that you can dynamically re-assign where incoming ports are sent to without needing any physical work. The downside is that because the port aggregation/tapping is done in physical computing hardware, you can get into buffer overflow issues and drop packets. In practice this is usually not an issue, but it is a consideration that must be made.</p>"},{"location":"notes/sysadmin/networking/#ethernet","title":"Ethernet","text":""},{"location":"notes/sysadmin/networking/#frame-check-sequence-fcs","title":"Frame Check Sequence (FCS)","text":"<p>This is an error-detecting code added to the end of an ethernet frame to detect corruption in the packet. A common implementation for FCS is that the receiver will compute a running sum of the frame, including the trailing FCS. The summation is expected to result in a fixed number, often zero. Another common algorithm is the CRC.</p>"},{"location":"notes/sysadmin/networking/#dns","title":"DNS","text":"<p>Types of DNS record types</p> Name Description A record The most common. It maps a name to an IP address AAAA record Same as A record, but points to an IPv6 address. CNAME Short for \"canonical name,\" it creates an alias for an A record. Remember, these point to other A records, not to IP addresses. NS A nameserver record specifies DNS servers for a particular domain. MX record Mail Exchange records shows where emails for a domain should be routed to. SOA Start of Authority records stores admin info about a domain. Includes email of admin and other things as well. TXT A Text record stores arbitrary data SRV This record stores the IP and port of particular services. CERT Stores public key certificates. DCHID Stores information related to DHCP. DNAME A Delegation Name record allows you to redirect entire subdomains to a new domain. For example, <code>www.example.com</code> is often redirected to <code>example.com</code> in many companies, as the <code>www</code> prefix is often not actually used."},{"location":"notes/sysadmin/networking/#tcp","title":"TCP","text":""},{"location":"notes/sysadmin/networking/#keepalive","title":"Keepalive","text":"<p>Keepalive is an OS parameter that tells the OS to regularly send empty TCP packets across the network every so often in order to keep the connection alive and prevent network components from expiring the connection.</p> <p>For example, in the <code>/etc/sysctl.conf</code> file, you can set these parameters:</p> Text Only<pre><code>net.ipv4.tcp_keepalive_time = 600\nnet.ipv4.tcp_keepalive_probes = 9\nnet.ipv4.tcp_keepalive_intvl = 25\n</code></pre> <p>If you find your TCP connections are hanging or being closed unexpectedly, it's possible they are being dropped by NAT or firewall.</p> <p>Relevant Links:</p> <ul> <li>TCP Keepalive Howto</li> </ul>"},{"location":"notes/sysadmin/networking/#ssh-optimizations","title":"SSH Optimizations","text":"<p>TCP has a common problem in high-latency/lossy network paths where it will spend a lot of time establishing connections, acking packets, and re-sending packets. rsync uses SSH, which uses TCP to send data over the wire. OpenSSH does not provide any means of tuning TCP parameters, but there is an OpenSSH fork called HPN-SSH that provides options for you to tune things like:</p> <ol> <li>TCP Recieve buffer size</li> <li>TCP send buffer size</li> <li>TCP window size</li> </ol> <p>These parameters can be increased to provide more tolerance to lossy or high-latency networks. In addition, you might also want to select different SSH ciphers to reduce load on the CPU. Here is a great blog on various benchmarks that were performed on commonly-available ciphers.</p> <p>Redhat has a blog on how to set buffer sizes.</p> Text Only<pre><code># sysctl -w net.core.rmem_default=262144\n# sysctl -w net.core.wmem_default=262144\n# sysctl -w net.core.rmem_max=262144\n# sysctl -w net.core.wmem_max=262144\n</code></pre>"},{"location":"notes/sysadmin/networking/#rsync","title":"rsync","text":""},{"location":"notes/sysadmin/networking/#optimizations","title":"Optimizations","text":"<p>rsync relies on SSH, and many optimizations can be applied to it as shown here. This is where most of your performance improvements will come from.</p> <p>The main method for improving aggregate rsync throughput is to spawn more processes so that more streams are being sent simultaneously. A single rsync process will eventually hit a max throughput, which is limited by TCP handshakes and retransmits. Some optimizations you can do:</p> <ol> <li>Increase TCP window size (more data sent per round trip)</li> <li>Increase TCP send buffer size (kernel parameter)</li> <li>Increase TCP receive buffer size (kernel parameter)</li> </ol>"},{"location":"notes/sysadmin/networking/#rdma","title":"RDMA","text":"<p>Remote Direct Memory Access is a method of direct memory access across the network that does not involve either server's operating system.</p>"},{"location":"notes/sysadmin/networking/#roce","title":"RoCE","text":"<p>RDMA over Converged Ethernet is an RDMA implementation over ethernet. It utilizes an Infiniband payload encapsulated by ethernet. RoCE's goal is to port Infiniband's specification of how to do RDMA over an ethernet network. This is why the Infiniband payload is encapsulated as that protocol is still used on the host side to perform the RDMA access.</p>"},{"location":"notes/sysadmin/networking/#arp","title":"ARP","text":"<p>Address Resolution Protocol is used for mapping MAC addresses to IP addresses. This is often used by routers on a local network to figure out what host (or MAC) has been assigned a specific IP.</p>"},{"location":"notes/sysadmin/networking/#mime-types","title":"MIME Types","text":"<p>Media Types (or formerly known as MIME types) are names given to particular message formats. Historically this has been used in HTTP to define what kind of data is in the message body, but it is more broadly applicable to any kind of messaging protocol. The Media type is roughly analagous to a file extension.</p>"},{"location":"notes/sysadmin/networking/#network-namespace","title":"Network Namespace","text":"<p>Linux allows you to create a \"network namespace\" that acts kind of like a chroot but for network interfaces. It allows you to isolate a process from the host's network cards and create your own virtualized network topology.</p> <p>https://medium.com/@tech_18484/how-to-create-network-namespace-in-linux-host-83ad56c4f46f</p>"},{"location":"notes/sysadmin/networking/#nccl","title":"NCCL","text":"<p>NCCL stands for NVIDIA Collective Communication Library. It is a library used in applications that need to do collective, cross-GPU actions. It's topology-aware and allows an abstracted interface to the set of GPUs being used across a cluster system, such that applications don't need to understand where a particular GPU resides.</p>"},{"location":"notes/sysadmin/networking/#load-balancing","title":"Load Balancing","text":""},{"location":"notes/sysadmin/networking/#layer-2-direct-server-return-dsr","title":"Layer 2 Direct Server Return (DSR)","text":"<p>DSR is a method of load balancing whereby the server sitting behind the load balancer will reply directly to the originating client instead of through the LB. The general flow is as follows:</p> <ol> <li>Client packets arrive in the load balancer.</li> <li>The load balancer makes a decision on which backend to forward to. It will modify the destination MAC address in the ethernet frame to the chosen backend and retransmit the packet to the MAC of its chosen backend (this is critical to make it appear to the layer 2 network that the packet did not originate from the LB).</li> <li>The chosen backend receives the packet. Because the layer 3 IP frame was untouched, the packet appears in all respects as if it came from the originating client. Thus, the backend will respond directly to the client IP through the default IP gateway.</li> </ol> <p>It should be noted that, of course, the backends and the load balancer need to be configured with a VIP. When the LB forwards the packet to the backend, the destination IP is unchanged, only the destination MAC. So this means that the LB and the backend services need to be on the same layer 2 network. Because all the backends are configured with the same VIP, they will respond to the LB-forwarded packet.</p>"},{"location":"notes/sysadmin/networking/#topologies","title":"Topologies","text":""},{"location":"notes/sysadmin/networking/#rail-optimizedhttpsinfohubdelltechnologiescomzh-cnlgenerative-ai-in-the-enterprise-with-nvidia-gpus-and-dell-powerswitch-networkingrail-optimized-topology","title":"[Rail-Optimized](https://infohub.delltechnologies.com/zh-cn/l/generative-ai-in-the-enterprise-with-nvidia-gpus-and-dell-powerswitch-networking/rail-optimized-topology/","text":"<p>In this topology, each NIC is connected to a different switch (or spine-leaf network) and is called a rail (represented by a unique color in the following figure). The rails are also interconnected at an upper tier. Therefore, this topology provides two ways to cross rails: through the Scale Up fabric (preferred) or through the upper tier of the Scale Out topology.</p> <p>For example, to communicate with GPU 8 on server 2, GPU 4 on server 1 can either:</p> <ol> <li>Transfer its data into the memory of GPU 8 on server 1. Then GPU 8 on server 1 communicates through NIC 8 on server 1 with GPU 8 on server 2, through NIC 8 on server 2. This path is shown with purple lines in Figure 14.</li> <li>Send its data to NIC 4 on server 1, which can reach through the upper tier to NIC 8 on server 2, coupled with GPU 8 on server 2.</li> </ol> <p>This property allows AI workloads to perform better on a Rail-Optimized topology than on a Pure Rail topology because the current Collective Communication Libraries are not yet fully optimized for the Pure Rail topology. As such, the Rail-Optimized topology is the recommended topology to build a Scale Out fabric.</p>"},{"location":"notes/sysadmin/nfs/","title":"NFS","text":""},{"location":"notes/sysadmin/nfs/#debug-logs","title":"Debug Logs","text":"Text Only<pre><code>$ rpcdebug -m nfs -s proc\nnfs        proc\n</code></pre> <p>You can view kernel logs (<code>/var/log/messages</code> or <code>/var/log/kern.log</code>) to view logs.</p>"},{"location":"notes/sysadmin/openstack/","title":"OpenStack","text":""},{"location":"notes/sysadmin/openstack/#neutron","title":"Neutron","text":"<p>Neutron is an API gateway that provides a control plane for the underlying SDN solution. It often sits in front of OVN. It is a networking-as-a-service (NaaS) framework.</p> <p>Its primary role is to act as an API layer for tenants to request and manage networking services. OVN itself will implement and enforce the requested networking logic using OVS. Tenants can interact directly with Neutron.</p>"},{"location":"notes/sysadmin/openstack/#ovn","title":"OVN","text":"<p>Open Virtual Network (OVN), is a system to support logical network abstraction in virtual machine and container environments. Sometimes called open source virtual networking for Open vSwitch, OVN complements the existing capabilities of OVS to add native support for logical network abstractions, such as logical L2 and L3 overlays, security groups and services such as DHCP.</p> <p>A physical network comprises physical wires, switches, and routers. A virtual network extends a physical network into a hypervisor or container platform, bridging VMs or containers into the physical network. An OVN logical network is a network implemented in software that is insulated from physical networks by tunnels or other encapsulations. This allows IP and other address spaces used in logical networks to overlap with those used on physical networks without causing conflicts. Logical network topologies can be arranged without regard for the topologies of the physical networks on which they run. Thus, VMs that are part of a logical network can migrate from one physical machine to another without network disruption.</p> <p>OVN acts as the control plane. It orchestrates and manages OVS instances.  It ensures the network topology and policies are consistently applied across the infrastructure.</p>"},{"location":"notes/sysadmin/openstack/#ovn-controller","title":"ovn-controller","text":"<p>The ovn-controller service runs on each Compute node and connects to the OVN southbound (SB) database server to retrieve the logical flows. The ovn-controller translates these logical flows into physical OpenFlow flows and adds the flows to the OVS bridge (br-int). To communicate with ovs-vswitchd and install the OpenFlow flows, the ovn-controller connects to the local ovsdb-server (which hosts conf.db) using the UNIX socket path that was passed when ovn-controller was started (for example unix:/var/run/openvswitch/db.sock).</p>"},{"location":"notes/sysadmin/openstack/#ovs","title":"OVS","text":"<p>Open vSwitch (OVS) is a software-defined networking (SDN) virtual switch similar to the Linux software bridge. OVS provides switching services to virtualized networks with support for industry standard OpenFlow and sFlow. OVS can also integrate with physical switches using layer 2 features, such as STP, LACP, and 802.1Q VLAN tagging. Open vSwitch version 1.11.0-1.el6 or later also supports tunneling with VXLAN and GRE.</p> <p>OVS acts as the data plane. It handles packet forwarding based on flow rules.</p>"},{"location":"notes/sysadmin/openstack/#northbound-database","title":"Northbound Database","text":"<p>The NB database contains high-level configuration and policy information, such as logical network definitions and access policies.</p>"},{"location":"notes/sysadmin/openstack/#southbound-database","title":"Southbound Database","text":"<p>The SB database contains low-level, system-specific configuration and runtime data for individual OVS instances.</p>"},{"location":"notes/sysadmin/osi-model/","title":"OSI Model","text":"<p>The Open Systems Interconnect model is a conceptual framework for understanding the various layers of abstraction involved in networked communication.</p> <p></p>"},{"location":"notes/sysadmin/osi-model/#layer-2-protocols","title":"Layer 2 Protocols","text":"<p>Data Link Layer</p>"},{"location":"notes/sysadmin/osi-model/#lldp","title":"LLDP","text":""},{"location":"notes/sysadmin/osi-model/#arp","title":"ARP","text":"<p>The Address Resolution Protocol allows computers within a switched network to resolve IP addresses to MAC addresses. This is done by simply broadcasting a request on the subnet that asks who owns a particular IP address. A response will arrive that contains the MAC address of the interface that is configured to listen to that address. It is certainly possible for multiple devices to respond to the same IP address, although this typically is not desired for obvious reasons.(1)</p> <ol> <li>In cases like where you configure a VIP (meaning, two devices are configured for the same IP address), only one of the devices would typically be \"active\" and respond to such requests. The VIP active/passive configuration is controlled by device-side applications like VRRP.</li> </ol>"},{"location":"notes/sysadmin/osi-model/#layer-3-protocols","title":"Layer 3 Protocols","text":"<p>Network Layer</p>"},{"location":"notes/sysadmin/osi-model/#icmp","title":"ICMP","text":"<p>Internet Control Message Protocol is used to diagnose issues in a network. The <code>traceroute</code> and <code>ping</code> commands use ICMP.</p>"},{"location":"notes/sysadmin/osi-model/#layer-4-protocols","title":"Layer 4 Protocols","text":"<p>Transport Layer</p>"},{"location":"notes/sysadmin/osi-model/#quic","title":"QUIC","text":"<p>QUIC is a transport-layer protocol that aims to be effectively equivalent to TCP but with much reduced latency. This is achieved primarily through an abbreviated handshake protocol that only requires 1 round trip, whereas TCP requires 3. It can be thought of as a TCP-like protocol with the efficiencies of UDP.</p> <p></p> <p>Congestion control algorithms are handled in userspace instead of kernel space (like TCP) which is claimed to allow the algorithms to rapidly evolve and improve.</p>"},{"location":"notes/sysadmin/osi-model/#tcp","title":"TCP","text":""},{"location":"notes/sysadmin/osi-model/#udp","title":"UDP","text":""},{"location":"notes/sysadmin/osi-model/#layer-7-protocols","title":"Layer 7 Protocols","text":""},{"location":"notes/sysadmin/osi-model/#http","title":"HTTP","text":"<p>Obviously this has to be mentioned. It's the most common protocol of them all! </p>"},{"location":"notes/sysadmin/osi-model/#websocket","title":"Websocket","text":""},{"location":"notes/sysadmin/osi-model/#grpc","title":"gRPC","text":"<p>Google Remote Procedure Call.</p>"},{"location":"notes/sysadmin/osi-model/#video-streaming","title":"Video Streaming","text":"<p>There are various standardized protocols for video streaming.</p>"},{"location":"notes/sysadmin/osi-model/#mpeg-dash","title":"MPEG-DASH","text":"<p>MPEG stands for \"Moving Picture Experts Group.\" DASH stands for \"Dynamic Adaptive Streaming over HTTP\". This protocol is used by YouTube and Netflix.</p>"},{"location":"notes/sysadmin/osi-model/#apple-hls","title":"Apple HLS","text":"<p>HLS stands for \"HTTP Live Streaming\"</p>"},{"location":"notes/sysadmin/osi-model/#microsoft-smooth-streaming","title":"Microsoft Smooth Streaming","text":"<p>Seems to be exclusively used by Microsoft's products</p>"},{"location":"notes/sysadmin/osi-model/#adobe-http-dynamic-streaming-hds","title":"Adobe HTTP Dynamic Streaming (HDS)","text":"<p>Mainly used for flash.</p>"},{"location":"notes/sysadmin/pcie/","title":"PCIe","text":""},{"location":"notes/sysadmin/pcie/#lanes","title":"Lanes","text":"<p>Each lane in a PCIe bus is composed of two differential signal pairs. One pair receives data and the other transmits. Thus, each PCIe lane consists of four signal wires. The use of a differential signal pair where the signal is mirrored 180 degrees on each wire in the pair, as opposed to a single signal wire, is a common noise reduction technique. </p>"},{"location":"notes/sysadmin/pcie/#link-training","title":"Link Training","text":"<p>The physical layer of PCI performs link training, which refers to a handshake that two PCI endpoints perform with each other to negotiate the parameters of the connection. Such parameters include things like link width, speed, and equalization settings.</p> <p>The link training handshake is handled by the Link Training and Status State Machine (LTSSM). This state machine ensures that each partner in the negotiation is progressing normally.</p> ChatGPT 4o <p>The PCIe (Peripheral Component Interconnect Express) link training process is a critical step during the initialization of PCIe connections between two devices. This process ensures that the link is established correctly, operating at the highest possible speed and width that both devices and the physical link can support reliably. Here is a detailed description of the PCIe link training process:</p>"},{"location":"notes/sysadmin/pcie/#overview-of-pcie-link-training","title":"Overview of PCIe Link Training:","text":"<ol> <li>Detection Phase:</li> <li> <p>The process begins with the detection of a connected device. The Root Complex (RC) or upstream device and the Endpoint (EP) or downstream device detect each other's presence through electrical signals on the link.</p> </li> <li> <p>Polling Phase:</p> </li> <li> <p>The devices transition into a polling state to establish communication parameters. They exchange training sets to synchronize and align their clocks. This phase ensures that both devices are ready to proceed with the link training.</p> </li> <li> <p>Configuration Phase:</p> </li> <li>During this phase, the devices determine the capabilities of the link, including the number of lanes and the supported data rates. They start with the lowest data rate (e.g., 2.5 GT/s for PCIe Gen1) and the maximum number of lanes physically available.</li> <li> <p>The devices negotiate and confirm the lane configuration and data rate. If the initial configuration is successful, they may attempt to train at higher speeds.</p> </li> <li> <p>Link Training and Status State Machine (LTSSM):</p> </li> <li> <p>The Link Training and Status State Machine (LTSSM) manages the state transitions during the link training process. The LTSSM defines various states, such as Detect, Polling, Configuration, and L0 (active state), and controls the flow between these states to ensure a stable link.</p> </li> <li> <p>Equalization:</p> </li> <li>Equalization is a critical step to compensate for signal degradation over the physical link. The devices adjust their transmitter and receiver parameters to optimize signal quality and minimize errors. This may involve tuning parameters such as pre-emphasis and de-emphasis.</li> <li> <p>Equalization typically occurs in multiple phases (Phase 1, Phase 2, and Phase 3) to fine-tune the signal characteristics for reliable high-speed data transfer.</p> </li> <li> <p>Training Sets Exchange:</p> </li> <li> <p>The devices exchange training sets, which are special data patterns used to test and adjust the link parameters. These training sets help in calibrating the link and ensuring that data can be transmitted and received correctly.</p> </li> <li> <p>Error Detection and Correction:</p> </li> <li> <p>During the training process, error detection mechanisms (such as Cyclic Redundancy Check or CRC) are used to identify and correct any errors in the communication. If errors are detected, the devices may re-initiate parts of the training process to correct the issues.</p> </li> <li> <p>Link Up (L0 State):</p> </li> <li>Once the link parameters are successfully negotiated and optimized, the devices transition to the L0 state, which is the active state where normal data transfer operations occur. In this state, the link is fully operational, and the devices can communicate at the negotiated speed and lane width.</li> </ol>"},{"location":"notes/sysadmin/pcie/#detailed-ltssm-states","title":"Detailed LTSSM States:","text":"<ol> <li>Detect: The link is powered up, and the presence of a connected device is detected through electrical signals.</li> <li>Polling: The devices exchange training sets to synchronize their clocks and align the link.</li> <li>Configuration: The devices negotiate the link parameters, such as speed and width, and configure the link accordingly.</li> <li>L0 (Active): The link is fully trained and operational, allowing normal data transfer.</li> <li>Recovery: If an error is detected, the link may enter a recovery state to attempt to re-establish a stable connection.</li> <li>L0s and L1 (Low Power States): These states are used for power management when the link is idle.</li> </ol>"},{"location":"notes/sysadmin/pcie/#summary","title":"Summary","text":"<p>The PCIe link training process is essential for ensuring a reliable and high-performance connection between PCIe devices. It involves detecting the connected devices, negotiating link parameters, performing signal equalization, and transitioning through various states managed by the LTSSM to achieve a stable and fully operational link. This process enables PCIe devices to communicate efficiently, taking full advantage of the available bandwidth and minimizing errors.</p>"},{"location":"notes/sysadmin/pcie/#switch","title":"Switch","text":"<p>A PCIe switch shuttles packets on the PCIe bus to different endpoints. It works in much the same way as a networking switch.</p>"},{"location":"notes/sysadmin/pcie/#bridge","title":"Bridge","text":"<p>A PCIe bridge is a device that allows communication across two different PCIe busses.</p>"},{"location":"notes/sysadmin/pcie/#ats","title":"ATS","text":"<p>The ATS, or Address Translation Services, supports translation DMA (Direct Memory Access) addresses to addresses on the PCIe device. It is comprised of many different sub-components that work together to provide fast, low-latency address translation.</p> <p>The ATS relieves the CPU from having to perform these address translations itself. It runs as a set of hardware components that bypass the CPU.</p>"},{"location":"notes/sysadmin/pcie/#translation-agent","title":"Translation Agent","text":"<p>The Translation Agent is a software component running on the host that translates addresses on behalf of the host for PCIe devices. It uses data structures like page tables and Translation Lookaside Buffers (TLB) to perform the virtual-to-physical memory translations.</p>"},{"location":"notes/sysadmin/pcie/#address-translation-cache","title":"Address Translation Cache","text":"<p>The ATC is a cache on the device that stores translations between virtual and physical addresses. This cache stores results from the TA and allows translations to be performed with much lower latency.</p>"},{"location":"notes/sysadmin/pcie/#address-translation-protection-table","title":"Address Translation Protection Table","text":"<p>The ATPT is a data structure used to store page tables for an address translation. It contains the set of address translations accessed by a Translation Agent to process PCIe requests.</p> <p>References:</p> <ul> <li>https://www.intel.com/content/www/us/en/docs/programmable/683686/20-4/address-translation-services-ats.html</li> <li>https://community.cadence.com/cadence_blogs_8/b/fv/posts/navigating-the-complexity-of-address-translation-verification-in-pci-express-6-0</li> </ul>"},{"location":"notes/sysadmin/pcie/#root-complex","title":"Root Complex","text":"<p>A Root Complex device connects the CPU and memory subsystem to the PCIe switch fabric. It generates transaction requests on behalf of the CPU, which is interconnected through a local bus. Root Complex functionality may be integrated in the chipset and/or the CPU.</p> <p>All CPU&lt;-&gt;Memory operations happen inside of the root complex. The CPU may attempt to access something on the PCIe bus, in which case the root complex will forward the request to the PCIe controller, from where the controller will route the request to the proper PCIe endpoint.</p> <p>References:</p> <ul> <li>https://electronics.stackexchange.com/questions/461251/what-is-the-role-of-the-root-complex-in-a-microprocessor-system-pci-express</li> </ul>"},{"location":"notes/sysadmin/pcie/#sysfs","title":"<code>sysfs</code>","text":"<p>You can interact with devices on the PCIe bus using <code>sysfs</code>. Details listed here.</p>"},{"location":"notes/sysadmin/pcie/#removing-device-from-pci","title":"Removing Device from PCI","text":"<p><code>sysfs</code> can be used to remove a specific PCIe device. If a GPU is being used by a VM, for example, we need to first remove the VM with <code>virsh destroy</code>.</p> <p>Then, we perform:</p> Text Only<pre><code>echo 1 &gt; /sys/bus/pci/devices/${address}/remove\necho 1 &gt; /sys/bus/pci/rescan\n</code></pre> <p>According to <code>kernel.org</code>:</p> <p>Quote</p> <p>The 'remove' file is used to remove the PCI device, by writing a non-zero integer to the file. This does not involve any kind of hot-plug functionality, e.g. powering off the device. The device is removed from the kernel's list of PCI devices, the sysfs directory for it is removed, and the device will be removed from any drivers attached to it. Removal of PCI root buses is disallowed.</p>"},{"location":"notes/sysadmin/pcie/#cli-tools","title":"CLI Tools","text":""},{"location":"notes/sysadmin/pcie/#lspci","title":"lspci","text":"Text Only<pre><code>$ lspci\n00:00.0 Host bridge: Advanced Micro Devices, Inc. [AMD] Starship/Matisse Root Complex\n00:00.2 IOMMU: Advanced Micro Devices, Inc. [AMD] Device 164f (rev 01)\n00:01.0 Host bridge: Advanced Micro Devices, Inc. [AMD] Starship/Matisse PCIe Dummy Host Bridge\n</code></pre>"},{"location":"notes/sysadmin/pcie/#filter-by-vendor","title":"Filter by Vendor","text":"<p>PCI-SIG maintains a list of vendor IDs that are used when reporting the vendor to the PCI system.</p> <p>For example, you can query all NVIDIA devices using the vendor ID <code>0x10de</code>:</p> Text Only<pre><code>$ lspci -v -d 10de:\n18:00.0 3D controller: NVIDIA Corporation Device 2330 (rev a1)\n2a:00.0 3D controller: NVIDIA Corporation Device 2330 (rev a1)\n</code></pre>"},{"location":"notes/sysadmin/pcie/#setpci","title":"setpci","text":"Text Only<pre><code>NAME\n       setpci - configure PCI devices\n\nSYNOPSIS\n       setpci [options] devices operations...\n\nDESCRIPTION\n       setpci is a utility for querying and configuring PCI devices.\n\n       All numbers are entered in hexadecimal notation.\n\n       Root privileges are necessary for almost all operations, excluding reads of the standard header of the configuration space on some operating systems.  Please see lspci(8) for details on access rights.\n</code></pre>"},{"location":"notes/sysadmin/repositories/","title":"Repositories","text":""},{"location":"notes/sysadmin/repositories/#apt","title":"apt","text":""},{"location":"notes/sysadmin/repositories/#security","title":"Security","text":"<p>Readings:</p> <ul> <li>https://blog.cloudflare.com/dont-use-apt-key</li> </ul> <p>To configure a GPG public key to be trusted by apt:</p> <ol> <li>Download public key from web</li> <li>Import to GPG     Text Only<pre><code>gpg --import /tmp/key.pub\n</code></pre></li> <li>Export the key in binary format:     Text Only<pre><code>gpg --export $KEY_ID &gt; /etc/apt/trusted.gpg.d/key_name.gpg\n</code></pre></li> </ol>"},{"location":"notes/sysadmin/security/","title":"Security","text":""},{"location":"notes/sysadmin/security/#http-authentication","title":"HTTP Authentication","text":"<p>There are various kinds of authentication mechanisms you may use in HTTP.</p>"},{"location":"notes/sysadmin/security/#basic","title":"Basic","text":"<p>Sends a base64-encoded <code>username:password</code> tuple in the <code>Authorization</code> header.</p>"},{"location":"notes/sysadmin/security/#api-keys","title":"API Keys","text":"<p>This is an API-specific secret that only the client and server know. It's typically used to provide authentication to specific API paths.</p>"},{"location":"notes/sysadmin/security/#bearer","title":"Bearer","text":"<p>This is a type of token that grants access to the \"bearer of this token.\" It's a string, normally generated by the server, that the client must send when making requests to the API.</p>"},{"location":"notes/sysadmin/security/#oauth-20","title":"OAuth 2.0","text":"<p>This is a more complex form of authentication that relies on authentication steps called \"flows\". The OAuth 2.0 server grants a token to a client that allows the client to access a protected resource on behalf of the owner, without having to know the owner's credentials.</p>"},{"location":"notes/sysadmin/security/#flows","title":"Flows","text":"<p>A \"flow\" is a challenge the client must perform in order to get access to an access token. The various types of flows are:</p> <ul> <li>Authorization Code: this is similar to how users use their Facebook or Google account to sign up for a web application.</li> <li>Implicit: requires the client to directly retrieve an access token. Useful in cases where the access token cannot be stored locally (due to being easily accessible by a third party). Suitable for web, desktop, and mobile apps that do not incldue a server component.</li> <li>Resource owner password credentials: Requires a username and password. Only suitable for trusted clients because the credentials are part of the request.</li> <li>Client credentials: Intended for server-to-server authentications. In this approach, the client is acting on its own behalf instead of on behalf of another user. It allows the client to specify their own credentials so they can access resources that they own.</li> </ul>"},{"location":"notes/sysadmin/security/#digest","title":"Digest","text":"<p>Similar to basic authentication, but it sends a hash of the username and password instead of a base64 encoded string (obviously base64 is easily reversible, while hashing is not).</p> <p>Digest is susceptible to man-in-the-middle attacks where the attacker can request Basic authorization, then send a Digest authentication to the upstream server. It also provides no way for the client to verify the identity of the server.</p> <p>Digest authentication allows the server to store the hash of username/passwords in a database instead of cleartext passwords, however if an attacker ever gains access to the database, they can send valid Digest authentications. This means that the password database must be protected as strictly as if it contained cleartext passwords.</p>"},{"location":"notes/sysadmin/security/#aws4-hmac-sha256","title":"AWS4-HMAC-SHA256","text":"Text Only<pre><code>Authorization: AWS4-HMAC-SHA256\nCredential=AKIAIOSFODNN7EXAMPLE/20130524/us-east-1/s3/aws4_request,\nSignedHeaders=host;range;x-amz-date,\nSignature=fe5f80f77d5fa3beca038a248ff027d0445342fe2855ddc963176630326f1024\n</code></pre> <p>In this authorization scheme, the client calculates a signature that is based off of a defined set of headers that the client gets to choose. The server will re-create the signature using the headers specified in the request, and if the signature matches, the request is granted. The signature is thus based off of the private access key (whose name is provided in the <code>Credential</code> header, and whose value is known only by the client and the server, and whose value is specific to an AWS region), the headers specified by the client, and a hash of the request payload (if the <code>s3:x-amz-content-sha256</code> condition key is provided).</p> <p>In its essence, this scheme uses a shared private key to create a digital signature of the request, which allows both the server to verify the identity of the cleint, and it protects against MITM attacks (assuming the private key has not been compromised) from modifying the request. It's still recommended to use in conjunction with HTTPS.</p> <p>The signed portions of the request are valid within 15 minutes of the original timestmap. This means that an attacker can re-use a signed request within 15 minutes and modify the unsigned portions. This is why Amazon recommends you always hash the contents, and as many headers as is practical.</p>"},{"location":"notes/sysadmin/security/#sssd","title":"SSSD","text":"<p>System Security Services Daemon provides a set of authentication and authorization services on Linux hosts, primarily used to authenticate/authorize user login to external services (such as LDAP), but can be used for other use-cases.</p>"},{"location":"notes/sysadmin/software-defined-networking/","title":"Software Defined Networking","text":""},{"location":"notes/sysadmin/software-defined-networking/#ndfc","title":"NDFC","text":"<p>Cisco's Nexus Dashboard Fabric Controller. Don't use it, it's closed source, has little in the way of tooling and monitoring, and is very mysterious.</p>"},{"location":"notes/sysadmin/software-defined-networking/#ovn","title":"OVN","text":"<p>Open Virtual Networking is an abstraction layer built on top of OVS. It provides logical network components such as virtual networks, routers, and security groups. OVN simplifies the definition and management of network connectivity and policies within virtualized environments. By leveraging switching capabilities of OVS, OVN implements overlay networking to enable advanced features like routing, access control, and distributed load balancing.</p> <p>OVN is what's called a Cloud Management System (CMS).</p>"},{"location":"notes/sysadmin/software-defined-networking/#ovs","title":"OVS","text":"<p>Open vSwitch provides a virtual switch implementation. It's a software-based switch that allows for the automatic creation and management of virtual ports, bridges, and tunnels.</p>"},{"location":"notes/sysadmin/software-defined-networking/#useful-links","title":"Useful Links","text":"<ul> <li>https://blog.vsq.cz/blog/2023-07-11-ovs-datapath-overview/</li> <li>https://fairbanks.nl/migrating-to-ovn/</li> </ul>"},{"location":"notes/sysadmin/storage/","title":"Storage","text":""},{"location":"notes/sysadmin/storage/#ssd","title":"SSD","text":"<p>Solid State Drives</p>"},{"location":"notes/sysadmin/storage/#sata","title":"SATA","text":"<p>A SATA (Serial AT Attachment) is a computer bus interface that many of the early SSD disks used. This is commonly used in desktop hardware, although it's starting to be phased out for NVMe.</p> <p>Typical read speeds are 550MB/s</p>"},{"location":"notes/sysadmin/storage/#nvme","title":"NVMe","text":"<p>Non-Volatile Memory Express is a more modern SSD that has much higher speeds than SATA. It typically comes in the M.2 form factor, but also come in U.2 and PCIe cards.</p> <p>Typical read speeds are up to 3,500 MB/s for PCIe Gen 3, 7,500MB/s for PCIe Gen 4.</p>"},{"location":"notes/sysadmin/storage/#m2","title":"M.2","text":"<p>M.2 is the most common form factor for consumer NVMe.</p>"},{"location":"notes/sysadmin/storage/#u2","title":"U.2","text":"<p>U.2 is a form factor more common in datacenter applications. It's mechanically identical to SATA but provides four PCIe lanes. U.2 can use 3.3 V, 5 V and 12 V while M.2 can only 3.3 V.</p>"},{"location":"notes/sysadmin/storage/#hdd","title":"HDD","text":"<p>Hard Disk Drives are drives which use magnetic spinning disks to store data. They are used for data which does not require high throughput, as disk head seek times can be quite high.</p> <p>Typical read speeds are between 80MB/s and 160MB/s.</p>"},{"location":"notes/sysadmin/storage/#raid","title":"RAID","text":"<p>RAID, or Redundant Array of Inexpensive Disks, is a data storage virutalization technology used for exposing multiple independent disks as a single hard drive to the operating system. Most Dell servers come with hardware RAID support natively, but you can also utilize a software-based implementation. RAID arrays offer data redundancy and increased throughput, depending on the RAID level.</p> Level Meaning Minimum Number of Disks Diagram RAID 0 Block-level striping, but no mirroring or parity. The contents of a single file can be distributed amongst multiple drives. The failure of any one disk can cause corruption in many files. The total capacity is the sum of all the drives in the array. 2 RAID 1 Provides data mirroring, without parity or striping. Data is written identically to two or more drives. Sustained read throughput approaches the sustained throughput of all the drives combined. Write throughput is usually slower than RAID 0 because data has to be duplicated (the slowest drive limits performance). 2 RAID 2 Provides bit-level striping with dedicated Hamming-code parity. Each sequential bit is on a different drive. Hamming-code parity is calculated using the parity code stored on at least one drive. This level is usually not used by any commercial system. 3 RAID 3 Provides byte-level striping with dedicated parity. Each sequential byte is stored on different drives. RAID 3 is not commonly used. 3 RAID 4 Provides block-level striping with dedicated parity. A single drive is dedicated to parity. 3 RAID 5 Provides striping and double parity. Parity information is distributed across many disks, which makes it faster than RAID 4. 3 RAID 6 Extends RAID 5 by creating two copies of the parity code, instead of just one. 4"},{"location":"notes/sysadmin/timekeeping/","title":"Timekeeping","text":""},{"location":"notes/sysadmin/timekeeping/#systemd-timesyncdservice","title":"<code>systemd-timesyncd.service</code>","text":"<p>Systemd has introduced their own SNTP implementation called timesyncd. Timesyncd only implements a subset of the NTP protocol, which is why it's called SNTP. It has been used since Debian \"bookworm\", and by extension, Ubuntu (which is downstream of Debian).</p>"},{"location":"notes/sysadmin/timekeeping/#timedatectl","title":"<code>timedatectl</code>","text":"<p>This is a command that can be used to view the current status of time synchronization.</p> Text Only<pre><code>$ timedatectl timesync-status\n       Server: 91.189.91.157 (ntp.ubuntu.com)\nPoll interval: 34min 8s (min: 32s; max 34min 8s)\n         Leap: normal\n      Version: 4\n      Stratum: 2\n    Reference: 84A36001\n    Precision: 1us (-24)\nRoot distance: 24.421ms (max: 5s)\n       Offset: -15.315ms\n        Delay: 48.943ms\n       Jitter: 10.320ms\n Packet count: 9\n    Frequency: -23.604ppm\n</code></pre>"},{"location":"notes/sysadmin/timekeeping/#chrony","title":"chrony","text":"<p>chrony implements the full NTP protocol and is often a replacement for timesyncd. A comparison of chrony against other implementations can be found here.</p>"},{"location":"notes/sysadmin/timekeeping/#ntp","title":"ntp","text":"<p>Network Time Protocol gives millisecond, and sometimes sub-millisecond, level of precision. It's based off of software timestamping which contributes to the reduced precision as compared to PTP. This is the standard implementation that is used when millisecond-level precision is acceptable.</p>"},{"location":"notes/sysadmin/timekeeping/#ptp","title":"ptp","text":"<p>Precision Time Protocol is a much more accurate form of network timekeeping, capable of reaching nanosecond and sometimes sub-nanosecond precision. It is typically driven off of GPS signals into a datacenter and replicated through a series of boundary clocks. By default it uses multicast routing. Managing PTP is a more complex protocol. It can be used with either software or hardware timestamping.</p> <p>The PTP protocol uses a multi-step, two-way message exchange that allows clocks to account for network latency.</p>"},{"location":"notes/sysadmin/timekeeping/#openntpd","title":"OpenNTPD","text":"<p>https://www.openntpd.org/</p>"},{"location":"notes/sysadmin/virtualization/","title":"Virtualization","text":""},{"location":"notes/sysadmin/virtualization/#hypervisor","title":"Hypervisor","text":"<p>There are two types of hypervisors, type 1 and type 2. Type 1 hypervisors are services that have direct access to hardware. They do not rely on the host OS to execute instructions.</p> <p>Type 2 hypervisors sit on top of the host OS which introduces a massive latency penalty.</p>"},{"location":"notes/sysadmin/virtualization/#kvm","title":"KVM","text":"<p>The Linux kernel provides a technology called the Kernel-based Virtual Machine. KVM makes use of the HVM (hardware virtual machine) extensions to give VMs direct access to host hardware. While KVM can be used by itself to launch virtual machines, it is not uncommon to see QEMU used in conjunction with KVM.</p>"},{"location":"notes/sysadmin/virtualization/#qemu","title":"QEMU","text":"<p>The Quick Emulator is a full hardware emulator that allows you to run a VM for any kind of architecture, which is its main advantage over KVM. However, QEMU is much slower as instructions are typically not run natively on the host hardware but are rather dynamically translated. QEMU is also often more difficult to configure than KVM.</p> <p>QEMU can work directly with KVM to allow acceleration on host platforms that support it. Lack of KVM support means QEMU must fully emulate the hardware which makes it much slower.</p>"},{"location":"notes/sysadmin/virtualization/#socket-types","title":"Socket Types","text":""},{"location":"notes/sysadmin/virtualization/#af_vsock","title":"AF_VSOCK","text":"<p>AF_VSOCK is a bidirectional socket used for secure communication between a hypervisor and a VM. QEMU can be configured to create this channel through the domain XML: https://libvirt.org/formatdomain.html#vsock</p> <p>Tip</p> <p>If the <code>auto</code> option is used, you can determine what CID libvirt gave to the socket by using <code>virsh dumpxml [domain]</code>. It will show you the assigned CID (called an <code>address</code>) in the domain XML. You can then use this address to open the communication channel as shown below.</p> <p>An example of a hypervisor and VM communicating:</p> Text Only<pre><code>root@hypervisor:/home/landon# socat - VSOCK-CONNECT:3:1234\nhost: hello\nvm: well hi der\n</code></pre> Text Only<pre><code>root@virtual-machine:/home/landon# socat VSOCK-LISTEN:1234,fork -\nhost: hello\nvm: well hi der\n</code></pre>"},{"location":"notes/sysadmin/virtualization/#firecracker","title":"Firecracker","text":"<p>Firecracker is an alternative to QEMU that is built specifically for microVMs. It was created by AWS for the Lambda service.</p>"},{"location":"notes/sysadmin/virtualization/#sr-iov","title":"SR-IOV","text":"<p>SR-IOV, or Single-Root I/O Virtualization, is a PCI-SIG standard that allows the isolation of PCIe resources for performance and manageability reasons. SR-IOV allows a device, like a network adapter, to separate access to its resources among various PCIe hardware functions. This is achieved through the introduction of two PCIe functions: Physical Functions (PFs) and Virtual Functions (VFs).</p> <p>SR-IOV enables a Single Root Function (such as an ethernet port) to appear as multiple, separate physical devices. A physical device with SR-IOV capabilities can be configured to appear in the PCI configuration space as multiple functions.</p> <p>Resources:</p> <ul> <li>https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/virtualization_deployment_and_administration_guide/sect-pci_devices-pci_passthrough</li> </ul>"},{"location":"notes/sysadmin/virtualization/#physical-and-virtual-functions","title":"Physical and Virtual Functions","text":"<p>SR-IOV uses physical and virtual functions to control or configure PCIe devices. Physical functions have the ability to move data in and out of the device while virtual functions are lightweight PCIe functions that support data flowing but also have a restricted set of configuration resources. The virtual or physical functions available to the hypervisor or guest operating system depend on the PCIe device.</p> <p>Physical Functions (PFs) are full PCIe devices that include the SR-IOV capabilities. Physical Functions are discovered, managed, and configured as normal PCI devices. Physical Functions configure and manage the SR-IOV functionality by assigning Virtual Functions.</p> <p>Virtual Functions (VFs) are simple PCIe functions that only process I/O. Each Virtual Function is derived from a Physical Function. The number of Virtual Functions a device may have is limited by the device hardware. A single Ethernet port, the Physical Device, may map to many Virtual Functions that can be shared to virtual machines.</p>"},{"location":"notes/sysadmin/virtualization/#configuration","title":"Configuration","text":"<p>SR-IOV is enabled through the system BIOS.</p> <p>https://docs.nvidia.com/networking/display/mlnxofedv582030lts/single+root+io+virtualization+(sr-iov)#src-2396585884_safe-id-U2luZ2xlUm9vdElPVmlydHVhbGl6YXRpb24oU1JJT1YpLXNldHRpbmd1cHNyLWlvdg</p>"},{"location":"notes/sysadmin/linux/cli-tools/","title":"CLI Tools","text":""},{"location":"notes/sysadmin/linux/cli-tools/#pdsh","title":"pdsh","text":"<p>Used to run commands to multiple nodes at once. By default uses <code>/etc/genders</code> to determine the hosts to run the commands on.</p> Text Only<pre><code>$ pdsh -g gpu \"echo hello\"\nnode-017: hello\nnode-018: hello\nnode-020: hello\n</code></pre>"},{"location":"notes/sysadmin/linux/cli-tools/#dshbak","title":"dshbak","text":"<p>Often used in conjunction with pdsh to aggregate command output.</p> Text Only<pre><code>$ pdsh -g gpu \"echo hello\" | dshbak -c\n----------------\nnode-[001-032]\n----------------\nhello\n</code></pre>"},{"location":"notes/sysadmin/linux/cli-tools/#strace","title":"strace","text":"<p>strace can be used to inspect the system calls being made by an application.</p> Text Only<pre><code>$ strace -f cat /tmp/hello.txt |&amp; egrep 'hello'\nexecve(\"/bin/cat\", [\"cat\", \"/tmp/hello.txt\"], 0x7fffffffc1a0 /* 115 vars */) = 0\nopenat(AT_FDCWD, \"/tmp/hello.txt\", O_RDONLY) = 3\nread(3, \"hello world\\n\", 131072)        = 12\nwrite(1, \"hello world\\n\", 12hello world\n</code></pre>"},{"location":"notes/sysadmin/linux/cli-tools/#view-listening-ports","title":"View listening ports","text":"netstat (linux)netstat (MacOS)lsof Bash<pre><code>$ netstat -ntulp\n</code></pre> Bash<pre><code>$ netstat -anv\nActive Internet connections (including servers)\nProto Recv-Q Send-Q  Local Address          Foreign Address        (state)      rhiwat  shiwat    pid   epid state  options           gencnt    flags   flags1 usscnt rtncnt fltrs\ntcp4       0      0  127.0.0.1.8001         127.0.0.1.60348        ESTABLISHED  407878  146988  34323      0 00102 00000004 000000000027587a 00000080 01000900      1      0 000001\ntcp4       0      0  127.0.0.1.60348        127.0.0.1.8001         ESTABLISHED  408300  146988  24807  24804 00102 00020000 0000000000275879 10180081 00080900      1      0 000001\ntcp4       0      0  192.168.50.89.60347    140.82.113.5.443       ESTABLISHED  131072  132432  24807  24804 00102 00020000 0000000000275878 10180081 00080900      1      0 000001\ntcp4       0      0  192.168.50.89.60344    149.137.136.16.443     ESTABLISHED 1511016  131768  24807  24804 00102 00020000 000000000027585b 10180081 00080900      1      0 000001\n</code></pre> Text Only<pre><code>$ sudo lsof -i -P -n | grep LISTEN\nsystemd         1             root   79u  IPv4  116762      0t0  TCP *:111 (LISTEN)\nrpcbind      2942             _rpc    4u  IPv4  116762      0t0  TCP *:111 (LISTEN)\nsystemd-r    2947  systemd-resolve   13u  IPv4  142435      0t0  TCP 127.0.0.53:53 (LISTEN)\nsshd         3175             root    3u  IPv4  273290      0t0  TCP *:22 (LISTEN)\nrpc.statd    3329            statd    9u  IPv4   18403      0t0  TCP *:35287 (LISTEN)\nvirt-expo  135395             root    9u  IPv4  334360      0t0  TCP *:9411 (LISTEN)\nnode_expo  178456         node-exp    3u  IPv4  593613      0t0  TCP *:9100 (LISTEN)\nsystemd_e  178515 systemd-exporter    3u  IPv4  624258      0t0  TCP *:9558 (LISTEN)\n</code></pre>"},{"location":"notes/sysadmin/linux/cli-tools/#uniq","title":"uniq","text":"<p>Use <code>uniq -c</code> to get counts of each occurence. This is more commonly used like <code>cat file.txt | sort | uniq -c</code>.</p>"},{"location":"notes/sysadmin/linux/cli-tools/#sort","title":"sort","text":"<p>Can also use sort with <code>-u</code>, which is equivalent to <code>sort | uniq</code>.</p>"},{"location":"notes/sysadmin/linux/cli-tools/#describe-a-file-handle-using-lsof","title":"Describe a file handle using lsof","text":"<p>This is from a useful gist here: https://gist.github.com/tonyc/1384523?permalink_comment_id=3334070</p> Text Only<pre><code>root@ml-512-node-031:/home/ubuntu# lsof -p 51953 -ad 100\nCOMMAND   PID   USER   FD   TYPE     DEVICE SIZE/OFF NODE NAME\npython  51953 ubuntu  100u  IPv4 2717208220      0t0  TCP ml-512-node-031:33914-&gt;ml-512-node-031:58209 (ESTABLISHED)\n</code></pre>"},{"location":"notes/sysadmin/linux/cli-tools/#tmux","title":"tmux","text":""},{"location":"notes/sysadmin/linux/cli-tools/#layouts","title":"Layouts","text":"<p>Cycle through layouts: PREFIX+B</p> <p>Layouts:</p> <ul> <li>even-horizontal - All panes are arranged side by side, with equal width</li> <li>even-vertical - All panes are stacked on top of each other, with equal height</li> <li>main-horizontal - One large pane on top, with smaller panes arranged horizontally below it</li> <li>main-vertical - One large pane on the left, with smaller panes arranged vertically to the right</li> <li>tiled - All panes are arranged to use the available space as efficiently as possible, with roughly equal size</li> </ul> <p>From the prompt (PREFIX+:):</p> <p><code>select-layout even-vertical</code> or PREFIX+Alt+2</p> <p>and</p> <p><code>select-layout even-horizontal</code> or PREFIX+Alt+1</p>"},{"location":"notes/sysadmin/linux/hugepages/","title":"Hugepages","text":"<p>Huge pages are a memory optimization technique whereby you grant your application memory space that uses larger memory page allocation sizes. The typical page size is 4096 bytes, but by enabling hugepages, you can get much larger page sizes. This improves performance in workloads that use large blocks of memory because there will be fewer requests sent to the page cache.</p>"},{"location":"notes/sysadmin/linux/hugepages/#allocating-huge-pages","title":"Allocating Huge Pages","text":"<p>The following examples are stolen from this blog post.</p>"},{"location":"notes/sysadmin/linux/hugepages/#using-mmap","title":"Using <code>mmap</code>","text":"C<pre><code>void *ptr = mmap(NULL, 8 * (1 &lt;&lt; 21), PROT_READ | PROT_WRITE,\n                 MAP_PRIVATE | MAP_ANONYMOUS | MAP_HUGETLB,\n                 -1, 0))\n</code></pre> <p>You can also link these mappings to a named file descriptor on the <code>hugetlbfs</code> filesystem. Hugepages are drawn from a pool of allocated pages. The size of this pool can be modified.</p>"},{"location":"notes/sysadmin/linux/hugepages/#kernel-command-line-parameter","title":"Kernel command-line parameter","text":"<p>https://www.kernel.org/doc/html/latest/admin-guide/mm/hugetlbpage.html</p> <p>The <code>hugepages</code> parameter can be provided to the kernel to reserve a pool of huge pages. This can also be allocated at runtime using the <code>procfs</code> or <code>sysfs</code> interface.</p> <p>For example:</p> Text Only<pre><code>root@primary:/home/ubuntu# echo 20 &gt; /proc/sys/vm/nr_hugepages\nroot@primary:/home/ubuntu# cat /proc/sys/vm/nr_hugepages\n20\n</code></pre> <p>Note</p> <p>Specifying the kernel command-line parameter is the more reliable method of allocating hugepage pools, as memory has not yet become fragmented. It's possible hugepage allocation can fail at runtime due to fragmentation.</p> <p>You can also specify more exact sizes:</p> Text Only<pre><code>root@primary:/home/ubuntu# cat /sys/kernel/mm/hugepages/hugepages-2048kB/free_hugepages\n20\nroot@primary:/home/ubuntu# ls /sys/kernel/mm/hugepages/\nhugepages-1048576kB  hugepages-2048kB  hugepages-32768kB  hugepages-64kB\n</code></pre>"},{"location":"notes/sysadmin/linux/hugepages/#transparent-huge-pages","title":"Transparent Huge Pages","text":"<p>Enabling THP allows the kernel to automatically promote regular pages into huge pages.</p> <p>References:</p> <ul> <li>https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/monitoring_and_managing_system_status_and_performance/configuring-huge-pages_monitoring-and-managing-system-status-and-performance</li> <li>https://rigtorp.se/hugepages/</li> </ul>"},{"location":"notes/sysadmin/linux/misc/","title":"Linux","text":""},{"location":"notes/sysadmin/linux/misc/#filesystems","title":"Filesystems","text":""},{"location":"notes/sysadmin/linux/misc/#fuse","title":"FUSE","text":"<p>FUSE stands for Filesystem in Userspace. It is a software interface that allows non-privileged applications to provide their own filesystem and mount it within the Linux file namespace. The FUSE module (which is a kernel module) provides a software bridge to the kernel interfaces.</p>"},{"location":"notes/sysadmin/linux/misc/#nfs","title":"NFS","text":"<p>placeholder</p>"},{"location":"notes/sysadmin/linux/misc/#vfs","title":"VFS","text":"<p>VFS, or Virtual File System, is a component of the Linux kernel that provides the filesystem interface to userspace programs. The VFS is what implements open, stat, chmod, and other similar filesystem-related system calls. The pathnames passed to these calls is used by the VFS to lookup the directory entry cache, aka dentry cache or dcache). This allows very fast lookups of dentries without needing to reference the backing filesystem.</p>"},{"location":"notes/sysadmin/linux/misc/#procfs","title":"procfs","text":"<p>https://en.wikipedia.org/wiki/Procfs https://docs.kernel.org/filesystems/proc.html</p> <p>procfs is a special filesystem maintained by the linux kernel that allows you to inspect the state of running processes.</p> Text Only<pre><code>$ ls -lah /proc/21/\ntotal 0\ndr-xr-xr-x   9 root root 0 Oct 28 17:16 .\ndr-xr-xr-x 238 root root 0 Oct 28 17:16 ..\ndr-xr-xr-x   2 root root 0 Dec  8 15:43 attr\n-rw-r--r--   1 root root 0 Dec  8 15:43 autogroup\n-r--------   1 root root 0 Dec  8 15:43 auxv\n-r--r--r--   1 root root 0 Dec  8 15:43 cgroup\n--w-------   1 root root 0 Dec  8 15:43 clear_refs\n</code></pre> <p>There are a lot of useful bits here. For example, you can inspect all open file descriptors for a process. In fact, this is what <code>lsof</code> uses to show open files:</p> Text Only<pre><code>$ ls -l /proc/79808/fd\nlr-x------ 1 ltclipp ltclipp 64 Dec  8 15:44 0 -&gt; /dev/null\nlrwx------ 1 ltclipp ltclipp 64 Dec  8 15:44 1 -&gt; 'socket:[126462260]'\nlrwx------ 1 ltclipp ltclipp 64 Dec  8 15:44 10 -&gt; /tmp/foo.txt\n</code></pre> <p>You can view your own kernel info:</p> Text Only<pre><code>$ ls -l /proc/self/\ndr-xr-xr-x   2 ltclipp ltclipp   0 Dec  8 15:47 attr\n-rw-r--r--   1 ltclipp ltclipp   0 Dec  8 15:47 autogroup\n-r--------   1 ltclipp ltclipp   0 Dec  8 15:47 auxv\n-r--r--r--   1 ltclipp ltclipp   0 Dec  8 15:47 cgroup\n--w-------   1 ltclipp ltclipp   0 Dec  8 15:47 clear_refs\n-r--r--r--   1 ltclipp ltclipp   0 Dec  8 15:47 cmdline\n-rw-r--r--   1 ltclipp ltclipp   0 Dec  8 15:47 comm\n-rw-r--r--   1 ltclipp ltclipp   0 Dec  8 15:47 coredump_filter\n</code></pre>"},{"location":"notes/sysadmin/linux/misc/#sysfs","title":"sysfs","text":"<p>sysfs is a kernel-maintained filesystem for interacting with various kernel subsystems, hardware devices, and device drivers.</p>"},{"location":"notes/sysadmin/linux/misc/#pcie-devices","title":"PCIe Devices","text":"<p>You can use sysfs to read the information about PCIe devices.</p> Text Only<pre><code># ls -lah /sys/bus/pci/devices/0000:db:00.0/\ntotal 0\ndrwxr-xr-x 4 root root    0 Jul 17  2023 .\ndrwxr-xr-x 6 root root    0 Jul 17  2023 ..\n-r--r--r-- 1 root root 4.0K May 10 18:03 aer_dev_correctable\n-r--r--r-- 1 root root 4.0K May 10 18:03 aer_dev_fatal\n-r--r--r-- 1 root root 4.0K May 10 18:03 aer_dev_nonfatal\n-r--r--r-- 1 root root 4.0K May 10 18:03 ari_enabled\n-rw-r--r-- 1 root root 4.0K May 10 18:03 broken_parity_status\n-r--r--r-- 1 root root 4.0K Jul 17  2023 class\n-rw-r--r-- 1 root root 4.0K Jul 17  2023 config\n-r--r--r-- 1 root root 4.0K May 10 18:03 consistent_dma_mask_bits\n-r--r--r-- 1 root root 4.0K May 10 18:03 current_link_speed\n-r--r--r-- 1 root root 4.0K May 10 18:03 current_link_width\n-rw-r--r-- 1 root root 4.0K May 10 18:03 d3cold_allowed\n</code></pre> <p>The device ID can be discovered by using <code>lspci</code>.</p>"},{"location":"notes/sysadmin/linux/misc/#resource-files","title":"<code>resource</code> files","text":"<p>The file located at <code>/sys/bus/pci/devices/*/resource</code> provides ASCII text that describes the host addresses of PCI resources for that device. For each region, there is a corresponding <code>/sys/bus/pci/devices/*/resource*</code> file that contains the contents of that region. You must memory-map to this file in order to access it.</p> <p>For example, using <code>lspci</code> we can introspect the regions:</p> Text Only<pre><code># lspci -n -s 0000:01:00.1 -vv\n0000:01:00.1 0200: 8086:10c9 (rev 01)\n        Subsystem: 10a9:8028\n        Control: I/O+ Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx+\n        Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast &gt;TAbort- &lt;TAbort- SERR- &lt;PERR- INTx-\n        Latency: 0, Cache Line Size: 64 bytes\n        Interrupt: pin B routed to IRQ 40\n        Region 0: Memory at b2140000 (32-bit, non-prefetchable) [size=128K]\n        Region 1: Memory at b2120000 (32-bit, non-prefetchable) [size=128K]\n        Region 2: I/O ports at 2000 [size=32]\n        Region 3: Memory at b2240000 (32-bit, non-prefetchable) [size=16K]\n</code></pre> <p>These regions can be device memory, IO ports, or other resources. The exact contents of the memory is going to be specific to the device in question.</p> <p>Readings:</p> <ul> <li>https://www.kernel.org/doc/Documentation/filesystems/sysfs-pci.txt</li> <li>https://techpubs.jurassic.nl/manuals/linux/developer/REACTLINUX_PG/sgi_html/ch07.html</li> </ul>"},{"location":"notes/sysadmin/linux/misc/#kernel-modules","title":"Kernel Modules","text":"<p>https://wiki.archlinux.org/title/Kernel_module#:~:text=Kernel%20modules%20are%20pieces%20of,as%20built%2Din%20or%20loadable.</p> <p>The linux kernel allows you to hot-load pieces of code, usually drivers (or really anything that needs to run in kernel space).</p> <p>You can view the list of available modules:</p> Text Only<pre><code> $ kmod list\nModule                  Size  Used by\nnfsv3                  57344  0\nnfs_acl                16384  1 nfsv3\nnfs                   389120  1 nfsv3\nlockd                 122880  2 nfsv3,nfs\ngrace                  16384  1 lockd\n</code></pre>"},{"location":"notes/sysadmin/linux/misc/#drac","title":"DRAC","text":"<p>https://en.wikipedia.org/wiki/Dell_DRAC</p> <p>A DRAC (Dell Remote Access Controller) is a hardware unit within a server chassis that is capable of monitoring, deploying, and interacting with the main server hardware and host outside of the typical kernel. It's often integrated into the motherboard itself, and acts as a standalone computer that you can log into and issue commands to.</p> <p>The main benefit of a DRAC is being able to independently execute commands to the host kernel (either through a console or through power cycling commands via hardware), monitoring the health of hardware components, configuring hardware, BIOS, host OS, and various other facets.</p>"},{"location":"notes/sysadmin/linux/misc/#kernel-parameters","title":"Kernel Parameters","text":""},{"location":"notes/sysadmin/linux/misc/#nohz_full","title":"nohz_full","text":"<p>https://www.kernel.org/doc/Documentation/timers/NO_HZ.txt</p> <p>An \"adaptive-tick\" CPU is one where the kernel can temporarily disable the scheduling clock ticks if there is only one runnable task on the core. This is useful in realtime or latency-sensitive applications that need to not be interrupted for scheduling work. The <code>nohz_full</code> kernel boot parameter specifies which cores should be the adaptive-tick cores.</p> Text Only<pre><code>nohz_full=4-7\n</code></pre> <p>Cores which are not currently configured (by the kernel's runtime logic) to receive scheduling interrupts are considered to be \"dyntick-idle\":</p> <p>Quote</p> <p>An idle CPU that is not receiving scheduling-clock interrupts is said to be \"dyntick-idle\", \"in dyntick-idle mode\", \"in nohz mode\", or \"running tickless\".  The remainder of this document will use \"dyntick-idle mode\".</p>"},{"location":"notes/sysadmin/linux/misc/#security","title":"Security","text":""},{"location":"notes/sysadmin/linux/misc/#selinux","title":"SELinux","text":"<p>Security-Enhanced Linux is a linux security module that provides the ability to implement access control policies. In Linux, the default access control mechanisms are done through what's called Discretionary Access Controls (DAC). The granularity of a DAC is only based off of user, group, and \"other\" permissions, and are applied to specific files.</p> <p>SELinux implements Mandatory Access Control (MAC). System resources have what's called an SELinux context. The context, otherwise known as an SELinux Label, abstracts away the underlying resources and instead focuses on only the security properties of the underlying object.</p>"},{"location":"notes/sysadmin/linux/misc/#sudoers","title":"sudoers","text":"<p>The <code>/etc/sudoers</code> file is a file on Linux systems that describes various actions that users are allowed to take as the root user. The man page describes in depth the details of this file:</p> Text Only<pre><code>SUDOERS(5)                                                                                                      File Formats Manual                                                                                                     SUDOERS(5)\n\nNAME\n       sudoers - default sudo security policy plugin\n\nDESCRIPTION\n       The sudoers policy plugin determines a user's sudo privileges.  It is the default sudo policy plugin.  The policy is driven by the /private/etc/sudoers file or, optionally, in LDAP.  The policy format is described in detail in the\n       SUDOERS FILE FORMAT section.  For information on storing sudoers policy information in LDAP, see sudoers.ldap(5).\n</code></pre>"},{"location":"notes/sysadmin/linux/misc/#acl","title":"ACL","text":"<p>An ACL, or Access Control List, commonly refers to extra access policies that are applied to specific files or directories. You can view ACLs in most POSIX-compatible filesystems using the <code>getfacl</code> command:</p> Text Only<pre><code>ubuntu@primary:/tmp$ echo hello &gt; test.txt\nubuntu@primary:/tmp$ getfacl test.txt\n# file: test.txt\n# owner: ubuntu\n# group: ubuntu\nuser::rw-\ngroup::rw-\nother::r--\n</code></pre> <p>You can apply ACLs using the <code>setfacl</code> command (some additional examples can be found here):</p> Text Only<pre><code>ubuntu@primary:/tmp$ setfacl -m user:ubuntu:rw test.txt\nubuntu@primary:/tmp$ getfacl test.txt\n# file: test.txt\n# owner: ubuntu\n# group: ubuntu\nuser::rw-\nuser:ubuntu:rw-\ngroup::rw-\nmask::rw-\nother::r--\n</code></pre> <p>As you can see, this allows you finer grained control over access to your filesystem objects.</p>"},{"location":"notes/sysadmin/linux/misc/#kickstart","title":"Kickstart","text":"<p>Kickstart is an installation mechanism provided by Redhat that allows you to install and configure operating systems in an automated fashion. Cobbler is used to automate the kickstart configuration process.</p>"},{"location":"notes/sysadmin/linux/misc/#posix-signals","title":"POSIX Signals","text":"number name default action Shortcut description 1 SIGHUP Terminate Hang up controlling terminal or process. Often used by many systems to mean \"please re-read and reload config.\" 2 SIGINT Terminate Ctrl+C Interrupt from keyboard. 3 SIGQUIT Core Ctrl+\\ Quit from keyboard. Similar to SIGINT, but it dumps a core in addition to terminating the program. 4 SIGILL Core Illegal instruction 6 SIGABRT Core Abort signal from abort(3) 8 SIGFPE Core Floating point exception. 9 SIGKILL Terminate Forcefully terminate a program. This signal is not catchable. 15 SIGTERM Terminate Gracefully terminate a program. This is similar in behavior to SIGINT, but it cannot be sent from the keyboard. Parent processes will typically send this signal to its children upon termination. 19, 18, 25 SIGCONT Continue Continue execution of a process that was stopped by SIGSTOP. You can also use the <code>bg</code> bash command to continue the process in the background. See Backgrounding a Terminal Process for more details. 17, 19, 23 SIGSTOP Stop Ctrl+Z Stop execution of a process, but allow it to be resumed through SIGCONT."},{"location":"notes/sysadmin/linux/misc/#kernel-bypass","title":"Kernel Bypass","text":"<p>Kernel Bypass is a technology implemented in Linux (and often other kernels as well) that allows network processing to happen in userspace. This often leads to a huge performance improvement for network-bound applications as the traffic does not have to pass through the kernel-userspace boundary.</p> <p>These are some kernel bypass techniques:</p>"},{"location":"notes/sysadmin/linux/misc/#packet_mmap","title":"PACKET_MMAP","text":"<p>Allows the kernel to allocate a circular buffer in userspace so that applications can read their memory directly, instead of making one system call per packet.</p>"},{"location":"notes/sysadmin/linux/misc/#pf_ring","title":"PF_RING","text":"<p>This is a type of network socket, originally implemented by Napatech ntop cards, that provides a circular ring buffer of the network traffic. This is a kernel module that you must load. The kernel module polls packets from the NIC through Linux NAPI and copies the packets from the NIC to the ring buffer, which lives in kernel space. The user application mmaps itself to this kernel buffer. PF_RING is capable of delivering packets to multiple ring buffers, which allows each application to be isolated from others.</p>"},{"location":"notes/sysadmin/linux/misc/#snabbswitch","title":"snabbswitch","text":"<p>This is a networking framework for Lua applications that allows the app to completely control a network card. The user application acts as a hardware driver. This is done on the PCI device level by mmapping the device registers with sysfs.</p>"},{"location":"notes/sysadmin/linux/misc/#dpdk","title":"DPDK","text":"<p>A networking framework written in C that is similar to snabbswitch. It also relies on User IO (UIO).</p>"},{"location":"notes/sysadmin/linux/misc/#niceness","title":"niceness","text":"<p>\"niceness\" is a parameter given to processes that determine their overall runtime priority.</p>"},{"location":"notes/sysadmin/linux/misc/#ionice","title":"ionice","text":"<p>`ionice`` determines the IO scheduling priority and class. The various classes that can be used:</p> <ul> <li>Idle: the program with idle IO priority will only get disk time if nothing else is using the disk.</li> <li>Best effort: This is the default scheduling class. Programs running best-effort are served in a round-robin fashion.</li> <li>Realtime: The program with realtime class priority will be given first access to the disk. This must be used with care as there is a potential for realtime processes to starve other processes of disk IO.</li> </ul>"},{"location":"notes/sysadmin/linux/misc/#nice","title":"nice","text":"<p><code>nice</code> determines the CPU scheduling priority. Processes have values between -20 (highest priority) and 19 (lowest priority).</p>"},{"location":"notes/sysadmin/linux/misc/#cpu-affinity","title":"CPU Affinity","text":"<p>The <code>taskset</code> command is used to set CPU affinity. Example:</p> Text Only<pre><code>$ taskset \u2013cp 2 914745\npid 914745\u2019s current affinity list: 0\npid 914745\u2019s new affinity list: 2\n</code></pre>"},{"location":"notes/sysadmin/linux/misc/#optimizing-for-the-numa-node","title":"Optimizing for the NUMA node","text":"<p>The <code>lscpu</code> command will show you which cores are on which NUMA node. If possible, applications should be given CPU affinities that are on a single NUMA node to prevent long-distance memory access on a different node.</p>"},{"location":"notes/sysadmin/linux/misc/#udev","title":"udev","text":"<p>udev is a replacement for the Device File System (DevFS) starting with the Linux 2.6 kernel series. It allows you to identify devices based on their properties, like vendor ID and device ID, dynamically. udev runs in userspace (as opposed to devfs which was executed in kernel space).</p> <p>udev allows for rules that specify what name is given to a device, regardless of which port it is plugged into. For example, a rule to always mount a hard drive with manufacturer \"iRiver\" and device code \"ABC\" as /dev/iriver is possible. This consistent naming of devices guarantees that scripts dependent on a specific device's existence will not be broken.</p> Text Only<pre><code>/etc/udev/rules.d/\n</code></pre>"},{"location":"notes/sysadmin/linux/misc/#syscalls","title":"Syscalls","text":""},{"location":"notes/sysadmin/linux/misc/#sendfile","title":"sendfile","text":"<p><code>sendfile</code> is an efficient way to copy data between two file descriptors. Because copying is done in kernel space, it eliminates the context switches needed to userspace in operations that would call <code>read</code> then <code>write</code>.</p> <p>A great article by my coworker at Lambda Labs on this topic can be found here: https://www.linuxjournal.com/article/6345</p>"},{"location":"notes/sysadmin/linux/systemd/","title":"systemd","text":""},{"location":"notes/sysadmin/linux/systemd/#verify-a-unit-file-is-valid","title":"Verify a unit file is valid.","text":"Text Only<pre><code>$ systemd-analyze verify my-server.service\n$ echo $?\n0\n</code></pre>"},{"location":"notes/sysadmin/linux/systemd/#verify-a-systemd-calendar-spec","title":"Verify a systemd calendar spec","text":"Text Only<pre><code>$ systemd-analyze calendar '12:*:*'\nOriginal form: 12:*:*\nNormalized form: *-*-* 12:*:*\n    Next elapse: Tue 2023-12-12 12:00:00 CST\n       (in UTC): Tue 2023-12-12 18:00:00 UTC\n       From now: 13h left\n</code></pre>"},{"location":"notes/sysadmin/linux/tty/","title":"TTY","text":""},{"location":"notes/sysadmin/linux/tty/#pty","title":"PTY","text":"<p>A PTY is a pseudo TTY.</p>"},{"location":"notes/sysadmin/linux/tty/#pts","title":"PTS","text":"<p>A PTS is the slave side of a PTY. You can often find these in the <code>/dev/pts/</code> directory. VMs launched by libvirt/QEMU often have their consoles exposed here.</p>"},{"location":"notes/sysadmin/linux/tty/#relevant-links","title":"Relevant Links","text":"<ul> <li>https://yakout.io/blog/terminal-under-the-hood/</li> <li>https://www.kernel.org/doc/html/v6.2/driver-api/tty/tty_buffer.html#id1</li> <li>https://www.kernel.org/doc/html/v6.2/driver-api/tty/tty_ldisc.html</li> </ul>"},{"location":"notes/system-design/databases/","title":"Databases","text":""},{"location":"notes/system-design/databases/#strategies-for-heavy-distributed-writes-in-sql","title":"Strategies for Heavy Distributed Writes in SQL","text":""},{"location":"notes/system-design/databases/#master-slave-replication","title":"Master-Slave Replication","text":"<p>You could have multiple replications of your master database that provide various writable locations. However, this still runs into the issue of needing to lock your database when accessing shared data.</p>"},{"location":"notes/system-design/databases/#sharding","title":"Sharding","text":"<p>You can shard your database such that each node is responsible only for a particular part of the DB, based on the partition key.</p>"},{"location":"notes/system-design/databases/#batched-updates","title":"Batched updates","text":""},{"location":"notes/system-design/databases/#kv-store","title":"KV Store","text":"<p>You could put writes into a distributed key-value store, and have some sort of \"flushing\" script that periodically sweeps the KV store and persists it to SQL. </p>"},{"location":"notes/system-design/databases/#message-queue","title":"Message Queue","text":"<p>Or, you could use message queues and batch the messages in groups of 1000's (for example).</p>"},{"location":"notes/system-design/databases/#data-lake","title":"Data lake","text":"<p>You could store the SQL updates in a sqlite file (ugh...) and import that directly into the MySQL/PostgreSQL/WhateverSQL. This only works if you can reasonably assume that the updates will not encounter any race conditions (for example, if the updates only consist of adding rows, and not modifying rows).</p>"},{"location":"notes/system-design/databases/#acid","title":"ACID","text":"<p>ACID stands for Atomic, Consistent, Isolated and Durable. This is a type of consistency guarantee that ensures database access and updates are executed correctly and that the database can recover from failures without losing data or impacting data consistency.</p>"},{"location":"notes/system-design/system-design-interview-alex-xu/","title":"System Design Interview \u2013 An insider's guide","text":"<p>https://books.google.com/books/about/System_Design_Interview_An_Insider_s_Gui.html?id=b_mUzQEACAAJ&amp;source=kp_book_description</p>"},{"location":"notes/system-design/system-design-interview-alex-xu/#chapter-1-scale-from-zero-to-millions-of-users","title":"Chapter 1: Scale from Zero To Millions of Users","text":"<p>Database Sharding</p> <p>Sharing is a horizontal scaling technique used in databases whereby multiple databases share the same schema, but will store different sets of data. Where a specific piece of data goes depends on the sharding key.</p> <p>Sharding has many problems:</p> <ol> <li>Resharding data: one shard might become too saturated due to uneven data distribution. This requires changing the sharding function and moving data around.</li> <li>Celebrity problem: what if Lady Gaga gets put on a single shard? Now that shard will be overloaded with reads.</li> <li>Joins: corss-shard joins become difficult (you need to think about how to optimize your query by considering where the data lives).</li> </ol>"},{"location":"notes/system-design/system-design-interview-alex-xu/#chapter-2-back-of-the-envelope-estimation","title":"Chapter 2: Back-of-the-Envelope Estimation","text":"<p>Back-of-the-envelope estimations are typically asked for in system design interviews.</p> <p>Tips:</p> <ul> <li>Write down your assumptions</li> <li>Label your units</li> <li>Write down QPS (queries per second), peak QPS, storage, cache (if applicable), number of servers etc.</li> </ul>"},{"location":"notes/system-design/system-design-interview-alex-xu/#power-of-two","title":"Power of Two","text":"power approximate value full name short name 10 1 Thousand 1 Kilobyte 1 KB 20 1 Million 1 Megabyte 1 MB 30 1 Billion 1 Gigabyte 1 GB 40 1 Trillion 1 Terabyte 1 TB 50 1 Quadrillion 1 Petabyte 1 PB"},{"location":"notes/system-design/system-design-interview-alex-xu/#latency-numbers","title":"Latency numbers","text":"<p>Here are some typical latency numbers every programmer should know:</p> Operation Name Time L1 cache reference 0.5 ns Branch mispredict 5 ns L2 cache reference 7 ns Mutex lock/unlock 100 ns Main memory reference 100ns Compress 1K bytes with Zippy 10,000 ns = 10 \u03bcs Send 2K bytes over 1Gbps network 20,000 ns = 20 \u03bcs Read 1 MB sequentially from memory 250,000 ns = 250 \u03bcs Round trip within the same datacenter 500,000 ns = 500 \u03bcs Disk seek 10,000,000 ns = 10 ms Read 1 MB sequentially from network 10,000,000 ns = 10 ms Read 1 MB sequentially from disk 30,000,000 ns = 30ms Send packet (California -&gt; Netherlands -&gt; California) 150,000,000 ns = 150ms"},{"location":"notes/system-design/system-design-interview-alex-xu/#availability-numbers","title":"Availability numbers","text":"<p>Usually measured in \"nines\", or number of 9's digits. Example: 3 nines is 99.9% availability.</p>"},{"location":"notes/system-design/system-design-interview-alex-xu/#chapter-3-a-framework-for-system-design","title":"Chapter 3: A Framework for System Design","text":""},{"location":"notes/system-design/system-design-interview-alex-xu/#4-step-process-for-interview","title":"4-step process for interview","text":"<ol> <li>Understand the problem and establish design scope<ul> <li>Do not rush into starting a solution. Slow down, ask questions, and think deeply about the requirements and assumptions. This is extremely important.</li> <li>When you ask a question, the interviewer will either answer or tell you to make an assumption. In either case, write down the answer, or the new assumption you have to make.</li> <li>Ask questions to understand the requirements. Examples:<ul> <li>What specific features are we going to build?</li> <li>How many users does the product have?</li> <li>How fast does the company anticipate to scale up?</li> <li>What is the company's technology stack? What existing services you might leverage to simplify the design?</li> </ul> </li> </ul> </li> <li>Propose high-level design and get buy-in<ul> <li>Come up with an initial blueprint. Ask for feedback. Treat your interviewer as a teammate and work together.</li> <li>Draw box diagrams with key components on a whiteboard or paper.</li> <li>Do back-of-the-envelope calculations to evaluate if your blueprint fits the scale constraints. Think out loud. Communicate with interviewer if back-of-the-envelope is necessary before diving into it.</li> <li>Go through a few concrete use-cases</li> <li>Should we include API endpoints and database schema? What level of detail are we looking for?</li> </ul> </li> <li>Design deep dive<ul> <li>You and the interviewer should have already agreed on the following objectives:<ol> <li>Agreed on overall goals</li> <li>Sketched out high-level blueprint for overall design</li> <li>Obtained feedback from interviewer on high-level design</li> <li>Had some intial ideas about areas to focus on based on interviewer's feedback</li> </ol> </li> </ul> </li> <li>Wrap up<ul> <li>Interviewer might ask a few follow up questions:<ol> <li>Identify system bottlenecks and potential improvements</li> <li>Might be useful to give the interviewer a recap of the design. Refreshing the interviewer's memory is helpful.</li> <li>Error cases?</li> <li>Operation issues. How do support this system in on-call? Observability/monitoring/logging?</li> <li>How to scale this up? If your design can handle 1 million users, what changes have to be made to scale to 10 million?</li> <li>Propose other refinements</li> </ol> </li> </ul> </li> </ol>"},{"location":"notes/system-design/system-design-interview-alex-xu/#dos-and-donts","title":"Do's and Don'ts","text":"<p>Dos:</p> <ol> <li>Ask for clarification. Do not assume assumption is correct.</li> <li>Understand the requirements</li> <li>There is no right answer nor the best answer. </li> <li>Let the interviewer know what you are thinking.</li> <li>Suggest multiple approaches if possible.</li> <li>Once you agree on blueprint, go into detail on each component. Design most critical components first.</li> <li>Bounce ideas off interviewer</li> <li>Never give up</li> </ol> <p>Don'ts:</p> <ol> <li>Don't be unprepared for typical interview questions</li> <li>Don't jump into a solution without clarifying requirements</li> <li>Don't go into too much detail on a single component. Start at high-level, then drill down where appropriate.</li> <li>If you get stuck, don't hesitate to ask for hints.</li> <li>Don't think in silence.</li> <li>Don't think the interviewer is done when you give the design. Ask for feedback early and often.</li> </ol>"},{"location":"notes/system-design/system-design-interview-alex-xu/#time-allocation","title":"Time allocation","text":"<p>Allocated 45 minutes or an hour is typical, but not enough to entirely flesh out a full system.</p> <p>Step 1: understanding problem and design scope (3-10 minutes)</p> <p>Step 2: Propose high-level design (10-15 minutes)</p> <p>Step 3: Design deep dive (10-25 minutes)</p> <p>Step 4: Wrap up (3-5 minutes)</p>"},{"location":"notes/system-design/system-design-interview-alex-xu/#chapter-4-design-a-rate-limiter","title":"Chapter 4: Design a Rate Limiter","text":"<p>(these are just my notes of what the author would ask in such an interview)</p>"},{"location":"notes/system-design/system-design-interview-alex-xu/#step-1","title":"Step 1","text":"<ol> <li>What kind of rate limiter? Client or server side? Answer: server side.</li> <li>Does the rate limiter throttle API requests based on IP, user ID, or other property? Answer: It should be flexible enough to support different kinds of throttle rules.</li> <li>Scale of the system? Startup-scale or big company with large user base? Answer: Large number of users</li> <li>Will the system work in a distributed environment? Answer: yes.</li> <li>Is the rate limiter a separate service or is it implemented in application code? Answer: it's a design decision up to you.</li> <li>Do we need to inform users when they're throttled? Answer: yes.</li> </ol> <p>Requirements</p> <ol> <li>Accurately limit excessive requests</li> <li>Low latency</li> <li>Use as little memory as possible</li> <li>Distributed rate limiting. Rate limiter should be shared across multiple processes/servers.</li> <li>Exception handling: show clear message to the user</li> <li>High fault tolerance: any problems with rate limiter will not affect the entire system.</li> </ol>"},{"location":"notes/system-design/system-design-interview-alex-xu/#step-2","title":"Step 2","text":"<p>Where to put rate limiter? You could put it as a separate microservice next to the API servers, but this introduces an issue of the API servers needing to potentially query the rate limiter. It's not a scalable solution.</p> <p>Cloud microservices usually implement rate limiting within an API gateway, in front of the API servers. </p> <p>These two implementations are both valid, but have various pros/cons. Things to consider:</p> <ol> <li>Company's current tech stack</li> <li>Identify rate limiting algorithm that meets business needs. You have much control if you implement your own solution. Using a third-party API gateway might limit you in your choices.</li> <li>Building your own limiter takes time.</li> </ol>"},{"location":"notes/system-design/system-design-interview-alex-xu/#choice-of-algorithms","title":"Choice of algorithms","text":"<ol> <li>Token bucket<ul> <li>Very common, well-understood.</li> <li>Used by large companies like Amazon and stripe.</li> <li>A bucket can contain a certain number of tokens. A refiller will periodically add a token to the bucket.</li> <li>Consumers of the API grab a token. If no token exists, the request is rate limited.</li> <li>Pros:<ul> <li>easy to implement</li> <li>memory efficient</li> <li>Allows bursts in traffic for short periods</li> </ul> </li> <li>Cons:<ul> <li>Two parameters to the algorithm might be challenging to tune properly.</li> </ul> </li> </ul> </li> <li>Leaking bucket<ul> <li>Requests go into a queue. If the queue is full, the request is dropped.</li> <li>The queue is processed at a fixed rate.</li> <li>Pros:<ul> <li>Memory efficient given limited queue size</li> <li>Requests are processed at a fixed rate, so it's suitable in cases where a stable outflow is needed.</li> </ul> </li> <li>Cons:<ul> <li>Bursts of traffic will fill up the queue with old requests. New requests will get dropped.</li> <li>Two parameters to the algorithm (queue size, outflow rate) might be difficult to tune.</li> </ul> </li> </ul> </li> <li>Fixed window counter algorithm<ul> <li>Each time window is allowed only a certain number of requests. If a request comes in when that limit is reached, it's dropped.</li> <li>Pros:<ul> <li>Memory efficient</li> <li>Easy to understand</li> <li>Resetting available quota at the end of time unit might fit some use cases.</li> </ul> </li> <li>Cons:<ul> <li>Spike in traffic can cause more requests to come through than what is intended/allowed.</li> </ul> </li> </ul> </li> <li>Sliding log window<ul> <li>Keeps track of request timestamps, cached in something like redis.</li> <li>When new request arrives, remove timestamps from redis bucket that are older than <code>now - window_size</code>.</li> <li>Add new request timestamp to bucket</li> <li>If bucket size is less than or equal to max allowed size, allow the request. Otherwise, deny it.</li> <li>Note: the main difference between the sliding log window and the fixed window is that the boundary of the fixed window is set on some regular interval (something like every second, or every minute, or every hour).</li> <li>Pros: <ul> <li>Rate limiting is very accurate. In any rolling window, requests will not exceed the limit.</li> </ul> </li> <li>Cons:<ul> <li>Memory intensive. Timestamps still need to be stored in memory even if the request is rejected.</li> </ul> </li> </ul> </li> <li>Sliding window counter<ul> <li>We use a set of predefined time windows, as in the Fixed Window algorithm, that have boundaries on some regular interval.</li> <li>We use another sliding window that overlaps these fixed windows.</li> <li>To calculate the number of requests in our rolling window, we:<ul> <li>Calculate the number of requests in the previous minute. Multiply it by the percentage of which our sliding window overlaps with the previous window.</li> <li>Calculate the number of requests in the current minute</li> <li>Formula: <code>requests_current_window + (requests_previous_window * percent_overlap_previous_window)</code>.</li> </ul> </li> <li>Pros:<ul> <li>Smooths out spikes in traffic</li> <li>Memory efficient (only have to store the requests counts of a few fixed windows)</li> </ul> </li> <li>Cons:<ul> <li>Assumes that the distribution of the previous window is uniform. Consequently, it might incorrectly block a request. For example, consider if all requests for the previous window came towards the beginning of the window. When we run our calculation, the average number of requests in the previous window might put us over our limit, even if our sliding window does not overlap when the previous requests happened.</li> <li>Author notes that according to experiments at Cloudflare, only 0.003% of requests are incorrectly rate-limited or allowed over 400 million requests.</li> </ul> </li> </ul> </li> </ol> <p>The author's high-level overview is as such:</p> <pre><code>flowchart\n    client --&gt; rateLimiter\n    rateLimiter --&gt; redis\n    rateLimiter --&gt; APIServers[API Servers]</code></pre>"},{"location":"notes/system-design/system-design-interview-alex-xu/#step-3","title":"Step 3","text":"<p>A basic data structure in Redis is to have a single counter value that we increment/decrement. Race conditions arise when you have multiple API gateway instances trying to read this counter value. This can be solved by:</p> <ul> <li>Using locks (not preferrable as they slow down the system)</li> <li>Sorted set data structure</li> <li>Lua scripts (I don't know what the author means by this).</li> </ul> <p>We want a centralized datastore like redis so that all rate limiter instances hit the same backing store. </p> <p>Question</p> <p>A question to myself is whether you should have one big-ass redis cluster for the whole world (probably not) or if you should have a single redis cluster for every datacenter (or region). With anycast DNS (or geocast), it might be sufficient for the rate limiters local to a specific datacenter to only use a redis backend specific to that datacenter. I can't immediately think of any reason why this wouldn't work.</p> <p>The author does note that you want to synchronize the data across datacenters with an eventual consistency model. This will come up in Chapter 6.</p>"},{"location":"notes/system-design/system-design-interview-alex-xu/#chapter-5-design-consistent-hashing","title":"Chapter 5: Design Consistent Hashing","text":"<p>Notes on using different hashing methods to route your requests to a specific server.</p>"},{"location":"notes/system-design/system-design-interview-alex-xu/#the-rehashing-problem","title":"The rehashing problem","text":"<p>Using the hash method <code>serverIdx = hash(key) % N</code> works when server size is fixed, but if you add more servers, then the mappings will get reshuffled quasi-randomly. This causes tons of cache misses and will tank performance.</p>"},{"location":"notes/system-design/system-design-interview-alex-xu/#consistent-hashing","title":"Consistent Hashing","text":"<p>Consistent hashing is the answer to the rehashing problem. I'm going to be borrowing a lot of diagrams from https://www.toptal.com/big-data/consistent-hashing, which is a great blog that describes in detail how consistent hashing works.</p> <p>The general idea is that both servers and users live within the same hash space. The servers, more often than not, will have a hash key that depends on the name. The users will have a hash that depends on their name, but also possibly their location.</p> <p>When a user is hashed, we find the closest adjacent server in a counter-clockwise manner (it could also be clockwise, it doesn't matter). When a new server gets added, there is only a small probability that any particular user needs to get re-mapped to a new server. Or in other words, the chance of a new server being blaced between a user and that user's prior server is quite low.</p> <p>In the case that does happen, a re-mapping will indeed have to occur, but the overall probability is low enough that the cache misses should be minimal.</p> <p>Note</p> <p>The author notes that it's very likely for nodes in a hash ring to become too clumped together, which would cause uneven distribution (this is called the Hotspot Key Problem). He notes that for every \"real\" node, we can inject a finite number of virtual nodes and hash those as well. This effectively causes the hash ring to become more uniformly distributed as it causes the standard deviation (in terms of empty space in the ring) to be smaller.</p>"},{"location":"notes/system-design/system-design-interview-alex-xu/#chapter-6-design-a-key-value-store","title":"Chapter 6: Design a Key-Value Store","text":"<p>We want to design a distributed key-value store. These are the requirements:</p> <ol> <li>Key-value pair is small: less than 10KB</li> <li>Store big data</li> <li>High availability</li> <li>High scalability</li> <li>Automatic scaling</li> <li>Tunable consistency</li> <li>Low latency</li> </ol>"},{"location":"notes/system-design/system-design-interview-alex-xu/#cap-theorem","title":"CAP Theorem","text":"<p>CAP stands for Consistency, Availability, and Partition Tolerance. The theorem states that a distributed system cannot meet more than 2 of those attributes at once. Partition Tolerance refers to the ability of the system to tolerate its network being unintentionally partitioned/separated (think: split brain scenarios).</p> <p>Key-value stores are categorized based on which of these attributes they meet:</p> <ul> <li>CP systems: supports consistency and partition tolerance while sacrificing availability.</li> <li>AP systems: supports availability and partition tolerance while sacrificing consistency.</li> <li>CA systems: supports consistency and availability while sacrificing partition tolerance. The author notes that because networks can fail, partition intolerance is generally not acceptable.</li> </ul>"},{"location":"notes/system-design/system-design-interview-alex-xu/#data-partitions","title":"Data partitions","text":"<p>We can use the consistent hashing method to partition data! This will minimize data movement when nodes are added/removed. We need to consider how to handle the case when data does need to be moved when we add/remove nodes. We also need to consider how our data will be replicated to multiple nodes for high availability purposes. </p> <p>When creating a key, we can simply place it to the first N nodes seen (when rotating clockwise along the hash ring). We ensure that we only look at real nodes, not virtual nodes, to ensure we're not incorrectly writing a key multiple times to the same server.</p>"},{"location":"notes/system-design/system-design-interview-alex-xu/#consistency","title":"Consistency","text":"<p>Quorum consensus can be used to ensure read/write consistency. First some definitions:</p> <ul> <li>N = the number of replicas</li> <li>W = write quorum size. Write operations must be ack'ed by W replicas to be considered successful.</li> <li>R = read quorum size. Read must wait for responses from R replicas.</li> </ul> <p>N/W/R can be tuned for various use cases:</p> <ul> <li>R=1, W=N, system optimized for fast read.</li> <li>W=1, R=N, systemm optimized for fast write.</li> <li>W+R&gt;N, strong consistency is guaranteed.</li> <li>W+R&lt;=N, strong consistency is not guaranteed.</li> </ul> <p>Types of consistency models:</p> <ol> <li>Strong consistency: read operations are guaranteed to have the most updated view of the data. Think: GPFS, LustreFS, most on-disk POSIX filesystems.</li> <li>Weak consistency: subsequent read operations not guaranteed to see the most updated view.</li> <li>Eventual consistency: read operations eventually receive the most updated view.</li> </ol>"},{"location":"notes/system-design/system-design-interview-alex-xu/#inconsistency-resolution-versioning","title":"Inconsistency resolution: versioning","text":"<p>When concurrent writes are made, how do we handle the conflicts. In CVMFS, all concurrent writes go through a serialization step on the publisher. Because it's all transaction based, transactions updates are processed serially, so there is no chance for conflicting writes to happen. CVMFS also explicitly versions each update in the root catalog.</p>"},{"location":"notes/system-design/system-design-interview-alex-xu/#vector-clock","title":"Vector Clock","text":"<p>The author recommend a vector clock as one possible solution. It's a <code>&lt;server,version&gt;</code> pair associated with a data item.</p>"},{"location":"notes/system-design/system-design-interview-alex-xu/#failure-detection","title":"Failure detection","text":"<p>We need to detect failures in our system. There are a few methods of doing this.</p>"},{"location":"notes/system-design/system-design-interview-alex-xu/#gossip-protocol","title":"Gossip Protocol","text":"<ul> <li>Each nodes maintains node membership list, which contains member IDs and heartbeat counters</li> <li>Each node periodically increments its heartbeat counter</li> <li>Each node periodically sends heartbeats to a set of random nodes, which in turn propagate to other set of nodes</li> <li>Once nodes receive heartbeats, membership list is updated to the latest info.</li> <li>If heartbeat has not increased for more than predefined period, the member his considered as offline. (question: should a peer eagerly declare to other peers that it considers something offline?)</li> </ul>"},{"location":"notes/system-design/system-design-interview-alex-xu/#temporary","title":"Temporary","text":"<p>In strict quorum approach, reads/writes will be blocked until there is quorum consensus. In a \"sloppy quorum\" approach, the system chooses the first W healthy servers for writes, and first R healthy servers for reads on the hash ring.</p> <p>For sloppy quorum, unavailable servers which come back up will go through what's called a \"hinted handoff,\" or in other words, its peers will push changes back to it to achieve consistency.</p>"},{"location":"notes/system-design/system-design-interview-alex-xu/#permanent","title":"Permanent","text":"<p>What happens if replica is permanently unavailable? We use what's called an Anti-Entropy Protocol. A Merkle Tree is suggested, which surprise surprise, is exactly what CVMFS uses! Merkle trees verify integrity first by comparing the root hash. If the hash is the same, the trees are identical. If they're not, we need to recurse into the tree to find which nodes are different. This can be done in a \\(O(log(n))\\) manner as each node contains hash pointers to its children, so we can easily find what part of the tree is different. Once the differences has been found, we can reconcile the differences.</p>"},{"location":"notes/system-design/system-design-interview-alex-xu/#datacenter-outages","title":"Datacenter outages","text":"<p>Your data must be replicated across multiple datacenters. You can do this by having the client interact with a coordinator that acts as a proxy to the key-value store. Using a consistent hash ring helps resolve this, along with the gossip protocol.</p>"},{"location":"notes/system-design/system-design-interview-alex-xu/#write-path","title":"Write Path","text":"<p>The author proposes a write path that's similar to how Cassandra works.</p> <pre><code>flowchart TD\n\n    client\n\n    subgraph Server\n        subgraph memory\n            memoryCache(Memory Cache)\n            processor(Write Processor)\n\n            processor --&gt;|\"(2)\"| memoryCache\n        end\n\n        subgraph disk\n            SSTables(SS Tables)\n            CommitLog(Commit Log)\n\n            memoryCache --&gt;|\"(3) Flush\"| SSTables\n            processor --&gt;|\"(1)\"| CommitLog\n        end\n\n    end\n\n    client --&gt; processor</code></pre> <ol> <li>The write request goes to a commit log in a file</li> <li>Data is saved to in-memory cache</li> <li>When cache is full, data is flushed to an SStable (1) on disk.</li> </ol> <ol> <li>An SSTable is a sorted-string table. It's a sorted list of key/value pairs.</li> </ol>"},{"location":"notes/system-design/system-design-interview-alex-xu/#read-path","title":"Read Path","text":"<p>A client will reference the in-memory cache initially. If it's not in the cache, it will be retrieved from disk. We have to find what SSTable contains the key (because they can be dispersed through many). A bloom filter (1) is a common method of solving this.</p> <ol> <li>What is a bloom filter? I had heard of this before but wasn't sure what exactly it is. It's a data structure that is used to test if an element is a member of a set. False positives are possible, but not false negatives, so it tells you either \"possibly in set\" or \"definitely not in set.\" Geeks for Geeks has a great article on how this works. It's actually quite a simple data structure, but it's very powerful!</li> </ol>"},{"location":"notes/system-design/system-design-interview-alex-xu/#summary","title":"Summary","text":"Goal/Problems Technique Ability to store big data Use consistent hashing to spread the load High availability reads Data replication. Multi-datacenter setup High availability writes Version and conflict resolution with vector clocks Dataset partition Consistent hashing Incremental scalability Consistent hashing Heterogeneity Consistent hashing Tunable consistency Quorum consensus Handling temporary failures sloppy quorum and hinted handoff Handling permanent failures Merkle tree Handling data center outages Cross-datacenter replication"},{"location":"notes/system-design/system-design-interview-alex-xu/#chapter-7-design-a-unique-id-generator-in-distributed-systems","title":"Chapter 7: Design a Unique ID Generator in Distributed Systems","text":"<p>The interviewer asks us to design a unique ID generator that works globally. </p> <p>Step 1</p> <p>We need to understand the problem scope. The constraints are:</p> <ol> <li>IDs must be unique and sortable</li> <li>It increments by time but not necessarily by 1.</li> <li>It only contains numerical values</li> <li>It should fit into 64 bits</li> <li>The system should be able to generate 10,000 IDs per second</li> </ol> <p>Step 2</p> <p>We need to propose a high-level design. There are a few options:</p> <ol> <li>UUID: this is a solution that already exists, however this doesn't work because it doesn't fit in 64 bits.</li> <li>Auto-incrementing ID in a database: also doesn't work because it's not scalable.</li> <li>Ticket server: a single server generates numeric IDs, but it's not scalable because it's a single point of failure.</li> <li>\"Twitter snowflake\": this is the approach we'll try. Twitter has a system called snowflake that uses a 64 bit ID. These 64 bits are partitioned accordingly:</li> </ol> bit range description 0:1 reserved for future use 1:42 timestamp 42:47 datacenter ID 47:52 machine ID 52:64 sequence number <p>The timestamp is a millisecond-since-epoch timestamp, with the epoch being Nov 04, 2010, 01:42:54 UTC. The sequence number is incremented by every ID generated on the machine, and is reset back to 0 once per millisecond.</p> <p>Step 3</p> <p>Now let's go into a deep-dive on our design.</p> <p>The timestamp uses 41 bits so it can only represent 2199023255551 milliseconds, or about 69 years. So this will work only up until sometime in 2079.</p> <p>The sequence number lets us generate 4096 seqnums per millisecond, so this is the maximum threshold that each server can generate.</p> <p>Step 4</p> <p>Now we need to wrap up our design. We designed a system that requires no synchronization between ID generators and is capable of scaling out to a huge number of servers. It is not possible to generate duplicate IDs for 69 years, assuming all datacenter IDs and machine IDs (within their datacenter) are unique.</p>"},{"location":"notes/system-design/system-design-interview-alex-xu/#chapter-11-design-a-news-feed-system","title":"Chapter 11: Design a News Feed System","text":"<p>I've skipped writing down notes for the previous few chapters as I'm intending to follow solutions for problems that are trivial to me personally (mind you, I still read the chapters obviously, but some of the design questions are fairly simple to me).</p> <p>In this chapter, we're asked to design a news feed. </p> <p>Step 1</p> <p>We need to ask follow-up questions. These are the author's questions:</p> <ol> <li>Question: What kinds of devices does this run on? Answer: Web and mobile app.</li> <li>Question: What are the important features? Answer: a user can publish a post and see friends' posts on the news feed page.</li> <li>Question: Is the news feed sorted by reverse chronological order or any particular order such as topic scores? Answer: assume it's sorted by reverse chronological order.</li> <li>Question: how many friends can a user have? Answer: 5000</li> <li>Question: What is the traffic volume? Answer: 10 million DAU.</li> <li>Question: Can it contain images, videos, or just text? Answer: it can contain media files, including images and videos.</li> </ol> <p>These are the kinds of questions I would ask:</p> <ol> <li>Can the news feed be populated via ads?</li> <li>Can it be populated with relevant news articles that you may find interesting?</li> <li>What are the latency requirements fom when a friend creates a post, to when it can potentially show up on a user's feed?</li> <li>Can a user decide to hide a particular friend's posts?</li> </ol> <p>Step 2</p> <p>There are two main components to this: feed publishing, and feed building.</p>"},{"location":"notes/system-design/system-design-interview-alex-xu/#feed-publishing-api","title":"Feed publishing API","text":"<p>To create a post, we send an HTTP POST request to an API:</p> method URL params POST /v1/me/feed <ul><li>content: the text of the post</li><li>auth_token: used to authenticate requests</li></ul>"},{"location":"notes/system-design/system-design-interview-alex-xu/#newsfeed-retrieval-api","title":"Newsfeed retrieval API","text":"<p>To get the news feed, we do:</p> method URL params GET /v1/me/feed <ul><li>auth_token: used to authenticate requests</li></ul> <p>Note</p> <p>This is not the way I would design such an API. My preference would be for the <code>POST</code> request to send to an endpoint like <code>/v1/me/posts</code>, which would then return a post ID that you could use to retrieve just that particular post, like <code>GET /v1/me/posts/51</code>. Additionally, it might make even more sense for the API to be structured around users. For example, if you wanted to get all posts for a particular user, you could do <code>GET /v1/users/8192/posts</code>. Under such a scheme, you would create a post under that user's ID like <code>POST /v1/users/8192/posts</code> and get a particular post like <code>GET /v1/users/8192/posts/51</code>. The feed for a particular user would be under <code>GET /v1/users/8192/feed</code>.</p> <p>This is a bit cleaner in my opinion and it makes it clear that every post and feed is specific to a particular user.</p> <p>Step 3</p>"},{"location":"notes/system-design/system-design-interview-alex-xu/#fanout-service","title":"Fanout service","text":"<p>There are two primary ways to architect this: fanout on write, or fanout on read.</p>"},{"location":"notes/system-design/system-design-interview-alex-xu/#fanout-on-write","title":"Fanout on Write","text":"<p>The feed is pre-computed for every user during write time. It's delivered to a friend's cache immediately after publish.</p> <p>Pros:</p> <ol> <li>Feed generated realtime and can be pushed immediately</li> <li>Fetching feed is fast because it's already been pre-computed.</li> </ol> <p>Cons: 1. If a user has many friends, fetching the friend list and generating feed is slow and time consuming. It's called the hotkey problem. 2. For inactive users, pre-computing the feed is a waste of resources.</p>"},{"location":"notes/system-design/system-design-interview-alex-xu/#fanout-on-read","title":"Fanout on Read","text":"<p>Pros:</p> <ol> <li>For inactive users, resources are not wasted on generating a feed.</li> <li>Data is not published to friends so there is no hotkey problem</li> </ol> <p>Cons:</p> <ol> <li>Fetching news feed is slow as it's not pre-computed.</li> </ol> <p>You can adopt a hybrid approach where we adtop a fanout-on-write model for most users, but for users like celebreties or well-connected accounts, we can grab the feed on read to prevent system overload. We can use consistent hashing here (yay consistent hashing! )</p> <p>The fanout service works as follows:</p> <ol> <li>Fetch friend IDs from the graph database.</li> <li>Get friends info from the user cache. The system filters based on user settings, for example if the user mutes someone.</li> <li>Send friends list and new post ID to the message queue.</li> <li>Fanout workers fetch data from message queue and store news feed data in news feed cache. The news feed cache is a <code>&lt;post_id,user_id&gt;</code> mapping table. Whehter new post is made, it will be appended to news feed table.</li> <li>Store <code>&lt;post_id,user_id&gt;</code> in news feed cache. </li> </ol>"},{"location":"notes/system-design/system-design-interview-alex-xu/#newsfeed-retrieval-deep-dive","title":"Newsfeed retrieval deep dive","text":"<ol> <li>A user sends a request to retrieve feed. The request looks like <code>GET /v1/me/feed</code></li> <li>Load balancer redistributes requests to web servers</li> <li>Web server call the news feed service to fetch feed.</li> <li>News feed service gets a list of post IDs from the feed cache.</li> <li>A user's feed is more than just a list of feed IDs. Contains username, profile picture, post content, image etc. Thus, news feed service fetches the complete user and post objects to construct the full news feed.</li> <li>News feed is returned in JSON format back to the client for rendering.</li> </ol>"},{"location":"notes/system-design/system-design-interview-alex-xu/#cache-architecture","title":"Cache architecture","text":"<ul> <li>News feed: stores IDs of new feeds</li> <li>Content: stores every post data.</li> <li>Social Graph: stores user relationships</li> <li>Action: stores info on whether a user liked, replied, or took another action on a post.</li> <li>Counters: stores counters for likes, replies, followers, following etc</li> </ul> <p>Step 4</p> <p>You can talk about scalability issues with this system. Maybe go into the ways in which other companies have solved this problem, like Twitter, Facebook etc. How might you scale the databases? SQL vs NoSQL? Read Replicas? How to keep web tier stateless? How to cache data? Supporting multiple datacenters? How to handle dead letter queues? Monitoring key metrics?</p>"},{"location":"notes/system-design/system-design-interview-alex-xu/#chapter-12-design-a-chat-system","title":"Chapter 12: Design a Chat System","text":"<p>The interviewer asks us to design a chat system. </p>"},{"location":"notes/system-design/system-design-interview-alex-xu/#step-1_1","title":"Step 1","text":"<p>Understand the requirements:</p> <ol> <li>This should support both 1 on 1 and group chat</li> <li>It's both a mobile and web app</li> <li>It should support 50 million DAU</li> <li>The important features are 1 on 1 chat, group chat, online indicator. The system only supports text messages.</li> <li>Text length is less than 100,000 characters long.</li> <li>e2e encryption is not an initial requirement.</li> <li>Chat history should be stored forever.</li> </ol>"},{"location":"notes/system-design/system-design-interview-alex-xu/#step-2_1","title":"Step 2","text":"<p>There are 3 main ways we could implement sending messages from client to client:</p> <ol> <li>Short polling: a client would periodically poll the server for new messages. This would work but it's inefficient because it requires opening and closing many connections.</li> <li>Long polling: same as short polling, but we can keep the connection open until we receive new messages. The disadvantages are:<ul> <li>The sender and receiver might not be on the same server.</li> <li>The server has no good way to tell if a client is disconnected.</li> <li>It's inefficient because long polling will still make periodic connection requests even if there are no messages being sent.</li> </ul> </li> <li>Websocket: Websocket is a protocol that allows you to asynchronously send and receive messages over the same connection. </li> </ol> <pre><code>flowchart TD\n    Sender &lt;--&gt;|Websocket| chatService[Chat Service]\n    Receiver &lt;--&gt;|Websocket| chatService</code></pre>"},{"location":"notes/system-design/system-design-interview-alex-xu/#high-level-design","title":"High-level design","text":"<p>Our system is partitioned into two types of services: stateful, and stateless. The stateful service includes the websocket connections for sending/receiving data. The stateless service includes things like user settings, group memberships, service discovery etc. Stateless services can remain as HTTP endpoints.</p> <pre><code>flowchart TD\n\n    subgraph Stateless\n        user --&gt;|HTTP| loadBalancer[Load Balancer]\n        loadBalancer --&gt; serviceDiscovery[Service Discovery]\n        loadBalancer --&gt; auth[Authentication Service]\n        loadBalancer --&gt; GroupManagement[Group Management]\n        loadBalancer --&gt; userProfile[User Profile]\n    end\n\n    subgraph Stateful\n        User1 &lt;--&gt;|Websocket| chatService[Chat Service]\n        User2 &lt;--&gt;|Websocket| chatService\n    end</code></pre> <p>Many of the stateless services listed above can be implemented using third-party solutions. The service discovery unit is a service that provides the client with a list of possible DNS names that it could connect to for the chat service. </p> <p>Where should we store chat history? A common solution might be a relational database, but at the scale needed by this system, SQL will begin to slow down tremendously as the indexes grow. A previous study showed that Facebook messenger and Whatsapp process 60 billion messages per day. That's probably not going to fit in most modern relational databases. Further, only the most recent chat messages are retrieved. A key-value store is a good fit for this pattern:</p> <ul> <li>It allows easy horizontal scaling</li> <li>It provides low latency access</li> <li>Relational databases do not handle long tails well</li> <li>Key-value stores are adopted by other proven chat systems. Facebook uses HBase and Discord uses Cassandra.</li> </ul>"},{"location":"notes/system-design/system-design-interview-alex-xu/#step-3_1","title":"Step 3","text":""},{"location":"notes/system-design/system-design-interview-alex-xu/#service-discovery","title":"Service Discovery","text":"<p>The service discovery will recommend the best chat server for the client to connect to. Based off of things like geographical region, node health, node load etc. Zookeeper is a popular choice for this.</p>"},{"location":"notes/system-design/system-design-interview-alex-xu/#chat-flow","title":"Chat flow","text":"<pre><code>flowchart TD\n    UserA --&gt; ChatServer\n    ChatServer --&gt; UserBQueue[(User B Queue)]\n    ChatServer --&gt; UserCQueue[(User B Queue)]\n    UserBQueue --&gt; UserB\n    UserCQueue --&gt; UserC</code></pre> <p>One way to route messages is for each user to have their own queue. This works fine for small-membership groups, but once membership becomes large, it becomes unacceptable to store the same message multiple times. This is why many chat services limit the number of members in a group.</p>"},{"location":"notes/system-design/system-design-interview-alex-xu/#online-presence-indicator","title":"Online Presence Indicator","text":"<p>We can have the user client send a heartbeat to the presence service. If the presence service doesn't receive a heartbeat within some amount of time, it can mark the user as offline. To fanout, we can have the presence service send to each user's individual queue. Again, this is okay for small groups, but is unsustainable for larger groups.</p>"},{"location":"notes/system-design/tools/","title":"Tools","text":"<p>This page shows various methods, techniques, algorithms, and data structures that may be useful in your system design career.</p>"},{"location":"notes/system-design/tools/#distributed-transactions","title":"Distributed Transactions","text":"<p>Transactions that span multiple microservices are difficult to manage due to the nature of distributed systems. Failures in the transaction need compensatory actions. </p>"},{"location":"notes/system-design/tools/#saga-pattern","title":"Saga Pattern","text":"<p>The Saga pattern is one way of managing distributed transactions.</p> <p></p> <p>There are two main ways to implement the saga pattern: orchestration and choreography.</p>"},{"location":"notes/system-design/tools/#choreography-based-saga","title":"Choreography-based Saga","text":"<p>This relies heavily on a good message broker. Each service in the saga listens to an event. Each step in the saga is announced as an event. The saga is propagated through completion by each service listening to its respective message channel.</p> <p>This is a highly decoupled form of distributed transactions because new services can be added simply by subscribing to a particular channel. The other pieces of the saga do not need to be aware of a new service being added. There is no centralized orchestration of this transaction. It's a collaborative, bottom-up approach.</p>"},{"location":"notes/system-design/tools/#orchestration-based-saga","title":"Orchestration-based Saga","text":"<p>Orchestrated Sagas rely on a single entity managing the entire end-to-end state of the transaction. This has benefits over choreography because it aggregates all of your business logic into a single deployable unit. Thus, it's easier to reason about the steps that will happen as it will be within a single codebase. You might choose orchestration in cases where the business logic needs to be rigorously and thoroughly tested, where failures are not tolerable. A choreography-based transaction presents challenges in testing because the behavior of the system depends on how it's deployed. This is not something you can easily test in CI/CD.</p> <p>This method is more coupled as any changes to the transaction must be done within a single service. It also means that the service performing the transaction has to be aware of every step of the transaction and every sub-service involved therein, which increases its logical complexity. However, this may be a justifiable cost given the needs of the business.</p>"},{"location":"notes/system-design/tools/#bloom-filters","title":"Bloom Filters","text":"<p>A bloom filter is a probabilistic data structure that tells you either:</p> <ol> <li>A particular piece of data is possibly in the set</li> <li>A particular piece of data is definitely not in the set.</li> </ol> <p>It is highly memory efficient (as most filters can reside totally in memory) and easy to implement in a distributed environment. The way it works starts with an array of bit-length N. Then:</p> <ol> <li>When you add an element to the set, you take its hash then <code>OR</code> the hash's bitwise value with the bloom filter. This adds the hash's bits to the array.</li> <li>To check of an element possibly exists in the set, take its hash and check if the 1's bits in the bloom filter match the 1's bits in the hash</li> <li>If the 1's match, then the element possibly already exists. If they don't, then it's a certainty the element was never added.</li> </ol> <p>The reason why you can only determine \"possibly exists in set\" is because different hashes can mark the same bit as 1. The probability of these false positives is dependent on the size of the filter and the number of elements added to it. This can be seen in the graph below:</p> <p></p>"},{"location":"notes/system-design/tools/#consistent-hashing","title":"Consistent Hashing","text":"<p>Consistent Hashing is a load balancing technique that allows you to add and remove nodes in a pool without causing massive redistributions in the client/server mappings (called the Rehashing Problem).</p> <p>The general idea is that both servers and users live within the same hash space. The servers, more often than not, will have a hash key that depends on the name. The users will have a hash that depends on their name, but also possibly their location.</p> <p>When a user is hashed, we find the closest adjacent server in a counter-clockwise manner (it could also be clockwise, it doesn't matter). When a new server gets added, there is only a small probability that any particular user needs to get re-mapped to a new server. Or in other words, the chance of a new server being blaced between a user and that user's prior server is quite low.</p> <p>In the case that does happen, a re-mapping will indeed have to occur, but the overall probability is low enough that the cache misses should be minimal.</p> <p>Virtual Nodes for Smoothing</p> <p> It's very likely for nodes in a hash ring to become too clumped together, which would cause uneven distribution (this is called the Hotspot Key Problem). You can smooth this out by using virtual nodes: for every real node, we also add some number of virtual nodes that maps back to the real node. This effectively causes the hash ring to become more uniformly distributed as it causes the standard deviation (in terms of empty space in the ring) to be smaller.</p> <p>You can see in a consistent hashing scheme, adding a node to the ring will cause only some of the clients to be re-mapped.</p>"},{"location":"notes/system-design/tools/#cap-theorem","title":"CAP Theorem","text":"<p>CAP stands for Consistency, Availability, and Partition Tolerance. The theorem states that distributed systems can exhibit at most two of these traits, which implies that a tradeoff must be made. In most systems, partition intolerance is not acceptable as networks and systems often fail. It's a bit of an oxymoron as well to state that a system is consistent and available, but not partition tolerant. So in reality, the tradeoff is usually between consistency and availability.</p>"},{"location":"notes/system-design/tools/#communication-protocols","title":"Communication Protocols","text":""},{"location":"notes/system-design/tools/#grpc","title":"gRPC","text":"<p>Used as a remote procedure call. Uses Protobuf to send and receive serialized structured data. Messages in protobuf are defined in a language-agnostic DSL.</p>"},{"location":"notes/system-design/tools/#websocket","title":"Websocket","text":"<p>Websocket is a web communication protocol that allows you to asynchronously send and receive messages over the same connection. It starts its life as an HTTP connection, and then is upgraded to a websocket session. This protocol is the answer to common HTTP polling techniques that are used to periodically check for updates from the server. Some examples of where this protocol is used:</p> <ol> <li>Real-time web applications where updates need to be continuously streamed.</li> <li>Chat applications where messages arrive asynchronously.</li> <li>Gaming applications where updates are fairly continual.</li> </ol> <p>Many of these kinds of data streaming problems were commonly solved through HTTP polling, however this solution is inefficient due to the constant HTTP/TCP handshakes that would need to occur even if no data was available.</p> <p>Websocket is generally served through ports 80 or 443 (HTTP/HTTPS), which allows it to pass through many types of firewalls. </p>"},{"location":"notes/system-design/tools/#polling-mechanisms","title":"Polling Mechanisms","text":""},{"location":"notes/system-design/tools/#short-polling","title":"Short polling","text":"<p>A client would periodically poll the server for new messages. This would work but it's inefficient because it requires opening and closing many connections.</p>"},{"location":"notes/system-design/tools/#long-polling","title":"Long polling","text":"<p>Same as short polling, but we can keep the connection open until we receive new messages. The disadvantages are: - The sender and receiver might not be on the same server. - The server has no good way to tell if a client is disconnected. - It's inefficient because long polling will still make periodic connection requests even if there are no messages being sent.</p>"},{"location":"notes/system-design/tools/#websocket_1","title":"Websocket","text":"<p>Websocket is a protocol that allows you to asynchronously send and receive messages over the same connection. </p>"},{"location":"notes/system-design/tools/#service-discovery","title":"Service Discovery","text":"<p>When you have a pool of services, for example a pool of websocket servers that provide clients with messages in a chat system, you often need to provide the client with a list of DNS hostnames they could connect to. This process is called \"Server-side Service Discovery\" and there are many off-the-shelf solutions:</p>"},{"location":"notes/system-design/tools/#etcd","title":"etcd","text":"<p>This is a distributed key-value store that can be used for service discovery (among many other things). It uses an eventually-consistent model through the gossip-based protocol. It does not have many built-in features for service discovery as it focuses more on the key-value store.</p>"},{"location":"notes/system-design/tools/#consul","title":"consul","text":"<p>consul is a Service Mesh that provides a rich suite of metrics and monitoring capabilities. It allows you to define health checks on your services. It also comes with native support for cross-datacenter replication, while etcd does not.</p> <p>consul also gives you distributed key-value stores</p>"},{"location":"notes/system-design/tools/#apache-zookeeper","title":"Apache Zookeeper","text":"<p>This is another distributed key/value store that also provides service discovery.</p>"},{"location":"notes/system-design/tools/#key-value-stores","title":"Key-Value Stores","text":"<ul> <li>redis</li> <li>cassandra</li> </ul> <p>Redis is often chosen over etcd for key-value stores because etcd requires consensus amongst its nodes before committing changes, which slows performance. The benefit of etcd is that it provides a lot of consistency guarantees. The docs claim it follows Sequential Consistency, which means that while a write does not have to be seen instantaneously, the sequence in which writes to a piece of data are seen is identical across all processors.</p>"},{"location":"notes/system-design/tools/#cassandra","title":"Cassandra","text":"<p>Cassandra is often compared against Redis. For a good comparison, look here. Many of the notes here are drawn from that blog.</p> <p>This is used by some large chat platforms like Discord to store chat history. Some aspects of Cassandra:</p> <ul> <li>Uses wide-column store as a database model, which makes it easy to store huge datasets. It acts as a two-dimensional key/val store.</li> <li>Focuses on stability, but it's slower than other platforms like Redis. Redis becomes much slower if you store huge datasets, and thus it's more suited for highly variable datasets.</li> <li>Cassandra focuses on consistency and partition tolerance, while redis focus on availability and partition tolerance.</li> <li>Schema-free.</li> <li>Uses Java, uses thrift protocol for API.</li> <li>More useful when you have distributed, linearly scalable, write-oriented, and democratic P2P database structure.</li> </ul>"},{"location":"notes/system-design/tools/#redis","title":"redis","text":"<p>Meant specifically as an in-memory key-value data store. Most commonly used for distributed key-val.</p>"},{"location":"notes/system-design/tools/#testing-methodologies","title":"Testing Methodologies","text":""},{"location":"notes/system-design/tools/#smoke-tests","title":"Smoke Tests","text":"<p>A Smoke Test tests basic functionality of a service. It can be thought of as a rough sanity check that major features are working as expected. Smoke testing is meant ot be lightweight and quick, and can be used as a gatekeeper for more involved and expensive integration tests.</p>"},{"location":"notes/system-design/tools/#ab-tests","title":"A/B Tests","text":"<p>A/B tests deploy two different implementations of a functionality and run them in parallel either in a development or production environment. The results of the two implementations are compared against each other to determine the viability of the new implementation.</p>"},{"location":"notes/system-design/tools/#functional-tests","title":"Functional Tests","text":"<p>Functional tests assert that the end state of a system is as you expect. They often assert specific details, like a specific value being committed to a database. Integration tests, on the other hand, are often defined as being a specific type of functional test that is simply checking that two or more systems can successfully communicate with each other.</p>"},{"location":"notes/system-design/tools/#integration-tests","title":"Integration Tests","text":"<p>Integration tests assert that two or more components of a system can properly communicate with each other. This is often used when interacting with databases: you might instantiate a Postgres database in docker-compose and assert that your application is sending valid SQL and can parse results from the database appropriately.</p>"},{"location":"notes/system-design/tools/#durable-execution","title":"Durable Execution","text":"<p>According to Temporal:</p> <p>Quote</p> <p>Durable Execution is a development abstraction that preserves complete application state so that upon host or software failure it can seamlessly migrate execution to another machine.</p> <p>Temporal.io is one such implementation of Durable Execution. The model allows your application to seamlessly continue execution even after hardware failures.</p>"},{"location":"blog/archive/2026/","title":"2026","text":""},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/archive/2018/","title":"2018","text":""},{"location":"blog/category/gpu-virtualization/","title":"GPU Virtualization","text":""},{"location":"blog/category/system-design/","title":"System Design","text":""},{"location":"blog/category/career/","title":"Career","text":""},{"location":"blog/category/misc/","title":"Misc","text":""},{"location":"blog/category/rv/","title":"RV","text":""},{"location":"blog/category/cloud/","title":"Cloud","text":""},{"location":"blog/category/math/","title":"Math","text":""},{"location":"blog/category/golang/","title":"Golang","text":""},{"location":"blog/category/poems/","title":"Poems","text":""},{"location":"blog/category/programming/","title":"Programming","text":""},{"location":"blog/category/python/","title":"Python","text":""},{"location":"blog/category/system-administration/","title":"System Administration","text":""},{"location":"blog/category/grafana/","title":"Grafana","text":""},{"location":"blog/category/aws/","title":"AWS","text":""},{"location":"blog/page/2/","title":"Index","text":""},{"location":"blog/page/3/","title":"Index","text":""}]}